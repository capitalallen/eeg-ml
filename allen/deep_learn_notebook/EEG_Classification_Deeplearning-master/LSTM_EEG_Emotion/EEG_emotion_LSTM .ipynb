{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Classifictaion using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This used used LSTM model to classify  electroencephalogram (EEG) brain signal and to predict the human emotions .The notebook classifies data into 3 classes negative,nuteral and positive.\n",
    "\n",
    "The dataset used for this notebook is freely avialable in the following link[https://www.kaggle.com/birdy654/eeg-brainwave-dataset-feeling-emotion](http://www.kaggle.com/birdy654/eeg-brainwave-dataset-feeling-emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  load & read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b     label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00  NEGATIVE  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57   NEUTRAL  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00  POSITIVE  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40  POSITIVE  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60   NEUTRAL  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('emotions.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['# mean_0_a', 'mean_1_a', 'mean_2_a', 'mean_3_a', 'mean_4_a',\n",
      "       'mean_d_0_a', 'mean_d_1_a', 'mean_d_2_a', 'mean_d_3_a', 'mean_d_4_a',\n",
      "       ...\n",
      "       'fft_741_b', 'fft_742_b', 'fft_743_b', 'fft_744_b', 'fft_745_b',\n",
      "       'fft_746_b', 'fft_747_b', 'fft_748_b', 'fft_749_b', 'label'],\n",
      "      dtype='object', length=2549)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2132 entries, 0 to 2131\n",
      "Columns: 2549 entries, # mean_0_a to label\n",
      "dtypes: float64(2548), object(1)\n",
      "memory usage: 41.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='label', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# mean_0_a    0\n",
       "mean_1_a      0\n",
       "mean_2_a      0\n",
       "mean_3_a      0\n",
       "mean_4_a      0\n",
       "             ..\n",
       "fft_746_b     0\n",
       "fft_747_b     0\n",
       "fft_748_b     0\n",
       "fft_749_b     0\n",
       "label         0\n",
       "Length: 2549, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n",
    "#no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
      "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
      "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
      "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
      "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
      "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
      "\n",
      "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
      "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
      "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
      "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
      "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
      "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
      "\n",
      "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  label  \n",
      "0       23.5     -215.0     280.00    -162.00    -162.00     280.00      2  \n",
      "1      -23.3      182.0       2.57     -31.60     -31.60       2.57      0  \n",
      "2      462.0     -267.0     281.00    -148.00    -148.00     281.00      1  \n",
      "3      299.0      132.0     -12.40       9.53       9.53     -12.40      1  \n",
      "4       12.0      119.0     -17.60      23.90      23.90     -17.60      0  \n",
      "\n",
      "[5 rows x 2549 columns]\n",
      "0    716\n",
      "1    708\n",
      "2    708\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "encode = ({'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': 2} )\n",
    "#new dataset with replaced values\n",
    "df_encoded = df.replace(encode)\n",
    "\n",
    "print(df_encoded.head())\n",
    "print(df_encoded['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00      2  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57      0  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00      1  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40      1  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60      0  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2132, 2548)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df_encoded.drop([\"label\"]  ,axis=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2132,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_encoded.loc[:,'label'].values\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1705, 2548)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0],1,x.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],1,x.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1705, 1, 2548) (427, 1, 2548)\n",
      "(1705, 3) (427, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,x_test.shape)\n",
    "print(y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 64)             668928    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 681,443\n",
      "Trainable params: 681,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1,2548),activation=\"relu\",return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32,activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(LSTM(100,return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(LSTM(50))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1705 samples, validate on 427 samples\n",
      "Epoch 1/50\n",
      "1705/1705 [==============================] - 3s 2ms/sample - loss: 0.6955 - accuracy: 0.8041 - val_loss: 0.4640 - val_accuracy: 0.9157\n",
      "Epoch 2/50\n",
      "1705/1705 [==============================] - 0s 223us/sample - loss: 0.4137 - accuracy: 0.9331 - val_loss: 0.3570 - val_accuracy: 0.9251\n",
      "Epoch 3/50\n",
      "1705/1705 [==============================] - 0s 212us/sample - loss: 0.3213 - accuracy: 0.9372 - val_loss: 0.3024 - val_accuracy: 0.9180\n",
      "Epoch 4/50\n",
      "1705/1705 [==============================] - 0s 262us/sample - loss: 0.2528 - accuracy: 0.9460 - val_loss: 0.2562 - val_accuracy: 0.9251\n",
      "Epoch 5/50\n",
      "1705/1705 [==============================] - 0s 257us/sample - loss: 0.2116 - accuracy: 0.9537 - val_loss: 0.2363 - val_accuracy: 0.9274\n",
      "Epoch 6/50\n",
      "1705/1705 [==============================] - 0s 243us/sample - loss: 0.1819 - accuracy: 0.9554 - val_loss: 0.1987 - val_accuracy: 0.9344\n",
      "Epoch 7/50\n",
      "1705/1705 [==============================] - 0s 209us/sample - loss: 0.1333 - accuracy: 0.9795 - val_loss: 0.1729 - val_accuracy: 0.9485\n",
      "Epoch 8/50\n",
      "1705/1705 [==============================] - 0s 212us/sample - loss: 0.1065 - accuracy: 0.9836 - val_loss: 0.1561 - val_accuracy: 0.9485\n",
      "Epoch 9/50\n",
      "1705/1705 [==============================] - 0s 241us/sample - loss: 0.0871 - accuracy: 0.9918 - val_loss: 0.1248 - val_accuracy: 0.9625\n",
      "Epoch 10/50\n",
      "1705/1705 [==============================] - 0s 219us/sample - loss: 0.0807 - accuracy: 0.9853 - val_loss: 0.1425 - val_accuracy: 0.9555\n",
      "Epoch 11/50\n",
      "1705/1705 [==============================] - 0s 220us/sample - loss: 0.0676 - accuracy: 0.9889 - val_loss: 0.1286 - val_accuracy: 0.9602\n",
      "Epoch 12/50\n",
      "1705/1705 [==============================] - 0s 217us/sample - loss: 0.0457 - accuracy: 0.9965 - val_loss: 0.1002 - val_accuracy: 0.9719\n",
      "Epoch 13/50\n",
      "1705/1705 [==============================] - 0s 210us/sample - loss: 0.0558 - accuracy: 0.9900 - val_loss: 0.1102 - val_accuracy: 0.9696\n",
      "Epoch 14/50\n",
      "1705/1705 [==============================] - 0s 223us/sample - loss: 0.0411 - accuracy: 0.9947 - val_loss: 0.1368 - val_accuracy: 0.9602\n",
      "Epoch 15/50\n",
      "1705/1705 [==============================] - 0s 221us/sample - loss: 0.0372 - accuracy: 0.9947 - val_loss: 0.0908 - val_accuracy: 0.9742\n",
      "Epoch 16/50\n",
      "1705/1705 [==============================] - 0s 211us/sample - loss: 0.0372 - accuracy: 0.9941 - val_loss: 0.0856 - val_accuracy: 0.9789\n",
      "Epoch 17/50\n",
      "1705/1705 [==============================] - 0s 214us/sample - loss: 0.0350 - accuracy: 0.9941 - val_loss: 0.1038 - val_accuracy: 0.9719\n",
      "Epoch 18/50\n",
      "1705/1705 [==============================] - 0s 213us/sample - loss: 0.0276 - accuracy: 0.9971 - val_loss: 0.1602 - val_accuracy: 0.9602\n",
      "Epoch 19/50\n",
      "1705/1705 [==============================] - 0s 235us/sample - loss: 0.0280 - accuracy: 0.9977 - val_loss: 0.1004 - val_accuracy: 0.9766\n",
      "Epoch 20/50\n",
      "1705/1705 [==============================] - 0s 215us/sample - loss: 0.0243 - accuracy: 0.9977 - val_loss: 0.1437 - val_accuracy: 0.9578\n",
      "Epoch 21/50\n",
      "1705/1705 [==============================] - 0s 210us/sample - loss: 0.0206 - accuracy: 0.9988 - val_loss: 0.1247 - val_accuracy: 0.9696\n",
      "Epoch 22/50\n",
      "1705/1705 [==============================] - 0s 221us/sample - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.1269 - val_accuracy: 0.9672\n",
      "Epoch 23/50\n",
      "1705/1705 [==============================] - 0s 212us/sample - loss: 0.0152 - accuracy: 0.9994 - val_loss: 0.1172 - val_accuracy: 0.9649\n",
      "Epoch 24/50\n",
      "1705/1705 [==============================] - 0s 219us/sample - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.1234 - val_accuracy: 0.9672\n",
      "Epoch 25/50\n",
      "1705/1705 [==============================] - 0s 216us/sample - loss: 0.0137 - accuracy: 0.9994 - val_loss: 0.1220 - val_accuracy: 0.9742\n",
      "Epoch 26/50\n",
      "1705/1705 [==============================] - 0s 209us/sample - loss: 0.0125 - accuracy: 0.9988 - val_loss: 0.1579 - val_accuracy: 0.9625\n",
      "Epoch 27/50\n",
      "1705/1705 [==============================] - 0s 207us/sample - loss: 0.0245 - accuracy: 0.9947 - val_loss: 0.1268 - val_accuracy: 0.9672\n",
      "Epoch 28/50\n",
      "1705/1705 [==============================] - 0s 224us/sample - loss: 0.0219 - accuracy: 0.9965 - val_loss: 0.1305 - val_accuracy: 0.9649\n",
      "Epoch 29/50\n",
      "1705/1705 [==============================] - 0s 213us/sample - loss: 0.0246 - accuracy: 0.9947 - val_loss: 0.1831 - val_accuracy: 0.9461\n",
      "Epoch 30/50\n",
      "1705/1705 [==============================] - 0s 219us/sample - loss: 0.0364 - accuracy: 0.9924 - val_loss: 0.1056 - val_accuracy: 0.9719\n",
      "Epoch 31/50\n",
      "1705/1705 [==============================] - 0s 215us/sample - loss: 0.0244 - accuracy: 0.9930 - val_loss: 0.1247 - val_accuracy: 0.9649\n",
      "Epoch 32/50\n",
      "1705/1705 [==============================] - 0s 214us/sample - loss: 0.0344 - accuracy: 0.9906 - val_loss: 0.0854 - val_accuracy: 0.9789\n",
      "Epoch 33/50\n",
      "1705/1705 [==============================] - 0s 220us/sample - loss: 0.0236 - accuracy: 0.9947 - val_loss: 0.1368 - val_accuracy: 0.9649\n",
      "Epoch 34/50\n",
      "1705/1705 [==============================] - 0s 220us/sample - loss: 0.0244 - accuracy: 0.9941 - val_loss: 0.1127 - val_accuracy: 0.9672\n",
      "Epoch 35/50\n",
      "1705/1705 [==============================] - 0s 226us/sample - loss: 0.0215 - accuracy: 0.9935 - val_loss: 0.0891 - val_accuracy: 0.9766\n",
      "Epoch 36/50\n",
      "1705/1705 [==============================] - 0s 234us/sample - loss: 0.0219 - accuracy: 0.9941 - val_loss: 0.1685 - val_accuracy: 0.9602\n",
      "Epoch 37/50\n",
      "1705/1705 [==============================] - 0s 201us/sample - loss: 0.0115 - accuracy: 0.9977 - val_loss: 0.1021 - val_accuracy: 0.9696\n",
      "Epoch 38/50\n",
      "1705/1705 [==============================] - 0s 212us/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.1278 - val_accuracy: 0.9696\n",
      "Epoch 39/50\n",
      "1705/1705 [==============================] - 0s 212us/sample - loss: 0.0202 - accuracy: 0.9959 - val_loss: 0.1123 - val_accuracy: 0.9766\n",
      "Epoch 40/50\n",
      "1705/1705 [==============================] - 0s 205us/sample - loss: 0.0136 - accuracy: 0.9971 - val_loss: 0.1532 - val_accuracy: 0.9696\n",
      "Epoch 41/50\n",
      "1705/1705 [==============================] - 0s 224us/sample - loss: 0.0104 - accuracy: 0.9988 - val_loss: 0.1074 - val_accuracy: 0.9742\n",
      "Epoch 42/50\n",
      "1705/1705 [==============================] - 0s 217us/sample - loss: 0.0103 - accuracy: 0.9982 - val_loss: 0.1122 - val_accuracy: 0.9766\n",
      "Epoch 43/50\n",
      "1705/1705 [==============================] - 0s 211us/sample - loss: 0.0199 - accuracy: 0.9947 - val_loss: 0.1564 - val_accuracy: 0.9602\n",
      "Epoch 44/50\n",
      "1705/1705 [==============================] - 0s 208us/sample - loss: 0.0098 - accuracy: 0.9982 - val_loss: 0.1403 - val_accuracy: 0.9742\n",
      "Epoch 45/50\n",
      "1705/1705 [==============================] - 0s 208us/sample - loss: 0.0062 - accuracy: 0.9994 - val_loss: 0.1132 - val_accuracy: 0.9766\n",
      "Epoch 46/50\n",
      "1705/1705 [==============================] - 0s 205us/sample - loss: 0.0115 - accuracy: 0.9982 - val_loss: 0.1920 - val_accuracy: 0.9602\n",
      "Epoch 47/50\n",
      "1705/1705 [==============================] - 0s 206us/sample - loss: 0.0108 - accuracy: 0.9982 - val_loss: 0.1171 - val_accuracy: 0.9742\n",
      "Epoch 48/50\n",
      "1705/1705 [==============================] - 0s 210us/sample - loss: 0.0082 - accuracy: 0.9988 - val_loss: 0.1393 - val_accuracy: 0.9696\n",
      "Epoch 49/50\n",
      "1705/1705 [==============================] - 0s 209us/sample - loss: 0.0055 - accuracy: 0.9994 - val_loss: 0.1482 - val_accuracy: 0.9625\n",
      "Epoch 50/50\n",
      "1705/1705 [==============================] - 0s 213us/sample - loss: 0.0053 - accuracy: 0.9994 - val_loss: 0.1093 - val_accuracy: 0.9719\n",
      "427/427 [==============================] - 0s 72us/sample - loss: 0.1093 - accuracy: 0.9719\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 100, validation_data= (x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427,)\n",
      "(427,)\n",
      "Training Accuracy: 0.9718969555035128\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred = model.predict(x_test)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n",
    "print(expected_classes.shape)\n",
    "print(predict_classes.shape)\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(f\"Training Accuracy: {correct}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
