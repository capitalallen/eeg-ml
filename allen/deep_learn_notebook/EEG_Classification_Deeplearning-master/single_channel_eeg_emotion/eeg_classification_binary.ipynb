{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eeg-classification-binary.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNo0t3tYuPY2RAmGRDsESzC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishshaji/EEG_Classification_Deeplearning/blob/master/single_channel_eeg_emotion/eeg_classification_binary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AAQXuDq19MI",
        "colab_type": "code",
        "outputId": "4f066411-2a6d-42dd-d8c5-47023e8b6f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-Pis1xM2Ly2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "random.seed(72)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcSJO3QY2bdn",
        "colab_type": "code",
        "outputId": "e609c827-13a3-4959-f17d-4bebf53ae900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "df=pd.read_csv(\"/content/gdrive/My Drive/data/emotion1channel.csv\")\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>attention</th>\n",
              "      <th>meditation</th>\n",
              "      <th>delta</th>\n",
              "      <th>theta</th>\n",
              "      <th>lowAplha</th>\n",
              "      <th>highAlpha</th>\n",
              "      <th>lowBeta</th>\n",
              "      <th>highBeta</th>\n",
              "      <th>lowGamma</th>\n",
              "      <th>highGamma</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>88</td>\n",
              "      <td>17</td>\n",
              "      <td>1290697</td>\n",
              "      <td>18187</td>\n",
              "      <td>6345</td>\n",
              "      <td>7462</td>\n",
              "      <td>7266</td>\n",
              "      <td>4278</td>\n",
              "      <td>2589</td>\n",
              "      <td>1228</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>88</td>\n",
              "      <td>17</td>\n",
              "      <td>105432</td>\n",
              "      <td>21344</td>\n",
              "      <td>8323</td>\n",
              "      <td>4496</td>\n",
              "      <td>4784</td>\n",
              "      <td>12071</td>\n",
              "      <td>2024</td>\n",
              "      <td>1123</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>83</td>\n",
              "      <td>29</td>\n",
              "      <td>732143</td>\n",
              "      <td>37527</td>\n",
              "      <td>48422</td>\n",
              "      <td>10286</td>\n",
              "      <td>9499</td>\n",
              "      <td>6050</td>\n",
              "      <td>2362</td>\n",
              "      <td>4157</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>80</td>\n",
              "      <td>26</td>\n",
              "      <td>21265</td>\n",
              "      <td>24517</td>\n",
              "      <td>7051</td>\n",
              "      <td>1790</td>\n",
              "      <td>9106</td>\n",
              "      <td>5771</td>\n",
              "      <td>1977</td>\n",
              "      <td>3265</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69</td>\n",
              "      <td>20</td>\n",
              "      <td>349390</td>\n",
              "      <td>145647</td>\n",
              "      <td>10068</td>\n",
              "      <td>21707</td>\n",
              "      <td>11878</td>\n",
              "      <td>19883</td>\n",
              "      <td>9971</td>\n",
              "      <td>6592</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13364</th>\n",
              "      <td>66</td>\n",
              "      <td>61</td>\n",
              "      <td>36288</td>\n",
              "      <td>10942</td>\n",
              "      <td>15819</td>\n",
              "      <td>4781</td>\n",
              "      <td>10488</td>\n",
              "      <td>11690</td>\n",
              "      <td>7245</td>\n",
              "      <td>2287</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13365</th>\n",
              "      <td>63</td>\n",
              "      <td>81</td>\n",
              "      <td>434483</td>\n",
              "      <td>16548</td>\n",
              "      <td>25760</td>\n",
              "      <td>10239</td>\n",
              "      <td>7332</td>\n",
              "      <td>3822</td>\n",
              "      <td>9214</td>\n",
              "      <td>5527</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13366</th>\n",
              "      <td>61</td>\n",
              "      <td>91</td>\n",
              "      <td>11198</td>\n",
              "      <td>21200</td>\n",
              "      <td>18905</td>\n",
              "      <td>20228</td>\n",
              "      <td>4850</td>\n",
              "      <td>12856</td>\n",
              "      <td>5412</td>\n",
              "      <td>7044</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13367</th>\n",
              "      <td>56</td>\n",
              "      <td>88</td>\n",
              "      <td>537338</td>\n",
              "      <td>31723</td>\n",
              "      <td>1915</td>\n",
              "      <td>13033</td>\n",
              "      <td>10876</td>\n",
              "      <td>8500</td>\n",
              "      <td>7453</td>\n",
              "      <td>3461</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13368</th>\n",
              "      <td>51</td>\n",
              "      <td>90</td>\n",
              "      <td>534966</td>\n",
              "      <td>54906</td>\n",
              "      <td>30588</td>\n",
              "      <td>22906</td>\n",
              "      <td>14624</td>\n",
              "      <td>17227</td>\n",
              "      <td>14293</td>\n",
              "      <td>3204</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13369 rows Ã— 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       attention  meditation    delta  ...  lowGamma  highGamma  class\n",
              "0             88          17  1290697  ...      2589       1228      7\n",
              "1             88          17   105432  ...      2024       1123      7\n",
              "2             83          29   732143  ...      2362       4157      7\n",
              "3             80          26    21265  ...      1977       3265      7\n",
              "4             69          20   349390  ...      9971       6592      7\n",
              "...          ...         ...      ...  ...       ...        ...    ...\n",
              "13364         66          61    36288  ...      7245       2287      1\n",
              "13365         63          81   434483  ...      9214       5527      1\n",
              "13366         61          91    11198  ...      5412       7044      1\n",
              "13367         56          88   537338  ...      7453       3461      1\n",
              "13368         51          90   534966  ...     14293       3204      1\n",
              "\n",
              "[13369 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLixuhjh2g6a",
        "colab_type": "code",
        "outputId": "deac9350-8345-4dee-9f66-621fb1d46a2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "df[\"class\"].unique()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 5, 3, 4, 0, 2, 1, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nELeZtus2owd",
        "colab_type": "code",
        "outputId": "6d6d9fd4-e76f-4d12-a220-77f2829c8596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df.loc[df[\"class\"] == 1, \"class\"] = 0\n",
        "df.loc[df[\"class\"] == 2, \"class\"] = 0\n",
        "df.loc[df[\"class\"] == 3, \"class\"] = 0\n",
        "df.loc[df[\"class\"] == 4, \"class\"] = 1\n",
        "df.loc[df[\"class\"] == 5, \"class\"] = 1\n",
        "df.loc[df[\"class\"] == 6, \"class\"] = 1\n",
        "df.loc[df[\"class\"] == 7, \"class\"] = 1\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>attention</th>\n",
              "      <th>meditation</th>\n",
              "      <th>delta</th>\n",
              "      <th>theta</th>\n",
              "      <th>lowAplha</th>\n",
              "      <th>highAlpha</th>\n",
              "      <th>lowBeta</th>\n",
              "      <th>highBeta</th>\n",
              "      <th>lowGamma</th>\n",
              "      <th>highGamma</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>88</td>\n",
              "      <td>17</td>\n",
              "      <td>1290697</td>\n",
              "      <td>18187</td>\n",
              "      <td>6345</td>\n",
              "      <td>7462</td>\n",
              "      <td>7266</td>\n",
              "      <td>4278</td>\n",
              "      <td>2589</td>\n",
              "      <td>1228</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>88</td>\n",
              "      <td>17</td>\n",
              "      <td>105432</td>\n",
              "      <td>21344</td>\n",
              "      <td>8323</td>\n",
              "      <td>4496</td>\n",
              "      <td>4784</td>\n",
              "      <td>12071</td>\n",
              "      <td>2024</td>\n",
              "      <td>1123</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>83</td>\n",
              "      <td>29</td>\n",
              "      <td>732143</td>\n",
              "      <td>37527</td>\n",
              "      <td>48422</td>\n",
              "      <td>10286</td>\n",
              "      <td>9499</td>\n",
              "      <td>6050</td>\n",
              "      <td>2362</td>\n",
              "      <td>4157</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>80</td>\n",
              "      <td>26</td>\n",
              "      <td>21265</td>\n",
              "      <td>24517</td>\n",
              "      <td>7051</td>\n",
              "      <td>1790</td>\n",
              "      <td>9106</td>\n",
              "      <td>5771</td>\n",
              "      <td>1977</td>\n",
              "      <td>3265</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69</td>\n",
              "      <td>20</td>\n",
              "      <td>349390</td>\n",
              "      <td>145647</td>\n",
              "      <td>10068</td>\n",
              "      <td>21707</td>\n",
              "      <td>11878</td>\n",
              "      <td>19883</td>\n",
              "      <td>9971</td>\n",
              "      <td>6592</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   attention  meditation    delta   theta  ...  highBeta  lowGamma  highGamma  class\n",
              "0         88          17  1290697   18187  ...      4278      2589       1228      1\n",
              "1         88          17   105432   21344  ...     12071      2024       1123      1\n",
              "2         83          29   732143   37527  ...      6050      2362       4157      1\n",
              "3         80          26    21265   24517  ...      5771      1977       3265      1\n",
              "4         69          20   349390  145647  ...     19883      9971       6592      1\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h6DbAJR2yrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=df.drop([\"class\",\"attention\",\"meditation\"]  ,axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh-pmcOAjqvK",
        "colab_type": "code",
        "outputId": "e0c82566-8d3c-44a8-b1e2-c42465c8106d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "y = df.loc[:,'class'].values\n",
        "print(y)\n",
        "print(x.values)\n",
        "x=x.values\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 0 0 0]\n",
            "[[1290697   18187    6345 ...    4278    2589    1228]\n",
            " [ 105432   21344    8323 ...   12071    2024    1123]\n",
            " [ 732143   37527   48422 ...    6050    2362    4157]\n",
            " ...\n",
            " [  11198   21200   18905 ...   12856    5412    7044]\n",
            " [ 537338   31723    1915 ...    8500    7453    3461]\n",
            " [ 534966   54906   30588 ...   17227   14293    3204]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vVS-2t12-2t",
        "colab_type": "code",
        "outputId": "e815ad3f-4f58-46ac-ca6b-57e92e461595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x)\n",
        "x = scaler.transform(x)\n",
        "from keras.utils import to_categorical\n",
        "y = to_categorical(y)\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       ...,\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9_NdI2G3Fga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.reshape(x, (x.shape[0],1,x.shape[1]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtcPe53W3kGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 155)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5ddLVk13NjQ",
        "colab_type": "code",
        "outputId": "d196481f-7173-4f2c-faf4-4ab87a3ec63a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"from tensorflow.keras import Sequential\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout,BatchNormalization\n",
        "from tensorflow.keras.layers import Embedding,Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(LSTM(128, return_sequences=True,activation=\"relu\",input_shape=(1,8)))  # returns a sequence of vectors of dimension 30\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(30, return_sequences=True,activation=\"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(30, return_sequences=True,activation=\"relu\"))\n",
        "\n",
        "model.add(Dropout(0.2))  # returns a sequence of vectors of dimension 30\n",
        "model.add(LSTM(30))  # return a single vector of dimension 30\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n",
        "model.fit(x_train, y_train, batch_size = 10, epochs = 100, validation_data=(x_test,y_test))\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from tensorflow.keras import Sequential\\n\\nfrom tensorflow.keras.layers import Dense, Dropout,BatchNormalization\\nfrom tensorflow.keras.layers import Embedding,Dropout\\nfrom tensorflow.keras.layers import LSTM\\ntf.keras.backend.clear_session()\\n\\n\\nmodel=Sequential()\\nmodel.add(LSTM(128, return_sequences=True,activation=\"relu\",input_shape=(1,8)))  # returns a sequence of vectors of dimension 30\\nmodel.add(BatchNormalization())\\nmodel.add(Dropout(0.2))\\nmodel.add(LSTM(30, return_sequences=True,activation=\"relu\"))\\nmodel.add(BatchNormalization())\\nmodel.add(LSTM(30, return_sequences=True,activation=\"relu\"))\\n\\nmodel.add(Dropout(0.2))  # returns a sequence of vectors of dimension 30\\nmodel.add(LSTM(30))  # return a single vector of dimension 30\\nmodel.add(Dense(2, activation=\\'sigmoid\\'))\\n\\nmodel.compile(loss=\\'binary_crossentropy\\',\\n              optimizer=\\'adam\\',\\n              metrics=[\\'accuracy\\'])\\n\\n\\n\\nmodel.summary()\\nmodel.fit(x_train, y_train, batch_size = 10, epochs = 100, validation_data=(x_test,y_test))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdBgNMGu9QQs",
        "colab_type": "code",
        "outputId": "735c8af8-4af3-4d9e-bcd1-8fd04a111a6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        }
      },
      "source": [
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, LSTM, Dropout, BatchNormalization\n",
        "model = Sequential()\n",
        "model.add(LSTM(512, input_shape = (1,8),activation=\"relu\",return_sequences=True))\n",
        "\n",
        "#model.add(LSTM(100, batch_input_shape = (None, None, x.shape[2])))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(LSTM(256,activation=\"relu\",return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "model.add(LSTM(128,activation=\"relu\",return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(LSTM(64,activation=\"relu\",return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(LSTM(32,activation=\"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Dense(700))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "rmsprop =keras.optimizers.RMSprop(lr=0.009, rho=0.9, epsilon=1e-08)\n",
        "model.compile(loss='mean_squared_error',\n",
        "                  optimizer=rmsprop,\n",
        "                  metrics=['accuracy'])\n",
        "#adam = keras.optimizers.Adam(lr=0.5, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_24 (LSTM)               (None, 1, 512)            1067008   \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 1, 512)            2048      \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 1, 512)            0         \n",
            "_________________________________________________________________\n",
            "lstm_25 (LSTM)               (None, 1, 256)            787456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 1, 256)            1024      \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 1, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_26 (LSTM)               (None, 1, 128)            197120    \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 1, 128)            512       \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 1, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_27 (LSTM)               (None, 1, 64)             49408     \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 1, 64)             256       \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 1, 64)             0         \n",
            "_________________________________________________________________\n",
            "lstm_28 (LSTM)               (None, 32)                12416     \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 700)               23100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 700)               2800      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 700)               0         \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 700)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 1402      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 2,144,678\n",
            "Trainable params: 2,141,294\n",
            "Non-trainable params: 3,384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APRGM1Dv9StV",
        "colab_type": "code",
        "outputId": "a89c54bd-4ea3-4294-9e77-1c39b500535d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x_train, y_train, batch_size = 100, epochs = 100, validation_data=(x_test,y_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10695 samples, validate on 2674 samples\n",
            "Epoch 1/100\n",
            "10695/10695 [==============================] - 11s 1ms/step - loss: 0.2240 - accuracy: 0.7020 - val_loss: 0.2114 - val_accuracy: 0.7057\n",
            "Epoch 2/100\n",
            "10695/10695 [==============================] - 8s 742us/step - loss: 0.2121 - accuracy: 0.7069 - val_loss: 0.2084 - val_accuracy: 0.7053\n",
            "Epoch 3/100\n",
            "10695/10695 [==============================] - 8s 721us/step - loss: 0.2097 - accuracy: 0.7066 - val_loss: 0.2120 - val_accuracy: 0.7057\n",
            "Epoch 4/100\n",
            "10695/10695 [==============================] - 8s 723us/step - loss: 0.2114 - accuracy: 0.7025 - val_loss: 0.2109 - val_accuracy: 0.6990\n",
            "Epoch 5/100\n",
            "10695/10695 [==============================] - 8s 728us/step - loss: 0.2105 - accuracy: 0.7058 - val_loss: 0.2079 - val_accuracy: 0.7046\n",
            "Epoch 6/100\n",
            "10695/10695 [==============================] - 8s 725us/step - loss: 0.2111 - accuracy: 0.7054 - val_loss: 0.2069 - val_accuracy: 0.7057\n",
            "Epoch 7/100\n",
            "10695/10695 [==============================] - 8s 729us/step - loss: 0.2079 - accuracy: 0.7075 - val_loss: 0.2067 - val_accuracy: 0.7057\n",
            "Epoch 8/100\n",
            "10695/10695 [==============================] - 8s 749us/step - loss: 0.2082 - accuracy: 0.7071 - val_loss: 0.2086 - val_accuracy: 0.7019\n",
            "Epoch 9/100\n",
            "10695/10695 [==============================] - 8s 736us/step - loss: 0.2107 - accuracy: 0.7020 - val_loss: 0.2068 - val_accuracy: 0.7042\n",
            "Epoch 10/100\n",
            "10695/10695 [==============================] - 8s 725us/step - loss: 0.2102 - accuracy: 0.7052 - val_loss: 0.2050 - val_accuracy: 0.7064\n",
            "Epoch 11/100\n",
            "10695/10695 [==============================] - 8s 726us/step - loss: 0.2079 - accuracy: 0.7072 - val_loss: 0.2050 - val_accuracy: 0.7057\n",
            "Epoch 12/100\n",
            "10695/10695 [==============================] - 8s 722us/step - loss: 0.2068 - accuracy: 0.7084 - val_loss: 0.2075 - val_accuracy: 0.7061\n",
            "Epoch 13/100\n",
            "10695/10695 [==============================] - 8s 726us/step - loss: 0.2090 - accuracy: 0.7059 - val_loss: 0.2050 - val_accuracy: 0.7061\n",
            "Epoch 14/100\n",
            "10695/10695 [==============================] - 8s 724us/step - loss: 0.2078 - accuracy: 0.7082 - val_loss: 0.2074 - val_accuracy: 0.7057\n",
            "Epoch 15/100\n",
            "10695/10695 [==============================] - 8s 720us/step - loss: 0.2073 - accuracy: 0.7077 - val_loss: 0.2052 - val_accuracy: 0.7064\n",
            "Epoch 16/100\n",
            "10695/10695 [==============================] - 8s 725us/step - loss: 0.2083 - accuracy: 0.7072 - val_loss: 0.2053 - val_accuracy: 0.7064\n",
            "Epoch 17/100\n",
            "10695/10695 [==============================] - 8s 718us/step - loss: 0.2083 - accuracy: 0.7067 - val_loss: 0.2060 - val_accuracy: 0.7072\n",
            "Epoch 18/100\n",
            "10695/10695 [==============================] - 8s 722us/step - loss: 0.2079 - accuracy: 0.7072 - val_loss: 0.2064 - val_accuracy: 0.7061\n",
            "Epoch 19/100\n",
            "10695/10695 [==============================] - 8s 725us/step - loss: 0.2076 - accuracy: 0.7067 - val_loss: 0.2062 - val_accuracy: 0.7057\n",
            "Epoch 20/100\n",
            "10695/10695 [==============================] - 8s 722us/step - loss: 0.2072 - accuracy: 0.7078 - val_loss: 0.2066 - val_accuracy: 0.7057\n",
            "Epoch 21/100\n",
            "10695/10695 [==============================] - 8s 726us/step - loss: 0.2080 - accuracy: 0.7077 - val_loss: 0.2054 - val_accuracy: 0.7057\n",
            "Epoch 22/100\n",
            "10695/10695 [==============================] - 8s 728us/step - loss: 0.2073 - accuracy: 0.7077 - val_loss: 0.2061 - val_accuracy: 0.7057\n",
            "Epoch 23/100\n",
            "10695/10695 [==============================] - 8s 727us/step - loss: 0.2073 - accuracy: 0.7075 - val_loss: 0.2060 - val_accuracy: 0.7057\n",
            "Epoch 24/100\n",
            "10695/10695 [==============================] - 8s 723us/step - loss: 0.2058 - accuracy: 0.7072 - val_loss: 0.2059 - val_accuracy: 0.7057\n",
            "Epoch 25/100\n",
            "10695/10695 [==============================] - 8s 725us/step - loss: 0.2060 - accuracy: 0.7076 - val_loss: 0.2057 - val_accuracy: 0.7057\n",
            "Epoch 26/100\n",
            "10695/10695 [==============================] - 8s 724us/step - loss: 0.2078 - accuracy: 0.7066 - val_loss: 0.2063 - val_accuracy: 0.7057\n",
            "Epoch 27/100\n",
            "10695/10695 [==============================] - 8s 721us/step - loss: 0.2076 - accuracy: 0.7076 - val_loss: 0.2060 - val_accuracy: 0.7057\n",
            "Epoch 28/100\n",
            "10695/10695 [==============================] - 8s 740us/step - loss: 0.2069 - accuracy: 0.7078 - val_loss: 0.2056 - val_accuracy: 0.7057\n",
            "Epoch 29/100\n",
            "10695/10695 [==============================] - 8s 720us/step - loss: 0.2062 - accuracy: 0.7072 - val_loss: 0.2060 - val_accuracy: 0.7057\n",
            "Epoch 30/100\n",
            "10695/10695 [==============================] - 8s 729us/step - loss: 0.2052 - accuracy: 0.7078 - val_loss: 0.2057 - val_accuracy: 0.7057\n",
            "Epoch 31/100\n",
            "10695/10695 [==============================] - 8s 723us/step - loss: 0.2056 - accuracy: 0.7076 - val_loss: 0.2056 - val_accuracy: 0.7057\n",
            "Epoch 32/100\n",
            "10695/10695 [==============================] - 8s 722us/step - loss: 0.2058 - accuracy: 0.7072 - val_loss: 0.2053 - val_accuracy: 0.7057\n",
            "Epoch 33/100\n",
            "10695/10695 [==============================] - 8s 721us/step - loss: 0.2064 - accuracy: 0.7069 - val_loss: 0.2054 - val_accuracy: 0.7057\n",
            "Epoch 34/100\n",
            "10695/10695 [==============================] - 8s 725us/step - loss: 0.2057 - accuracy: 0.7076 - val_loss: 0.2056 - val_accuracy: 0.7057\n",
            "Epoch 35/100\n",
            "10695/10695 [==============================] - 8s 718us/step - loss: 0.2059 - accuracy: 0.7065 - val_loss: 0.2055 - val_accuracy: 0.7057\n",
            "Epoch 36/100\n",
            "10695/10695 [==============================] - 8s 723us/step - loss: 0.2079 - accuracy: 0.7054 - val_loss: 0.2049 - val_accuracy: 0.7053\n",
            "Epoch 37/100\n",
            "10695/10695 [==============================] - 8s 718us/step - loss: 0.2076 - accuracy: 0.7069 - val_loss: 0.2055 - val_accuracy: 0.7049\n",
            "Epoch 38/100\n",
            "10695/10695 [==============================] - 8s 720us/step - loss: 0.2075 - accuracy: 0.7079 - val_loss: 0.2049 - val_accuracy: 0.7053\n",
            "Epoch 39/100\n",
            "10695/10695 [==============================] - 8s 723us/step - loss: 0.2072 - accuracy: 0.7095 - val_loss: 0.2054 - val_accuracy: 0.7061\n",
            "Epoch 40/100\n",
            "10695/10695 [==============================] - 8s 724us/step - loss: 0.2072 - accuracy: 0.7069 - val_loss: 0.2052 - val_accuracy: 0.7053\n",
            "Epoch 41/100\n",
            "10695/10695 [==============================] - 8s 722us/step - loss: 0.2071 - accuracy: 0.7072 - val_loss: 0.2052 - val_accuracy: 0.7061\n",
            "Epoch 42/100\n",
            "10695/10695 [==============================] - 8s 729us/step - loss: 0.2068 - accuracy: 0.7076 - val_loss: 0.2054 - val_accuracy: 0.7061\n",
            "Epoch 43/100\n",
            "10695/10695 [==============================] - 8s 730us/step - loss: 0.2067 - accuracy: 0.7071 - val_loss: 0.2060 - val_accuracy: 0.7057\n",
            "Epoch 44/100\n",
            "10695/10695 [==============================] - 8s 727us/step - loss: 0.2054 - accuracy: 0.7071 - val_loss: 0.2054 - val_accuracy: 0.7057\n",
            "Epoch 45/100\n",
            "10695/10695 [==============================] - 8s 727us/step - loss: 0.2067 - accuracy: 0.7075 - val_loss: 0.2065 - val_accuracy: 0.7057\n",
            "Epoch 46/100\n",
            "10695/10695 [==============================] - 8s 729us/step - loss: 0.2062 - accuracy: 0.7073 - val_loss: 0.2059 - val_accuracy: 0.7057\n",
            "Epoch 47/100\n",
            "10695/10695 [==============================] - 8s 718us/step - loss: 0.2068 - accuracy: 0.7063 - val_loss: 0.2057 - val_accuracy: 0.7049\n",
            "Epoch 48/100\n",
            "10695/10695 [==============================] - 8s 744us/step - loss: 0.2069 - accuracy: 0.7071 - val_loss: 0.2047 - val_accuracy: 0.7057\n",
            "Epoch 49/100\n",
            "10695/10695 [==============================] - 8s 744us/step - loss: 0.2062 - accuracy: 0.7068 - val_loss: 0.2055 - val_accuracy: 0.7057\n",
            "Epoch 50/100\n",
            "10695/10695 [==============================] - 8s 741us/step - loss: 0.2057 - accuracy: 0.7055 - val_loss: 0.2049 - val_accuracy: 0.7057\n",
            "Epoch 51/100\n",
            "10695/10695 [==============================] - 8s 721us/step - loss: 0.2047 - accuracy: 0.7074 - val_loss: 0.2055 - val_accuracy: 0.7057\n",
            "Epoch 52/100\n",
            "10695/10695 [==============================] - 8s 722us/step - loss: 0.2051 - accuracy: 0.7076 - val_loss: 0.2054 - val_accuracy: 0.7057\n",
            "Epoch 53/100\n",
            "10695/10695 [==============================] - 8s 728us/step - loss: 0.2045 - accuracy: 0.7087 - val_loss: 0.2056 - val_accuracy: 0.7057\n",
            "Epoch 54/100\n",
            "10695/10695 [==============================] - 8s 720us/step - loss: 0.2059 - accuracy: 0.7050 - val_loss: 0.2050 - val_accuracy: 0.7057\n",
            "Epoch 55/100\n",
            "10695/10695 [==============================] - 8s 726us/step - loss: 0.2047 - accuracy: 0.7073 - val_loss: 0.2058 - val_accuracy: 0.7057\n",
            "Epoch 56/100\n",
            "10695/10695 [==============================] - 8s 725us/step - loss: 0.2046 - accuracy: 0.7080 - val_loss: 0.2049 - val_accuracy: 0.7057\n",
            "Epoch 57/100\n",
            "10695/10695 [==============================] - 8s 722us/step - loss: 0.2051 - accuracy: 0.7071 - val_loss: 0.2051 - val_accuracy: 0.7053\n",
            "Epoch 58/100\n",
            "10695/10695 [==============================] - 8s 722us/step - loss: 0.2043 - accuracy: 0.7076 - val_loss: 0.2051 - val_accuracy: 0.7057\n",
            "Epoch 59/100\n",
            "10695/10695 [==============================] - 8s 726us/step - loss: 0.2044 - accuracy: 0.7085 - val_loss: 0.2052 - val_accuracy: 0.7057\n",
            "Epoch 60/100\n",
            "10695/10695 [==============================] - 8s 725us/step - loss: 0.2037 - accuracy: 0.7074 - val_loss: 0.2053 - val_accuracy: 0.7057\n",
            "Epoch 61/100\n",
            "10695/10695 [==============================] - 8s 729us/step - loss: 0.2037 - accuracy: 0.7073 - val_loss: 0.2048 - val_accuracy: 0.7057\n",
            "Epoch 62/100\n",
            "10695/10695 [==============================] - 8s 728us/step - loss: 0.2038 - accuracy: 0.7083 - val_loss: 0.2045 - val_accuracy: 0.7061\n",
            "Epoch 63/100\n",
            "10695/10695 [==============================] - 8s 729us/step - loss: 0.2039 - accuracy: 0.7072 - val_loss: 0.2054 - val_accuracy: 0.7057\n",
            "Epoch 64/100\n",
            "10695/10695 [==============================] - 8s 724us/step - loss: 0.2038 - accuracy: 0.7060 - val_loss: 0.2052 - val_accuracy: 0.7057\n",
            "Epoch 65/100\n",
            "10695/10695 [==============================] - 8s 731us/step - loss: 0.2035 - accuracy: 0.7080 - val_loss: 0.2049 - val_accuracy: 0.7061\n",
            "Epoch 66/100\n",
            "10695/10695 [==============================] - 8s 726us/step - loss: 0.2039 - accuracy: 0.7089 - val_loss: 0.2059 - val_accuracy: 0.7053\n",
            "Epoch 67/100\n",
            "10695/10695 [==============================] - 8s 737us/step - loss: 0.2039 - accuracy: 0.7081 - val_loss: 0.2054 - val_accuracy: 0.7057\n",
            "Epoch 68/100\n",
            "10695/10695 [==============================] - 8s 734us/step - loss: 0.2041 - accuracy: 0.7072 - val_loss: 0.2051 - val_accuracy: 0.7061\n",
            "Epoch 69/100\n",
            "10695/10695 [==============================] - 8s 732us/step - loss: 0.2034 - accuracy: 0.7084 - val_loss: 0.2051 - val_accuracy: 0.7053\n",
            "Epoch 70/100\n",
            "10695/10695 [==============================] - 8s 735us/step - loss: 0.2029 - accuracy: 0.7069 - val_loss: 0.2045 - val_accuracy: 0.7068\n",
            "Epoch 71/100\n",
            "10695/10695 [==============================] - 8s 727us/step - loss: 0.2033 - accuracy: 0.7081 - val_loss: 0.2058 - val_accuracy: 0.7053\n",
            "Epoch 72/100\n",
            "10695/10695 [==============================] - 8s 733us/step - loss: 0.2031 - accuracy: 0.7088 - val_loss: 0.2047 - val_accuracy: 0.7061\n",
            "Epoch 73/100\n",
            "10695/10695 [==============================] - 8s 738us/step - loss: 0.2036 - accuracy: 0.7079 - val_loss: 0.2050 - val_accuracy: 0.7057\n",
            "Epoch 74/100\n",
            "10695/10695 [==============================] - 8s 736us/step - loss: 0.2032 - accuracy: 0.7072 - val_loss: 0.2051 - val_accuracy: 0.7057\n",
            "Epoch 75/100\n",
            "10695/10695 [==============================] - 8s 739us/step - loss: 0.2028 - accuracy: 0.7085 - val_loss: 0.2045 - val_accuracy: 0.7064\n",
            "Epoch 76/100\n",
            "10695/10695 [==============================] - 8s 737us/step - loss: 0.2033 - accuracy: 0.7070 - val_loss: 0.2056 - val_accuracy: 0.7053\n",
            "Epoch 77/100\n",
            "10695/10695 [==============================] - 8s 735us/step - loss: 0.2028 - accuracy: 0.7090 - val_loss: 0.2053 - val_accuracy: 0.7053\n",
            "Epoch 78/100\n",
            "10695/10695 [==============================] - 8s 739us/step - loss: 0.2018 - accuracy: 0.7084 - val_loss: 0.2048 - val_accuracy: 0.7061\n",
            "Epoch 79/100\n",
            "10695/10695 [==============================] - 8s 738us/step - loss: 0.2032 - accuracy: 0.7083 - val_loss: 0.2049 - val_accuracy: 0.7053\n",
            "Epoch 80/100\n",
            "10695/10695 [==============================] - 8s 733us/step - loss: 0.2024 - accuracy: 0.7095 - val_loss: 0.2049 - val_accuracy: 0.7053\n",
            "Epoch 81/100\n",
            "10695/10695 [==============================] - 8s 741us/step - loss: 0.2022 - accuracy: 0.7084 - val_loss: 0.2059 - val_accuracy: 0.7046\n",
            "Epoch 82/100\n",
            "10695/10695 [==============================] - 8s 736us/step - loss: 0.2023 - accuracy: 0.7092 - val_loss: 0.2048 - val_accuracy: 0.7061\n",
            "Epoch 83/100\n",
            "10695/10695 [==============================] - 8s 738us/step - loss: 0.2011 - accuracy: 0.7110 - val_loss: 0.2060 - val_accuracy: 0.7046\n",
            "Epoch 84/100\n",
            "10695/10695 [==============================] - 8s 739us/step - loss: 0.2026 - accuracy: 0.7091 - val_loss: 0.2053 - val_accuracy: 0.7053\n",
            "Epoch 85/100\n",
            "10695/10695 [==============================] - 8s 738us/step - loss: 0.2027 - accuracy: 0.7081 - val_loss: 0.2055 - val_accuracy: 0.7049\n",
            "Epoch 86/100\n",
            "10695/10695 [==============================] - 8s 739us/step - loss: 0.2020 - accuracy: 0.7072 - val_loss: 0.2054 - val_accuracy: 0.7049\n",
            "Epoch 87/100\n",
            "10695/10695 [==============================] - 8s 738us/step - loss: 0.2019 - accuracy: 0.7091 - val_loss: 0.2056 - val_accuracy: 0.7053\n",
            "Epoch 88/100\n",
            "10695/10695 [==============================] - 8s 735us/step - loss: 0.2014 - accuracy: 0.7096 - val_loss: 0.2065 - val_accuracy: 0.7049\n",
            "Epoch 89/100\n",
            "10695/10695 [==============================] - 8s 744us/step - loss: 0.2022 - accuracy: 0.7087 - val_loss: 0.2057 - val_accuracy: 0.7046\n",
            "Epoch 90/100\n",
            "10695/10695 [==============================] - 8s 747us/step - loss: 0.2012 - accuracy: 0.7107 - val_loss: 0.2059 - val_accuracy: 0.7031\n",
            "Epoch 91/100\n",
            "10695/10695 [==============================] - 8s 739us/step - loss: 0.2015 - accuracy: 0.7109 - val_loss: 0.2063 - val_accuracy: 0.7034\n",
            "Epoch 92/100\n",
            "10695/10695 [==============================] - 8s 736us/step - loss: 0.2014 - accuracy: 0.7081 - val_loss: 0.2056 - val_accuracy: 0.7034\n",
            "Epoch 93/100\n",
            "10695/10695 [==============================] - 8s 739us/step - loss: 0.2020 - accuracy: 0.7096 - val_loss: 0.2050 - val_accuracy: 0.7046\n",
            "Epoch 94/100\n",
            "10695/10695 [==============================] - 8s 736us/step - loss: 0.2010 - accuracy: 0.7113 - val_loss: 0.2057 - val_accuracy: 0.7061\n",
            "Epoch 95/100\n",
            "10695/10695 [==============================] - 8s 734us/step - loss: 0.2021 - accuracy: 0.7101 - val_loss: 0.2055 - val_accuracy: 0.7061\n",
            "Epoch 96/100\n",
            "10695/10695 [==============================] - 8s 736us/step - loss: 0.2012 - accuracy: 0.7099 - val_loss: 0.2047 - val_accuracy: 0.7053\n",
            "Epoch 97/100\n",
            "10695/10695 [==============================] - 8s 734us/step - loss: 0.2015 - accuracy: 0.7083 - val_loss: 0.2064 - val_accuracy: 0.7053\n",
            "Epoch 98/100\n",
            "10695/10695 [==============================] - 8s 736us/step - loss: 0.2015 - accuracy: 0.7083 - val_loss: 0.2057 - val_accuracy: 0.7049\n",
            "Epoch 99/100\n",
            "10695/10695 [==============================] - 8s 734us/step - loss: 0.2008 - accuracy: 0.7102 - val_loss: 0.2054 - val_accuracy: 0.7027\n",
            "Epoch 100/100\n",
            "10695/10695 [==============================] - 8s 735us/step - loss: 0.2011 - accuracy: 0.7096 - val_loss: 0.2052 - val_accuracy: 0.7031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f68dedb9710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F51Vrjc59Zpf",
        "colab_type": "code",
        "outputId": "f5fb8aef-0c74-47eb-d118-8438e84eac7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "pred = model.predict(x_test)\n",
        "predict_classes = np.argmax(pred,axis=1)\n",
        "expected_classes = np.argmax(y_test,axis=1)\n",
        "print(expected_classes.shape)\n",
        "print(predict_classes.shape)\n",
        "correct = accuracy_score(expected_classes,predict_classes)\n",
        "print(f\"Training Accuracy: {correct}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2674,)\n",
            "(2674,)\n",
            "Training Accuracy: 0.7030665669409125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbpfIXDhSWAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}