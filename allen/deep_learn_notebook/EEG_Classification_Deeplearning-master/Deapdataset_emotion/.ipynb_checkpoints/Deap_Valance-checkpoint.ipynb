{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ashishshaji/EEG_Classification_Deeplearning/blob/master/Deapdataset_emotion/Deap_Valance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QVkjoVU_uDK3"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-d1488d53be8c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-d1488d53be8c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip3 install git+https://github.com/forrestbao/pyeeg.git\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/forrestbao/pyeeg.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "XqjBGbE8LBao",
    "outputId": "a752f522-d997-4626-dfd3-6dcfcd4c35f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hai\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"hai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5SPLfnVBY6LY"
   },
   "outputs": [],
   "source": [
    " import numpy as np\n",
    "\n",
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import pyeeg as pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iq8EcwGxsgw4"
   },
   "outputs": [],
   "source": [
    "channel = [1,2,3,4,6,11,13,17,19,20,21,25,29,31] #14 Channels chosen to fit Emotiv Epoch+\n",
    "band = [4,8,12,16,25,45] #5 bands\n",
    "window_size = 256 #Averaging band power of 2 sec\n",
    "step_size = 16 #Each 0.125 sec update once\n",
    "sample_rate = 70 #Sampling rate of 128 Hz\n",
    "subjectList = ['01','02','03']\n",
    "#List of subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLMSKXbch6TX"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMHsERGylAEu"
   },
   "outputs": [],
   "source": [
    "def FFT_Processing (sub, channel, band, window_size, step_size, sample_rate):\n",
    "    '''\n",
    "    arguments:  string subject\n",
    "                list channel indice\n",
    "                list band\n",
    "                int window size for FFT\n",
    "                int step size for FFT\n",
    "                int sample rate for FFT\n",
    "    return:     void\n",
    "    '''\n",
    "    meta = []\n",
    "    with open('/content/drive/My Drive/data/s' + sub + '.dat', 'rb') as file:\n",
    "\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "\n",
    "        for i in range (0,39):\n",
    "            # loop over 0-39 trails\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            start = 0;\n",
    "\n",
    "            while start + window_size < data.shape[1]:\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    Y = pe.bin_power(X, band, sample_rate) #FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\n",
    "                    meta_data = meta_data + list(Y[0])\n",
    "\n",
    "                meta_array.append(np.array(meta_data))\n",
    "                meta_array.append(labels)\n",
    "\n",
    "                meta.append(np.array(meta_array))    \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta = np.array(meta)\n",
    "        np.save('out\\s' + sub, meta, allow_pickle=True, fix_imports=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VzAJVGQBt-5v"
   },
   "outputs": [],
   "source": [
    "for subjects in subjectList:\n",
    "    FFT_Processing (subjects, channel, band, window_size, step_size, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZGXiP2t2DFC"
   },
   "outputs": [],
   "source": [
    "\n",
    "data_training = []\n",
    "label_training = []\n",
    "data_testing = []\n",
    "label_testing = []\n",
    "data_validation = []\n",
    "label_validation = []\n",
    "\n",
    "for subjects in subjectList:\n",
    "  \n",
    "\n",
    "    with open('/content/out\\s' + subjects + '.npy', 'rb') as file:\n",
    "        sub = np.load(file,allow_pickle=True)\n",
    "        for i in range (0,sub.shape[0]):\n",
    "            if i % 8 == 0 or i % 8== 0:\n",
    "                data_testing.append(sub[i][0])\n",
    "                label_testing.append(sub[i][1])\n",
    "           \n",
    "            else:\n",
    "                data_training.append(sub[i][0])\n",
    "                label_training.append(sub[i][1])\n",
    "\n",
    "np.save('out\\data_training', np.array(data_training), allow_pickle=True, fix_imports=True)\n",
    "np.save('out\\label_training', np.array(label_training), allow_pickle=True, fix_imports=True)\n",
    "print(\"training dataset:\", np.array(data_training).shape, np.array(label_training).shape)\n",
    "\n",
    "\n",
    "np.save('out\\data_testing', np.array(data_testing), allow_pickle=True, fix_imports=True)\n",
    "np.save('out\\label_testing', np.array(label_testing), allow_pickle=True, fix_imports=True)\n",
    "print(\"testing dataset:\", np.array(data_testing).shape, np.array(label_testing).shape)\n",
    "\"\"\"\n",
    "np.save('out\\data_validation', np.array(data_validation), allow_pickle=True, fix_imports=True)\n",
    "np.save('out\\label_validation', np.array(label_validation), allow_pickle=True, fix_imports=True)\n",
    "print(\"validation dataset:\", np.array(data_validation).shape, np.array(label_validation).shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CFcayQrJ-h0I"
   },
   "outputs": [],
   "source": [
    "data= []\n",
    "label = []\n",
    "for subjects in subjectList:\n",
    "  \n",
    "\n",
    "    with open('/content/out\\s' + subjects + '.npy', 'rb') as file:\n",
    "        sub = np.load(file,allow_pickle=True)\n",
    "        for i in range (0,sub.shape[0]):\n",
    "          data.append(sub[i][0])\n",
    "          label.append(sub[i][1])\n",
    "np.save('data', np.array(data), allow_pickle=True, fix_imports=True)\n",
    "np.save('label', np.array(label), allow_pickle=True, fix_imports=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xBM9U5Kht73H"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data=data)\n",
    "df.to_csv(\"data.csv\",index=False)\n",
    "\n",
    "df1=pd.DataFrame(data=label)\n",
    "df1.to_csv(\"label.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3nEbmeVBikG"
   },
   "outputs": [],
   "source": [
    "\n",
    "data1=pd.read_csv(\"/content/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "colab_type": "code",
    "id": "oqaQzAAqBtud",
    "outputId": "33850005-bbc5-4dad-e83e-664c869edb0a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467.141307</td>\n",
       "      <td>957.354712</td>\n",
       "      <td>975.500368</td>\n",
       "      <td>886.561347</td>\n",
       "      <td>257.611040</td>\n",
       "      <td>1729.379210</td>\n",
       "      <td>1079.293596</td>\n",
       "      <td>1017.848244</td>\n",
       "      <td>950.011987</td>\n",
       "      <td>277.194922</td>\n",
       "      <td>1433.669444</td>\n",
       "      <td>817.648062</td>\n",
       "      <td>872.888936</td>\n",
       "      <td>723.820692</td>\n",
       "      <td>330.799976</td>\n",
       "      <td>1305.646204</td>\n",
       "      <td>725.308620</td>\n",
       "      <td>723.069802</td>\n",
       "      <td>616.762953</td>\n",
       "      <td>298.419600</td>\n",
       "      <td>2232.091852</td>\n",
       "      <td>898.597142</td>\n",
       "      <td>882.548212</td>\n",
       "      <td>671.334023</td>\n",
       "      <td>204.950299</td>\n",
       "      <td>1656.711555</td>\n",
       "      <td>1079.888219</td>\n",
       "      <td>824.615939</td>\n",
       "      <td>1219.687224</td>\n",
       "      <td>255.591341</td>\n",
       "      <td>1764.225492</td>\n",
       "      <td>1107.110340</td>\n",
       "      <td>704.251457</td>\n",
       "      <td>1358.720731</td>\n",
       "      <td>306.223984</td>\n",
       "      <td>1354.731752</td>\n",
       "      <td>1192.497892</td>\n",
       "      <td>843.459052</td>\n",
       "      <td>1046.209998</td>\n",
       "      <td>289.844396</td>\n",
       "      <td>1493.501713</td>\n",
       "      <td>1399.348733</td>\n",
       "      <td>769.013475</td>\n",
       "      <td>953.694157</td>\n",
       "      <td>173.338741</td>\n",
       "      <td>1603.607021</td>\n",
       "      <td>1310.021584</td>\n",
       "      <td>693.965254</td>\n",
       "      <td>846.343685</td>\n",
       "      <td>125.412422</td>\n",
       "      <td>1422.062276</td>\n",
       "      <td>1059.968148</td>\n",
       "      <td>850.467191</td>\n",
       "      <td>748.996894</td>\n",
       "      <td>93.335423</td>\n",
       "      <td>1762.946195</td>\n",
       "      <td>1194.199707</td>\n",
       "      <td>895.388314</td>\n",
       "      <td>1031.599669</td>\n",
       "      <td>191.863673</td>\n",
       "      <td>1616.579471</td>\n",
       "      <td>1015.132346</td>\n",
       "      <td>769.122057</td>\n",
       "      <td>991.446909</td>\n",
       "      <td>286.112522</td>\n",
       "      <td>1897.870710</td>\n",
       "      <td>1178.996354</td>\n",
       "      <td>740.014784</td>\n",
       "      <td>722.807957</td>\n",
       "      <td>218.773480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1402.080988</td>\n",
       "      <td>872.471171</td>\n",
       "      <td>1069.522141</td>\n",
       "      <td>1008.312775</td>\n",
       "      <td>741.738698</td>\n",
       "      <td>1642.212773</td>\n",
       "      <td>959.802426</td>\n",
       "      <td>1016.890625</td>\n",
       "      <td>1016.753121</td>\n",
       "      <td>486.161277</td>\n",
       "      <td>1354.062445</td>\n",
       "      <td>878.847001</td>\n",
       "      <td>958.429401</td>\n",
       "      <td>910.218926</td>\n",
       "      <td>615.571954</td>\n",
       "      <td>1265.203101</td>\n",
       "      <td>706.681078</td>\n",
       "      <td>654.674940</td>\n",
       "      <td>633.749765</td>\n",
       "      <td>313.357285</td>\n",
       "      <td>2056.260468</td>\n",
       "      <td>1058.232232</td>\n",
       "      <td>802.216823</td>\n",
       "      <td>670.808728</td>\n",
       "      <td>461.209980</td>\n",
       "      <td>1766.592968</td>\n",
       "      <td>1159.323820</td>\n",
       "      <td>771.428152</td>\n",
       "      <td>1301.045163</td>\n",
       "      <td>156.490462</td>\n",
       "      <td>1949.898716</td>\n",
       "      <td>1130.282763</td>\n",
       "      <td>732.055302</td>\n",
       "      <td>1373.459516</td>\n",
       "      <td>306.594376</td>\n",
       "      <td>1474.416934</td>\n",
       "      <td>1096.577648</td>\n",
       "      <td>926.281507</td>\n",
       "      <td>1003.731817</td>\n",
       "      <td>663.875404</td>\n",
       "      <td>1592.669797</td>\n",
       "      <td>1330.542492</td>\n",
       "      <td>916.290209</td>\n",
       "      <td>1023.781200</td>\n",
       "      <td>520.303004</td>\n",
       "      <td>1718.349673</td>\n",
       "      <td>1247.833293</td>\n",
       "      <td>820.819259</td>\n",
       "      <td>912.190335</td>\n",
       "      <td>591.559445</td>\n",
       "      <td>1425.008458</td>\n",
       "      <td>1033.144391</td>\n",
       "      <td>842.420917</td>\n",
       "      <td>759.374420</td>\n",
       "      <td>262.314522</td>\n",
       "      <td>1701.925470</td>\n",
       "      <td>1125.489718</td>\n",
       "      <td>934.487025</td>\n",
       "      <td>1018.607924</td>\n",
       "      <td>95.647129</td>\n",
       "      <td>1639.170401</td>\n",
       "      <td>924.285483</td>\n",
       "      <td>828.084355</td>\n",
       "      <td>971.772255</td>\n",
       "      <td>189.171448</td>\n",
       "      <td>1902.103188</td>\n",
       "      <td>1122.875330</td>\n",
       "      <td>761.432035</td>\n",
       "      <td>768.601395</td>\n",
       "      <td>359.695638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1569.859186</td>\n",
       "      <td>895.845443</td>\n",
       "      <td>1035.124465</td>\n",
       "      <td>918.612673</td>\n",
       "      <td>113.293654</td>\n",
       "      <td>1678.108287</td>\n",
       "      <td>964.516236</td>\n",
       "      <td>989.247998</td>\n",
       "      <td>968.499790</td>\n",
       "      <td>138.022717</td>\n",
       "      <td>1512.815238</td>\n",
       "      <td>849.518585</td>\n",
       "      <td>957.796820</td>\n",
       "      <td>730.217385</td>\n",
       "      <td>157.638490</td>\n",
       "      <td>1193.064950</td>\n",
       "      <td>705.971428</td>\n",
       "      <td>648.232958</td>\n",
       "      <td>581.856549</td>\n",
       "      <td>128.978108</td>\n",
       "      <td>2072.009750</td>\n",
       "      <td>1053.208172</td>\n",
       "      <td>795.622787</td>\n",
       "      <td>675.250085</td>\n",
       "      <td>167.663108</td>\n",
       "      <td>1737.261852</td>\n",
       "      <td>1075.516124</td>\n",
       "      <td>758.877244</td>\n",
       "      <td>1315.218595</td>\n",
       "      <td>85.620275</td>\n",
       "      <td>2014.907939</td>\n",
       "      <td>1045.126448</td>\n",
       "      <td>679.225013</td>\n",
       "      <td>1392.514859</td>\n",
       "      <td>128.924136</td>\n",
       "      <td>1459.507358</td>\n",
       "      <td>1081.735389</td>\n",
       "      <td>853.304644</td>\n",
       "      <td>1008.100736</td>\n",
       "      <td>287.471715</td>\n",
       "      <td>1588.010936</td>\n",
       "      <td>1342.419097</td>\n",
       "      <td>689.104182</td>\n",
       "      <td>1008.266888</td>\n",
       "      <td>237.521150</td>\n",
       "      <td>1724.381262</td>\n",
       "      <td>1164.161906</td>\n",
       "      <td>695.856885</td>\n",
       "      <td>875.675110</td>\n",
       "      <td>436.251403</td>\n",
       "      <td>1397.172065</td>\n",
       "      <td>1012.874024</td>\n",
       "      <td>809.841018</td>\n",
       "      <td>745.851272</td>\n",
       "      <td>204.838749</td>\n",
       "      <td>1710.948437</td>\n",
       "      <td>1128.137211</td>\n",
       "      <td>912.992413</td>\n",
       "      <td>1039.179102</td>\n",
       "      <td>86.181027</td>\n",
       "      <td>1546.708333</td>\n",
       "      <td>986.628413</td>\n",
       "      <td>876.599937</td>\n",
       "      <td>997.347209</td>\n",
       "      <td>244.353918</td>\n",
       "      <td>2057.569881</td>\n",
       "      <td>1161.216557</td>\n",
       "      <td>736.055635</td>\n",
       "      <td>768.684543</td>\n",
       "      <td>240.153545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1497.247865</td>\n",
       "      <td>851.437981</td>\n",
       "      <td>959.506731</td>\n",
       "      <td>882.725062</td>\n",
       "      <td>260.333692</td>\n",
       "      <td>1537.752623</td>\n",
       "      <td>911.636878</td>\n",
       "      <td>936.074910</td>\n",
       "      <td>956.197358</td>\n",
       "      <td>383.036705</td>\n",
       "      <td>1466.166625</td>\n",
       "      <td>821.374869</td>\n",
       "      <td>915.590645</td>\n",
       "      <td>718.993460</td>\n",
       "      <td>264.939008</td>\n",
       "      <td>1153.478859</td>\n",
       "      <td>727.930612</td>\n",
       "      <td>662.914916</td>\n",
       "      <td>579.770036</td>\n",
       "      <td>38.336660</td>\n",
       "      <td>1981.945626</td>\n",
       "      <td>1015.629848</td>\n",
       "      <td>736.276584</td>\n",
       "      <td>643.631106</td>\n",
       "      <td>100.856273</td>\n",
       "      <td>1716.463882</td>\n",
       "      <td>1048.936441</td>\n",
       "      <td>850.745935</td>\n",
       "      <td>1307.695070</td>\n",
       "      <td>269.783889</td>\n",
       "      <td>2022.015829</td>\n",
       "      <td>985.740043</td>\n",
       "      <td>639.632709</td>\n",
       "      <td>1213.912870</td>\n",
       "      <td>155.828922</td>\n",
       "      <td>1426.565363</td>\n",
       "      <td>1071.744516</td>\n",
       "      <td>809.549044</td>\n",
       "      <td>900.042874</td>\n",
       "      <td>187.749513</td>\n",
       "      <td>1606.090444</td>\n",
       "      <td>1333.141882</td>\n",
       "      <td>680.583627</td>\n",
       "      <td>953.088303</td>\n",
       "      <td>126.407193</td>\n",
       "      <td>1566.081137</td>\n",
       "      <td>1195.820948</td>\n",
       "      <td>698.178600</td>\n",
       "      <td>881.870647</td>\n",
       "      <td>96.490325</td>\n",
       "      <td>1352.822208</td>\n",
       "      <td>1003.004396</td>\n",
       "      <td>714.490625</td>\n",
       "      <td>689.309734</td>\n",
       "      <td>288.547656</td>\n",
       "      <td>1733.662003</td>\n",
       "      <td>1032.301365</td>\n",
       "      <td>864.132934</td>\n",
       "      <td>1044.879723</td>\n",
       "      <td>295.031080</td>\n",
       "      <td>1452.084801</td>\n",
       "      <td>914.005947</td>\n",
       "      <td>814.784673</td>\n",
       "      <td>1033.127883</td>\n",
       "      <td>229.448013</td>\n",
       "      <td>1989.591808</td>\n",
       "      <td>1069.121464</td>\n",
       "      <td>686.572034</td>\n",
       "      <td>697.596546</td>\n",
       "      <td>50.708541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1314.476209</td>\n",
       "      <td>919.669964</td>\n",
       "      <td>983.308354</td>\n",
       "      <td>873.276553</td>\n",
       "      <td>352.814075</td>\n",
       "      <td>1351.133996</td>\n",
       "      <td>1007.996757</td>\n",
       "      <td>1000.436238</td>\n",
       "      <td>982.752578</td>\n",
       "      <td>241.910344</td>\n",
       "      <td>1365.528446</td>\n",
       "      <td>836.299082</td>\n",
       "      <td>896.955110</td>\n",
       "      <td>770.991880</td>\n",
       "      <td>335.313540</td>\n",
       "      <td>1153.619664</td>\n",
       "      <td>677.503778</td>\n",
       "      <td>656.333489</td>\n",
       "      <td>650.367034</td>\n",
       "      <td>186.358504</td>\n",
       "      <td>1988.754148</td>\n",
       "      <td>930.684804</td>\n",
       "      <td>716.361353</td>\n",
       "      <td>678.678956</td>\n",
       "      <td>217.714789</td>\n",
       "      <td>1671.694613</td>\n",
       "      <td>1029.471761</td>\n",
       "      <td>715.351107</td>\n",
       "      <td>1309.754926</td>\n",
       "      <td>717.462493</td>\n",
       "      <td>1796.784555</td>\n",
       "      <td>970.935291</td>\n",
       "      <td>729.876409</td>\n",
       "      <td>1299.443289</td>\n",
       "      <td>833.285292</td>\n",
       "      <td>1333.225115</td>\n",
       "      <td>971.413051</td>\n",
       "      <td>926.535011</td>\n",
       "      <td>1008.014232</td>\n",
       "      <td>617.743786</td>\n",
       "      <td>1474.792479</td>\n",
       "      <td>1304.231857</td>\n",
       "      <td>751.054540</td>\n",
       "      <td>977.279729</td>\n",
       "      <td>528.974133</td>\n",
       "      <td>1392.898948</td>\n",
       "      <td>1160.822947</td>\n",
       "      <td>851.801058</td>\n",
       "      <td>962.550420</td>\n",
       "      <td>604.932454</td>\n",
       "      <td>1302.503049</td>\n",
       "      <td>979.466268</td>\n",
       "      <td>721.727814</td>\n",
       "      <td>813.394808</td>\n",
       "      <td>453.678913</td>\n",
       "      <td>1667.357360</td>\n",
       "      <td>990.644883</td>\n",
       "      <td>944.854638</td>\n",
       "      <td>1118.501809</td>\n",
       "      <td>355.815490</td>\n",
       "      <td>1433.935229</td>\n",
       "      <td>901.104793</td>\n",
       "      <td>794.645651</td>\n",
       "      <td>1000.369776</td>\n",
       "      <td>199.378771</td>\n",
       "      <td>1739.514883</td>\n",
       "      <td>1011.118559</td>\n",
       "      <td>824.067509</td>\n",
       "      <td>737.803041</td>\n",
       "      <td>496.043021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57091</th>\n",
       "      <td>2941.466671</td>\n",
       "      <td>1667.990021</td>\n",
       "      <td>803.598803</td>\n",
       "      <td>1727.582956</td>\n",
       "      <td>223.909796</td>\n",
       "      <td>1904.887696</td>\n",
       "      <td>1012.261170</td>\n",
       "      <td>516.610274</td>\n",
       "      <td>882.058539</td>\n",
       "      <td>161.676784</td>\n",
       "      <td>2329.456370</td>\n",
       "      <td>834.570396</td>\n",
       "      <td>1014.672026</td>\n",
       "      <td>2062.871611</td>\n",
       "      <td>603.657936</td>\n",
       "      <td>7730.640440</td>\n",
       "      <td>4254.998160</td>\n",
       "      <td>2826.165833</td>\n",
       "      <td>4532.372657</td>\n",
       "      <td>1181.350465</td>\n",
       "      <td>11851.364825</td>\n",
       "      <td>5849.113754</td>\n",
       "      <td>3922.455626</td>\n",
       "      <td>5956.682345</td>\n",
       "      <td>1892.403868</td>\n",
       "      <td>7867.274591</td>\n",
       "      <td>3859.253942</td>\n",
       "      <td>2841.091409</td>\n",
       "      <td>4132.475928</td>\n",
       "      <td>1581.404059</td>\n",
       "      <td>1899.968241</td>\n",
       "      <td>1662.832729</td>\n",
       "      <td>892.079241</td>\n",
       "      <td>1560.426509</td>\n",
       "      <td>255.658668</td>\n",
       "      <td>2901.713634</td>\n",
       "      <td>861.218871</td>\n",
       "      <td>1088.280208</td>\n",
       "      <td>1284.597648</td>\n",
       "      <td>633.105094</td>\n",
       "      <td>4408.827750</td>\n",
       "      <td>2451.523060</td>\n",
       "      <td>1451.495616</td>\n",
       "      <td>2464.771892</td>\n",
       "      <td>332.211064</td>\n",
       "      <td>5440.239383</td>\n",
       "      <td>3054.575827</td>\n",
       "      <td>1612.544749</td>\n",
       "      <td>3353.728242</td>\n",
       "      <td>351.599124</td>\n",
       "      <td>6277.604858</td>\n",
       "      <td>3551.471387</td>\n",
       "      <td>2340.519096</td>\n",
       "      <td>3829.785758</td>\n",
       "      <td>600.896223</td>\n",
       "      <td>5789.363898</td>\n",
       "      <td>3018.579331</td>\n",
       "      <td>2112.325093</td>\n",
       "      <td>2876.215631</td>\n",
       "      <td>1081.985534</td>\n",
       "      <td>2627.995444</td>\n",
       "      <td>1496.161376</td>\n",
       "      <td>917.761482</td>\n",
       "      <td>1594.510611</td>\n",
       "      <td>326.740728</td>\n",
       "      <td>2422.375347</td>\n",
       "      <td>1534.229437</td>\n",
       "      <td>898.993497</td>\n",
       "      <td>1709.510794</td>\n",
       "      <td>644.181206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57092</th>\n",
       "      <td>3049.738693</td>\n",
       "      <td>1498.095234</td>\n",
       "      <td>709.837644</td>\n",
       "      <td>1641.664859</td>\n",
       "      <td>126.113945</td>\n",
       "      <td>1926.276835</td>\n",
       "      <td>977.730550</td>\n",
       "      <td>478.385776</td>\n",
       "      <td>835.131073</td>\n",
       "      <td>243.453441</td>\n",
       "      <td>2255.005842</td>\n",
       "      <td>830.259705</td>\n",
       "      <td>922.540548</td>\n",
       "      <td>2051.801919</td>\n",
       "      <td>701.898978</td>\n",
       "      <td>7612.554415</td>\n",
       "      <td>3798.358123</td>\n",
       "      <td>2607.958201</td>\n",
       "      <td>4463.401268</td>\n",
       "      <td>1129.233681</td>\n",
       "      <td>11042.612050</td>\n",
       "      <td>5331.398725</td>\n",
       "      <td>3568.298746</td>\n",
       "      <td>6000.337323</td>\n",
       "      <td>1979.858357</td>\n",
       "      <td>7586.804896</td>\n",
       "      <td>3463.597899</td>\n",
       "      <td>2649.746840</td>\n",
       "      <td>4067.730575</td>\n",
       "      <td>1438.938103</td>\n",
       "      <td>1762.120648</td>\n",
       "      <td>1485.765184</td>\n",
       "      <td>989.421496</td>\n",
       "      <td>1624.633771</td>\n",
       "      <td>112.916309</td>\n",
       "      <td>2838.112952</td>\n",
       "      <td>811.576339</td>\n",
       "      <td>1062.598154</td>\n",
       "      <td>1258.507007</td>\n",
       "      <td>75.040603</td>\n",
       "      <td>4464.729766</td>\n",
       "      <td>2253.233501</td>\n",
       "      <td>1530.158857</td>\n",
       "      <td>2403.471101</td>\n",
       "      <td>938.116051</td>\n",
       "      <td>5522.155020</td>\n",
       "      <td>2802.163159</td>\n",
       "      <td>1565.457287</td>\n",
       "      <td>3269.305026</td>\n",
       "      <td>446.315241</td>\n",
       "      <td>6239.292645</td>\n",
       "      <td>3211.537852</td>\n",
       "      <td>2188.280674</td>\n",
       "      <td>3733.732488</td>\n",
       "      <td>546.093127</td>\n",
       "      <td>5562.775653</td>\n",
       "      <td>2724.468844</td>\n",
       "      <td>1928.284555</td>\n",
       "      <td>2867.457303</td>\n",
       "      <td>1365.259508</td>\n",
       "      <td>2697.377552</td>\n",
       "      <td>1423.471864</td>\n",
       "      <td>994.804976</td>\n",
       "      <td>1580.707942</td>\n",
       "      <td>692.703520</td>\n",
       "      <td>2381.595408</td>\n",
       "      <td>1343.168243</td>\n",
       "      <td>1004.720599</td>\n",
       "      <td>1693.348616</td>\n",
       "      <td>225.530345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57093</th>\n",
       "      <td>2964.787119</td>\n",
       "      <td>1472.638330</td>\n",
       "      <td>636.652963</td>\n",
       "      <td>1655.984762</td>\n",
       "      <td>617.598199</td>\n",
       "      <td>1899.897030</td>\n",
       "      <td>978.312992</td>\n",
       "      <td>485.249539</td>\n",
       "      <td>796.081759</td>\n",
       "      <td>65.974175</td>\n",
       "      <td>2120.976800</td>\n",
       "      <td>852.974457</td>\n",
       "      <td>790.928173</td>\n",
       "      <td>2058.591625</td>\n",
       "      <td>294.574250</td>\n",
       "      <td>7245.308064</td>\n",
       "      <td>3900.643936</td>\n",
       "      <td>2636.998506</td>\n",
       "      <td>4789.524289</td>\n",
       "      <td>2225.875812</td>\n",
       "      <td>10794.910799</td>\n",
       "      <td>5571.961214</td>\n",
       "      <td>3656.185844</td>\n",
       "      <td>6773.077563</td>\n",
       "      <td>3824.449318</td>\n",
       "      <td>7290.709109</td>\n",
       "      <td>3659.630504</td>\n",
       "      <td>2627.019335</td>\n",
       "      <td>4631.418874</td>\n",
       "      <td>2078.972358</td>\n",
       "      <td>1726.510697</td>\n",
       "      <td>1378.026682</td>\n",
       "      <td>937.677083</td>\n",
       "      <td>1660.196278</td>\n",
       "      <td>226.671523</td>\n",
       "      <td>2799.797873</td>\n",
       "      <td>828.934131</td>\n",
       "      <td>1038.490322</td>\n",
       "      <td>1361.561641</td>\n",
       "      <td>315.312045</td>\n",
       "      <td>4440.837479</td>\n",
       "      <td>2316.938788</td>\n",
       "      <td>1509.524252</td>\n",
       "      <td>2549.313937</td>\n",
       "      <td>873.527948</td>\n",
       "      <td>5436.673409</td>\n",
       "      <td>2871.387178</td>\n",
       "      <td>1696.023449</td>\n",
       "      <td>3480.744218</td>\n",
       "      <td>1317.570310</td>\n",
       "      <td>6145.241668</td>\n",
       "      <td>3348.579686</td>\n",
       "      <td>2296.245907</td>\n",
       "      <td>4077.755911</td>\n",
       "      <td>1845.087258</td>\n",
       "      <td>5327.450358</td>\n",
       "      <td>2767.390881</td>\n",
       "      <td>1848.563909</td>\n",
       "      <td>3178.313652</td>\n",
       "      <td>1581.932751</td>\n",
       "      <td>2678.347948</td>\n",
       "      <td>1383.908050</td>\n",
       "      <td>950.089451</td>\n",
       "      <td>1522.653417</td>\n",
       "      <td>189.358511</td>\n",
       "      <td>2321.112019</td>\n",
       "      <td>1227.066058</td>\n",
       "      <td>909.301931</td>\n",
       "      <td>1690.580232</td>\n",
       "      <td>312.744441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57094</th>\n",
       "      <td>2970.340973</td>\n",
       "      <td>1545.754969</td>\n",
       "      <td>630.235332</td>\n",
       "      <td>1581.842738</td>\n",
       "      <td>328.654513</td>\n",
       "      <td>1850.154481</td>\n",
       "      <td>1026.533202</td>\n",
       "      <td>556.135042</td>\n",
       "      <td>826.079311</td>\n",
       "      <td>140.041788</td>\n",
       "      <td>2047.745140</td>\n",
       "      <td>812.490763</td>\n",
       "      <td>827.463933</td>\n",
       "      <td>2086.080849</td>\n",
       "      <td>204.589545</td>\n",
       "      <td>7599.928994</td>\n",
       "      <td>3958.605020</td>\n",
       "      <td>2526.275357</td>\n",
       "      <td>4554.689620</td>\n",
       "      <td>664.178331</td>\n",
       "      <td>11257.106390</td>\n",
       "      <td>5470.737652</td>\n",
       "      <td>3339.456111</td>\n",
       "      <td>6383.835989</td>\n",
       "      <td>1221.166088</td>\n",
       "      <td>7587.931172</td>\n",
       "      <td>3654.391917</td>\n",
       "      <td>2481.799379</td>\n",
       "      <td>4470.943698</td>\n",
       "      <td>769.083353</td>\n",
       "      <td>1759.137834</td>\n",
       "      <td>1376.234240</td>\n",
       "      <td>1037.159789</td>\n",
       "      <td>1647.487576</td>\n",
       "      <td>46.250983</td>\n",
       "      <td>2632.839528</td>\n",
       "      <td>823.878363</td>\n",
       "      <td>1009.373095</td>\n",
       "      <td>1357.155951</td>\n",
       "      <td>199.131039</td>\n",
       "      <td>4570.146339</td>\n",
       "      <td>2283.875139</td>\n",
       "      <td>1467.025544</td>\n",
       "      <td>2468.728980</td>\n",
       "      <td>408.595949</td>\n",
       "      <td>5614.428549</td>\n",
       "      <td>2822.993159</td>\n",
       "      <td>1638.810974</td>\n",
       "      <td>3438.201533</td>\n",
       "      <td>939.672695</td>\n",
       "      <td>6335.009774</td>\n",
       "      <td>3320.133820</td>\n",
       "      <td>2226.389953</td>\n",
       "      <td>3877.464068</td>\n",
       "      <td>1032.388436</td>\n",
       "      <td>5512.096268</td>\n",
       "      <td>2790.972849</td>\n",
       "      <td>1780.718153</td>\n",
       "      <td>3010.526897</td>\n",
       "      <td>181.907899</td>\n",
       "      <td>2648.255510</td>\n",
       "      <td>1455.751825</td>\n",
       "      <td>917.489586</td>\n",
       "      <td>1486.280766</td>\n",
       "      <td>191.773088</td>\n",
       "      <td>2354.550619</td>\n",
       "      <td>1168.893927</td>\n",
       "      <td>971.752502</td>\n",
       "      <td>1694.133270</td>\n",
       "      <td>253.219807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57095</th>\n",
       "      <td>2868.539146</td>\n",
       "      <td>1433.048874</td>\n",
       "      <td>630.472243</td>\n",
       "      <td>1443.779869</td>\n",
       "      <td>83.912603</td>\n",
       "      <td>1779.971062</td>\n",
       "      <td>1036.209900</td>\n",
       "      <td>553.687620</td>\n",
       "      <td>838.453628</td>\n",
       "      <td>114.017750</td>\n",
       "      <td>1962.326251</td>\n",
       "      <td>784.111216</td>\n",
       "      <td>711.166610</td>\n",
       "      <td>1906.783877</td>\n",
       "      <td>769.836785</td>\n",
       "      <td>7731.654868</td>\n",
       "      <td>3514.384166</td>\n",
       "      <td>2240.404893</td>\n",
       "      <td>4434.779661</td>\n",
       "      <td>1647.096008</td>\n",
       "      <td>11371.759230</td>\n",
       "      <td>4715.944475</td>\n",
       "      <td>3052.976861</td>\n",
       "      <td>6268.211104</td>\n",
       "      <td>2686.397938</td>\n",
       "      <td>7722.093858</td>\n",
       "      <td>3100.480503</td>\n",
       "      <td>2201.875922</td>\n",
       "      <td>4270.262758</td>\n",
       "      <td>1845.209966</td>\n",
       "      <td>1771.649683</td>\n",
       "      <td>1354.710392</td>\n",
       "      <td>1020.956832</td>\n",
       "      <td>1671.372849</td>\n",
       "      <td>214.114137</td>\n",
       "      <td>2631.650029</td>\n",
       "      <td>839.502896</td>\n",
       "      <td>1004.017526</td>\n",
       "      <td>1301.560623</td>\n",
       "      <td>254.120765</td>\n",
       "      <td>4395.261834</td>\n",
       "      <td>1844.036193</td>\n",
       "      <td>1268.548534</td>\n",
       "      <td>2449.589618</td>\n",
       "      <td>1043.599681</td>\n",
       "      <td>5290.894464</td>\n",
       "      <td>2500.387335</td>\n",
       "      <td>1419.444042</td>\n",
       "      <td>3402.041151</td>\n",
       "      <td>1264.084196</td>\n",
       "      <td>6001.801726</td>\n",
       "      <td>2787.469318</td>\n",
       "      <td>1916.481744</td>\n",
       "      <td>3757.103923</td>\n",
       "      <td>1822.346656</td>\n",
       "      <td>5827.367447</td>\n",
       "      <td>2429.079207</td>\n",
       "      <td>1562.361165</td>\n",
       "      <td>2933.645374</td>\n",
       "      <td>1259.050706</td>\n",
       "      <td>2492.285071</td>\n",
       "      <td>1279.431597</td>\n",
       "      <td>872.320766</td>\n",
       "      <td>1428.854053</td>\n",
       "      <td>232.031419</td>\n",
       "      <td>2264.869749</td>\n",
       "      <td>1192.679350</td>\n",
       "      <td>943.719858</td>\n",
       "      <td>1623.675020</td>\n",
       "      <td>266.491813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57096 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1  ...           68          69\n",
       "0      1467.141307   957.354712  ...   722.807957  218.773480\n",
       "1      1402.080988   872.471171  ...   768.601395  359.695638\n",
       "2      1569.859186   895.845443  ...   768.684543  240.153545\n",
       "3      1497.247865   851.437981  ...   697.596546   50.708541\n",
       "4      1314.476209   919.669964  ...   737.803041  496.043021\n",
       "...            ...          ...  ...          ...         ...\n",
       "57091  2941.466671  1667.990021  ...  1709.510794  644.181206\n",
       "57092  3049.738693  1498.095234  ...  1693.348616  225.530345\n",
       "57093  2964.787119  1472.638330  ...  1690.580232  312.744441\n",
       "57094  2970.340973  1545.754969  ...  1694.133270  253.219807\n",
       "57095  2868.539146  1433.048874  ...  1623.675020  266.491813\n",
       "\n",
       "[57096 rows x 70 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "6tjKhClBBvnB",
    "outputId": "adb2fd23-9404-43f3-c0b9-36e96ee1720f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.71</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.71</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.71</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.71</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.71</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57091</th>\n",
       "      <td>4.33</td>\n",
       "      <td>6.21</td>\n",
       "      <td>5.68</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57092</th>\n",
       "      <td>4.33</td>\n",
       "      <td>6.21</td>\n",
       "      <td>5.68</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57093</th>\n",
       "      <td>4.33</td>\n",
       "      <td>6.21</td>\n",
       "      <td>5.68</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57094</th>\n",
       "      <td>4.33</td>\n",
       "      <td>6.21</td>\n",
       "      <td>5.68</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57095</th>\n",
       "      <td>4.33</td>\n",
       "      <td>6.21</td>\n",
       "      <td>5.68</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57096 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1     2     3\n",
       "0      7.71  7.60  6.90  7.83\n",
       "1      7.71  7.60  6.90  7.83\n",
       "2      7.71  7.60  6.90  7.83\n",
       "3      7.71  7.60  6.90  7.83\n",
       "4      7.71  7.60  6.90  7.83\n",
       "...     ...   ...   ...   ...\n",
       "57091  4.33  6.21  5.68  4.62\n",
       "57092  4.33  6.21  5.68  4.62\n",
       "57093  4.33  6.21  5.68  4.62\n",
       "57094  4.33  6.21  5.68  4.62\n",
       "57095  4.33  6.21  5.68  4.62\n",
       "\n",
       "[57096 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label1=pd.read_csv(\"/content/label.csv\")\n",
    "label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vHNFxKNCzVA"
   },
   "outputs": [],
   "source": [
    "x=data1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "colab_type": "code",
    "id": "KkOVzVQmDPw8",
    "outputId": "0a73601f-5438-4df1-eb63-c6d084094455"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label1.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4K0sRcyD7v-"
   },
   "outputs": [],
   "source": [
    "y_val=label1.loc[:,'0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "JIvAMkCWCX93",
    "outputId": "5cbcbc6b-0e6d-40b0-d403-e891bc0f1a5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.71, 8.1 , 8.58, 4.94, 6.96, 8.27, 7.44, 7.32, 4.04, 1.99, 2.99,\n",
       "       2.71, 1.95, 4.18, 3.17, 6.81, 2.46, 7.23, 7.17, 8.26, 9.  , 7.09,\n",
       "       8.15, 7.04, 8.86, 7.28, 7.35, 3.88, 1.36, 2.08, 3.03, 2.28, 3.81,\n",
       "       2.06, 2.9 , 2.31, 3.33, 3.24, 8.01, 6.05, 5.04, 5.  , 4.96, 4.99,\n",
       "       7.08, 8.94, 6.  , 8.97, 1.  , 4.06, 2.01, 4.87, 5.33, 7.21, 7.55,\n",
       "       4.69, 6.92, 6.79, 5.45, 7.14, 7.33, 4.45, 7.94, 6.36, 7.91, 7.29,\n",
       "       7.36, 7.15, 4.56, 7.1 , 8.14, 6.26, 3.65, 3.31, 3.45, 4.67, 6.72,\n",
       "       5.59, 4.47, 3.87, 4.44, 5.17, 4.05, 4.72, 3.18, 5.01, 4.92, 4.53,\n",
       "       4.33])"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "c9tQ8UKNfdtI",
    "outputId": "443e457d-ceec-43ae-cb4b-0ba11e055096"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        7.71\n",
       "1        7.71\n",
       "2        7.71\n",
       "3        7.71\n",
       "4        7.71\n",
       "         ... \n",
       "57091    4.33\n",
       "57092    4.33\n",
       "57093    4.33\n",
       "57094    4.33\n",
       "57095    4.33\n",
       "Name: 0, Length: 57096, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "u6OfYsu5oina",
    "outputId": "ee463354-0c37-405c-9494-ed461619e91a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.71, 7.71, 7.71, ..., 4.33, 4.33, 4.33])"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "xGLKl-9Zfjaf",
    "outputId": "4fa0b166-ccac-41bd-9582-399e24934e52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y_val)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "colab_type": "code",
    "id": "DQui7-n9Usd3",
    "outputId": "6c250ee0-7487-4f64-9f0c-ccbe0a666c85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.56079892, -0.6150587 , -0.55497708, ..., -0.41330096,\n",
       "        -1.32299145, -0.40788506],\n",
       "       [-0.56646156, -0.64999505, -0.51476463, ..., -0.31285867,\n",
       "        -1.23765226,  0.07824073],\n",
       "       [-0.55185869, -0.64037467, -0.52947627, ..., -0.43186851,\n",
       "        -1.23749731, -0.33413228],\n",
       "       ...,\n",
       "       [-0.43044874, -0.40297836, -0.69989972, ...,  0.38061925,\n",
       "         0.48051812, -0.08372236],\n",
       "       [-0.42996535, -0.37288503, -0.70264449, ...,  0.67349896,\n",
       "         0.48713945, -0.28905884],\n",
       "       [-0.43882585, -0.41927259, -0.70254317, ...,  0.5420319 ,\n",
       "         0.3558357 , -0.24327566]])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zru43-IYfvMO"
   },
   "outputs": [],
   "source": [
    "x = np.reshape(x, (x.shape[0],1,x.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "PlVSh-hs3G9M",
    "outputId": "04301f4e-4c56-493c-ec13-0dfa6e7ff0a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57096,)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "Sh3ai8VD1m1a",
    "outputId": "11c9bcfa-c779-4152-95a9-80410d42d98e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57096, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "1TQMW0F7nYL-",
    "outputId": "4fb69115-c565-4d5a-d270-852c89adfe02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EaKwv9nFgEX0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "FRJ5Fu1RmCdj",
    "outputId": "025bf5d0-37e0-4046-e5e5-d4cac40a3754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45676, 1, 70)\n",
      "(45676, 10)\n",
      "(11420, 1, 70)\n",
      "(11420, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "evd-F_sl3wPL",
    "outputId": "1e81b0c5-6407-483c-be5a-71e324891013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 512)         1193984   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 256)         787456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 128)         197120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, None, 64)          49408     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 2,244,682\n",
      "Trainable params: 2,242,698\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM,BatchNormalization,Activation\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, batch_input_shape = (None, None, x.shape[2]),return_sequences=True))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(LSTM(256,activation=\"relu\",return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(LSTM(128,activation=\"relu\",return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(LSTM(64,activation=\"relu\",return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "model.add(LSTM(32,activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "rmsprop =keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08)\n",
    "model.compile(loss='mean_squared_error',\n",
    "                  optimizer=rmsprop,\n",
    "                  metrics=['accuracy'])\n",
    "#adam = keras.optimizers.Adam(lr=0.5, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6aNe2-s1gRTb",
    "outputId": "82dfb09f-03d9-4059-c994-7e1edafe230d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45676 samples, validate on 11420 samples\n",
      "Epoch 1/900\n",
      "45676/45676 [==============================] - 2s 55us/step - loss: 0.0876 - accuracy: 0.2548 - val_loss: 0.0848 - val_accuracy: 0.2664\n",
      "Epoch 2/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0827 - accuracy: 0.2948 - val_loss: 0.0816 - val_accuracy: 0.3084\n",
      "Epoch 3/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0806 - accuracy: 0.3210 - val_loss: 0.0800 - val_accuracy: 0.3390\n",
      "Epoch 4/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0791 - accuracy: 0.3479 - val_loss: 0.0785 - val_accuracy: 0.3584\n",
      "Epoch 5/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0777 - accuracy: 0.3635 - val_loss: 0.0773 - val_accuracy: 0.3633\n",
      "Epoch 6/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0768 - accuracy: 0.3687 - val_loss: 0.0765 - val_accuracy: 0.3705\n",
      "Epoch 7/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0761 - accuracy: 0.3742 - val_loss: 0.0758 - val_accuracy: 0.3754\n",
      "Epoch 8/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0755 - accuracy: 0.3786 - val_loss: 0.0753 - val_accuracy: 0.3774\n",
      "Epoch 9/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0750 - accuracy: 0.3828 - val_loss: 0.0748 - val_accuracy: 0.3799\n",
      "Epoch 10/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0745 - accuracy: 0.3860 - val_loss: 0.0743 - val_accuracy: 0.3856\n",
      "Epoch 11/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0740 - accuracy: 0.3894 - val_loss: 0.0739 - val_accuracy: 0.3872\n",
      "Epoch 12/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0736 - accuracy: 0.3947 - val_loss: 0.0735 - val_accuracy: 0.3912\n",
      "Epoch 13/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0733 - accuracy: 0.3975 - val_loss: 0.0731 - val_accuracy: 0.3929\n",
      "Epoch 14/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0729 - accuracy: 0.4025 - val_loss: 0.0727 - val_accuracy: 0.3973\n",
      "Epoch 15/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0725 - accuracy: 0.4079 - val_loss: 0.0723 - val_accuracy: 0.4061\n",
      "Epoch 16/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0721 - accuracy: 0.4139 - val_loss: 0.0719 - val_accuracy: 0.4089\n",
      "Epoch 17/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0717 - accuracy: 0.4183 - val_loss: 0.0716 - val_accuracy: 0.4119\n",
      "Epoch 18/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0713 - accuracy: 0.4234 - val_loss: 0.0712 - val_accuracy: 0.4183\n",
      "Epoch 19/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0710 - accuracy: 0.4274 - val_loss: 0.0708 - val_accuracy: 0.4236\n",
      "Epoch 20/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0707 - accuracy: 0.4311 - val_loss: 0.0705 - val_accuracy: 0.4297\n",
      "Epoch 21/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0703 - accuracy: 0.4372 - val_loss: 0.0701 - val_accuracy: 0.4362\n",
      "Epoch 22/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0700 - accuracy: 0.4391 - val_loss: 0.0698 - val_accuracy: 0.4383\n",
      "Epoch 23/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0696 - accuracy: 0.4456 - val_loss: 0.0695 - val_accuracy: 0.4447\n",
      "Epoch 24/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0693 - accuracy: 0.4465 - val_loss: 0.0691 - val_accuracy: 0.4496\n",
      "Epoch 25/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0690 - accuracy: 0.4490 - val_loss: 0.0689 - val_accuracy: 0.4487\n",
      "Epoch 26/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0687 - accuracy: 0.4537 - val_loss: 0.0686 - val_accuracy: 0.4511\n",
      "Epoch 27/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0684 - accuracy: 0.4546 - val_loss: 0.0683 - val_accuracy: 0.4546\n",
      "Epoch 28/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0682 - accuracy: 0.4543 - val_loss: 0.0681 - val_accuracy: 0.4581\n",
      "Epoch 29/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0679 - accuracy: 0.4612 - val_loss: 0.0678 - val_accuracy: 0.4611\n",
      "Epoch 30/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0677 - accuracy: 0.4608 - val_loss: 0.0675 - val_accuracy: 0.4640\n",
      "Epoch 31/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0674 - accuracy: 0.4647 - val_loss: 0.0674 - val_accuracy: 0.4645\n",
      "Epoch 32/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0672 - accuracy: 0.4677 - val_loss: 0.0671 - val_accuracy: 0.4669\n",
      "Epoch 33/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0670 - accuracy: 0.4676 - val_loss: 0.0669 - val_accuracy: 0.4692\n",
      "Epoch 34/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0668 - accuracy: 0.4715 - val_loss: 0.0667 - val_accuracy: 0.4698\n",
      "Epoch 35/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0665 - accuracy: 0.4730 - val_loss: 0.0664 - val_accuracy: 0.4735\n",
      "Epoch 36/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0663 - accuracy: 0.4750 - val_loss: 0.0663 - val_accuracy: 0.4725\n",
      "Epoch 37/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0660 - accuracy: 0.4776 - val_loss: 0.0660 - val_accuracy: 0.4792\n",
      "Epoch 38/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0658 - accuracy: 0.4804 - val_loss: 0.0657 - val_accuracy: 0.4820\n",
      "Epoch 39/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0656 - accuracy: 0.4836 - val_loss: 0.0656 - val_accuracy: 0.4831\n",
      "Epoch 40/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0654 - accuracy: 0.4844 - val_loss: 0.0653 - val_accuracy: 0.4875\n",
      "Epoch 41/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0652 - accuracy: 0.4863 - val_loss: 0.0651 - val_accuracy: 0.4902\n",
      "Epoch 42/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0651 - accuracy: 0.4878 - val_loss: 0.0650 - val_accuracy: 0.4927\n",
      "Epoch 43/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0648 - accuracy: 0.4931 - val_loss: 0.0647 - val_accuracy: 0.4930\n",
      "Epoch 44/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0645 - accuracy: 0.4944 - val_loss: 0.0646 - val_accuracy: 0.4949\n",
      "Epoch 45/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0644 - accuracy: 0.4961 - val_loss: 0.0643 - val_accuracy: 0.4953\n",
      "Epoch 46/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0641 - accuracy: 0.4995 - val_loss: 0.0641 - val_accuracy: 0.5002\n",
      "Epoch 47/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0639 - accuracy: 0.5011 - val_loss: 0.0639 - val_accuracy: 0.5018\n",
      "Epoch 48/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0637 - accuracy: 0.5038 - val_loss: 0.0637 - val_accuracy: 0.5035\n",
      "Epoch 49/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0635 - accuracy: 0.5047 - val_loss: 0.0635 - val_accuracy: 0.5060\n",
      "Epoch 50/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0634 - accuracy: 0.5076 - val_loss: 0.0634 - val_accuracy: 0.5052\n",
      "Epoch 51/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0631 - accuracy: 0.5087 - val_loss: 0.0632 - val_accuracy: 0.5075\n",
      "Epoch 52/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0630 - accuracy: 0.5123 - val_loss: 0.0630 - val_accuracy: 0.5097\n",
      "Epoch 53/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0628 - accuracy: 0.5132 - val_loss: 0.0628 - val_accuracy: 0.5104\n",
      "Epoch 54/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0626 - accuracy: 0.5142 - val_loss: 0.0627 - val_accuracy: 0.5113\n",
      "Epoch 55/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0624 - accuracy: 0.5158 - val_loss: 0.0624 - val_accuracy: 0.5151\n",
      "Epoch 56/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0624 - accuracy: 0.5166 - val_loss: 0.0623 - val_accuracy: 0.5160\n",
      "Epoch 57/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0622 - accuracy: 0.5180 - val_loss: 0.0621 - val_accuracy: 0.5185\n",
      "Epoch 58/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0621 - accuracy: 0.5181 - val_loss: 0.0619 - val_accuracy: 0.5187\n",
      "Epoch 59/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0619 - accuracy: 0.5198 - val_loss: 0.0618 - val_accuracy: 0.5194\n",
      "Epoch 60/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0617 - accuracy: 0.5215 - val_loss: 0.0616 - val_accuracy: 0.5238\n",
      "Epoch 61/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0615 - accuracy: 0.5248 - val_loss: 0.0615 - val_accuracy: 0.5271\n",
      "Epoch 62/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0614 - accuracy: 0.5259 - val_loss: 0.0613 - val_accuracy: 0.5250\n",
      "Epoch 63/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0612 - accuracy: 0.5271 - val_loss: 0.0611 - val_accuracy: 0.5266\n",
      "Epoch 64/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0610 - accuracy: 0.5310 - val_loss: 0.0610 - val_accuracy: 0.5280\n",
      "Epoch 65/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0609 - accuracy: 0.5303 - val_loss: 0.0609 - val_accuracy: 0.5280\n",
      "Epoch 66/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0607 - accuracy: 0.5319 - val_loss: 0.0608 - val_accuracy: 0.5283\n",
      "Epoch 67/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0606 - accuracy: 0.5315 - val_loss: 0.0606 - val_accuracy: 0.5326\n",
      "Epoch 68/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0604 - accuracy: 0.5355 - val_loss: 0.0605 - val_accuracy: 0.5354\n",
      "Epoch 69/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0603 - accuracy: 0.5373 - val_loss: 0.0603 - val_accuracy: 0.5356\n",
      "Epoch 70/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0601 - accuracy: 0.5390 - val_loss: 0.0602 - val_accuracy: 0.5367\n",
      "Epoch 71/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0599 - accuracy: 0.5398 - val_loss: 0.0600 - val_accuracy: 0.5381\n",
      "Epoch 72/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0597 - accuracy: 0.5418 - val_loss: 0.0598 - val_accuracy: 0.5426\n",
      "Epoch 73/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0597 - accuracy: 0.5419 - val_loss: 0.0597 - val_accuracy: 0.5432\n",
      "Epoch 74/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0596 - accuracy: 0.5427 - val_loss: 0.0596 - val_accuracy: 0.5405\n",
      "Epoch 75/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0594 - accuracy: 0.5442 - val_loss: 0.0594 - val_accuracy: 0.5471\n",
      "Epoch 76/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0593 - accuracy: 0.5474 - val_loss: 0.0593 - val_accuracy: 0.5483\n",
      "Epoch 77/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0591 - accuracy: 0.5471 - val_loss: 0.0591 - val_accuracy: 0.5472\n",
      "Epoch 78/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0589 - accuracy: 0.5492 - val_loss: 0.0590 - val_accuracy: 0.5472\n",
      "Epoch 79/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0589 - accuracy: 0.5496 - val_loss: 0.0588 - val_accuracy: 0.5485\n",
      "Epoch 80/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0587 - accuracy: 0.5498 - val_loss: 0.0587 - val_accuracy: 0.5520\n",
      "Epoch 81/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0585 - accuracy: 0.5518 - val_loss: 0.0586 - val_accuracy: 0.5529\n",
      "Epoch 82/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0584 - accuracy: 0.5537 - val_loss: 0.0585 - val_accuracy: 0.5536\n",
      "Epoch 83/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0584 - accuracy: 0.5533 - val_loss: 0.0583 - val_accuracy: 0.5530\n",
      "Epoch 84/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0580 - accuracy: 0.5577 - val_loss: 0.0581 - val_accuracy: 0.5562\n",
      "Epoch 85/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0580 - accuracy: 0.5574 - val_loss: 0.0580 - val_accuracy: 0.5560\n",
      "Epoch 86/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0577 - accuracy: 0.5603 - val_loss: 0.0579 - val_accuracy: 0.5607\n",
      "Epoch 87/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0578 - accuracy: 0.5581 - val_loss: 0.0577 - val_accuracy: 0.5593\n",
      "Epoch 88/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0576 - accuracy: 0.5617 - val_loss: 0.0576 - val_accuracy: 0.5594\n",
      "Epoch 89/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0575 - accuracy: 0.5623 - val_loss: 0.0575 - val_accuracy: 0.5594\n",
      "Epoch 90/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0574 - accuracy: 0.5641 - val_loss: 0.0574 - val_accuracy: 0.5635\n",
      "Epoch 91/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0573 - accuracy: 0.5645 - val_loss: 0.0572 - val_accuracy: 0.5647\n",
      "Epoch 92/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0571 - accuracy: 0.5652 - val_loss: 0.0570 - val_accuracy: 0.5674\n",
      "Epoch 93/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0569 - accuracy: 0.5665 - val_loss: 0.0569 - val_accuracy: 0.5681\n",
      "Epoch 94/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0569 - accuracy: 0.5675 - val_loss: 0.0568 - val_accuracy: 0.5673\n",
      "Epoch 95/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0566 - accuracy: 0.5694 - val_loss: 0.0567 - val_accuracy: 0.5701\n",
      "Epoch 96/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0567 - accuracy: 0.5694 - val_loss: 0.0566 - val_accuracy: 0.5721\n",
      "Epoch 97/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0565 - accuracy: 0.5733 - val_loss: 0.0565 - val_accuracy: 0.5709\n",
      "Epoch 98/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0564 - accuracy: 0.5699 - val_loss: 0.0563 - val_accuracy: 0.5756\n",
      "Epoch 99/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0563 - accuracy: 0.5721 - val_loss: 0.0562 - val_accuracy: 0.5743\n",
      "Epoch 100/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0561 - accuracy: 0.5758 - val_loss: 0.0561 - val_accuracy: 0.5782\n",
      "Epoch 101/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0560 - accuracy: 0.5744 - val_loss: 0.0560 - val_accuracy: 0.5791\n",
      "Epoch 102/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0559 - accuracy: 0.5772 - val_loss: 0.0560 - val_accuracy: 0.5764\n",
      "Epoch 103/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0558 - accuracy: 0.5780 - val_loss: 0.0558 - val_accuracy: 0.5790\n",
      "Epoch 104/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0557 - accuracy: 0.5777 - val_loss: 0.0558 - val_accuracy: 0.5799\n",
      "Epoch 105/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0556 - accuracy: 0.5786 - val_loss: 0.0554 - val_accuracy: 0.5842\n",
      "Epoch 106/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0556 - accuracy: 0.5771 - val_loss: 0.0554 - val_accuracy: 0.5831\n",
      "Epoch 107/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0553 - accuracy: 0.5820 - val_loss: 0.0554 - val_accuracy: 0.5820\n",
      "Epoch 108/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0553 - accuracy: 0.5809 - val_loss: 0.0551 - val_accuracy: 0.5857\n",
      "Epoch 109/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0549 - accuracy: 0.5859 - val_loss: 0.0550 - val_accuracy: 0.5901\n",
      "Epoch 110/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0550 - accuracy: 0.5835 - val_loss: 0.0550 - val_accuracy: 0.5866\n",
      "Epoch 111/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0548 - accuracy: 0.5881 - val_loss: 0.0548 - val_accuracy: 0.5879\n",
      "Epoch 112/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0547 - accuracy: 0.5865 - val_loss: 0.0548 - val_accuracy: 0.5877\n",
      "Epoch 113/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0547 - accuracy: 0.5884 - val_loss: 0.0546 - val_accuracy: 0.5898\n",
      "Epoch 114/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0543 - accuracy: 0.5904 - val_loss: 0.0544 - val_accuracy: 0.5932\n",
      "Epoch 115/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0544 - accuracy: 0.5920 - val_loss: 0.0544 - val_accuracy: 0.5916\n",
      "Epoch 116/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0544 - accuracy: 0.5898 - val_loss: 0.0542 - val_accuracy: 0.5951\n",
      "Epoch 117/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0542 - accuracy: 0.5919 - val_loss: 0.0541 - val_accuracy: 0.5953\n",
      "Epoch 118/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0542 - accuracy: 0.5929 - val_loss: 0.0540 - val_accuracy: 0.5991\n",
      "Epoch 119/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0540 - accuracy: 0.5939 - val_loss: 0.0539 - val_accuracy: 0.5992\n",
      "Epoch 120/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0539 - accuracy: 0.5949 - val_loss: 0.0537 - val_accuracy: 0.5981\n",
      "Epoch 121/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0539 - accuracy: 0.5964 - val_loss: 0.0537 - val_accuracy: 0.5979\n",
      "Epoch 122/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0536 - accuracy: 0.5976 - val_loss: 0.0535 - val_accuracy: 0.6019\n",
      "Epoch 123/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0535 - accuracy: 0.5992 - val_loss: 0.0535 - val_accuracy: 0.6013\n",
      "Epoch 124/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0533 - accuracy: 0.6009 - val_loss: 0.0534 - val_accuracy: 0.6022\n",
      "Epoch 125/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0534 - accuracy: 0.6010 - val_loss: 0.0533 - val_accuracy: 0.6051\n",
      "Epoch 126/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0531 - accuracy: 0.6038 - val_loss: 0.0531 - val_accuracy: 0.6036\n",
      "Epoch 127/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0532 - accuracy: 0.6021 - val_loss: 0.0531 - val_accuracy: 0.6049\n",
      "Epoch 128/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0530 - accuracy: 0.6043 - val_loss: 0.0529 - val_accuracy: 0.6063\n",
      "Epoch 129/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0529 - accuracy: 0.6048 - val_loss: 0.0528 - val_accuracy: 0.6095\n",
      "Epoch 130/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0528 - accuracy: 0.6050 - val_loss: 0.0528 - val_accuracy: 0.6084\n",
      "Epoch 131/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0528 - accuracy: 0.6046 - val_loss: 0.0525 - val_accuracy: 0.6115\n",
      "Epoch 132/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0526 - accuracy: 0.6096 - val_loss: 0.0525 - val_accuracy: 0.6134\n",
      "Epoch 133/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0526 - accuracy: 0.6084 - val_loss: 0.0524 - val_accuracy: 0.6132\n",
      "Epoch 134/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0524 - accuracy: 0.6092 - val_loss: 0.0523 - val_accuracy: 0.6147\n",
      "Epoch 135/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0523 - accuracy: 0.6087 - val_loss: 0.0523 - val_accuracy: 0.6111\n",
      "Epoch 136/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0523 - accuracy: 0.6101 - val_loss: 0.0521 - val_accuracy: 0.6141\n",
      "Epoch 137/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0521 - accuracy: 0.6120 - val_loss: 0.0520 - val_accuracy: 0.6165\n",
      "Epoch 138/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0522 - accuracy: 0.6115 - val_loss: 0.0519 - val_accuracy: 0.6184\n",
      "Epoch 139/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0518 - accuracy: 0.6153 - val_loss: 0.0518 - val_accuracy: 0.6173\n",
      "Epoch 140/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0518 - accuracy: 0.6149 - val_loss: 0.0517 - val_accuracy: 0.6190\n",
      "Epoch 141/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0518 - accuracy: 0.6155 - val_loss: 0.0516 - val_accuracy: 0.6180\n",
      "Epoch 142/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0516 - accuracy: 0.6159 - val_loss: 0.0515 - val_accuracy: 0.6201\n",
      "Epoch 143/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0515 - accuracy: 0.6186 - val_loss: 0.0515 - val_accuracy: 0.6219\n",
      "Epoch 144/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0515 - accuracy: 0.6180 - val_loss: 0.0513 - val_accuracy: 0.6221\n",
      "Epoch 145/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0513 - accuracy: 0.6198 - val_loss: 0.0512 - val_accuracy: 0.6229\n",
      "Epoch 146/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0513 - accuracy: 0.6199 - val_loss: 0.0512 - val_accuracy: 0.6229\n",
      "Epoch 147/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0512 - accuracy: 0.6203 - val_loss: 0.0509 - val_accuracy: 0.6264\n",
      "Epoch 148/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0511 - accuracy: 0.6201 - val_loss: 0.0509 - val_accuracy: 0.6262\n",
      "Epoch 149/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0510 - accuracy: 0.6227 - val_loss: 0.0508 - val_accuracy: 0.6262\n",
      "Epoch 150/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0509 - accuracy: 0.6228 - val_loss: 0.0507 - val_accuracy: 0.6282\n",
      "Epoch 151/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0508 - accuracy: 0.6255 - val_loss: 0.0505 - val_accuracy: 0.6303\n",
      "Epoch 152/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0508 - accuracy: 0.6224 - val_loss: 0.0505 - val_accuracy: 0.6278\n",
      "Epoch 153/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0507 - accuracy: 0.6250 - val_loss: 0.0503 - val_accuracy: 0.6305\n",
      "Epoch 154/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0504 - accuracy: 0.6292 - val_loss: 0.0503 - val_accuracy: 0.6313\n",
      "Epoch 155/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0504 - accuracy: 0.6278 - val_loss: 0.0502 - val_accuracy: 0.6320\n",
      "Epoch 156/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0504 - accuracy: 0.6270 - val_loss: 0.0500 - val_accuracy: 0.6330\n",
      "Epoch 157/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0502 - accuracy: 0.6277 - val_loss: 0.0499 - val_accuracy: 0.6344\n",
      "Epoch 158/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0503 - accuracy: 0.6295 - val_loss: 0.0499 - val_accuracy: 0.6344\n",
      "Epoch 159/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0501 - accuracy: 0.6295 - val_loss: 0.0498 - val_accuracy: 0.6337\n",
      "Epoch 160/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0500 - accuracy: 0.6314 - val_loss: 0.0497 - val_accuracy: 0.6351\n",
      "Epoch 161/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0498 - accuracy: 0.6328 - val_loss: 0.0496 - val_accuracy: 0.6356\n",
      "Epoch 162/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0498 - accuracy: 0.6328 - val_loss: 0.0495 - val_accuracy: 0.6381\n",
      "Epoch 163/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0497 - accuracy: 0.6332 - val_loss: 0.0494 - val_accuracy: 0.6383\n",
      "Epoch 164/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0496 - accuracy: 0.6349 - val_loss: 0.0493 - val_accuracy: 0.6373\n",
      "Epoch 165/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0495 - accuracy: 0.6355 - val_loss: 0.0493 - val_accuracy: 0.6393\n",
      "Epoch 166/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0494 - accuracy: 0.6368 - val_loss: 0.0492 - val_accuracy: 0.6400\n",
      "Epoch 167/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0493 - accuracy: 0.6390 - val_loss: 0.0490 - val_accuracy: 0.6410\n",
      "Epoch 168/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0492 - accuracy: 0.6386 - val_loss: 0.0490 - val_accuracy: 0.6408\n",
      "Epoch 169/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0492 - accuracy: 0.6406 - val_loss: 0.0488 - val_accuracy: 0.6420\n",
      "Epoch 170/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0490 - accuracy: 0.6410 - val_loss: 0.0488 - val_accuracy: 0.6430\n",
      "Epoch 171/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0489 - accuracy: 0.6413 - val_loss: 0.0487 - val_accuracy: 0.6442\n",
      "Epoch 172/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0489 - accuracy: 0.6417 - val_loss: 0.0486 - val_accuracy: 0.6423\n",
      "Epoch 173/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0488 - accuracy: 0.6411 - val_loss: 0.0485 - val_accuracy: 0.6431\n",
      "Epoch 174/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0488 - accuracy: 0.6425 - val_loss: 0.0484 - val_accuracy: 0.6455\n",
      "Epoch 175/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0486 - accuracy: 0.6430 - val_loss: 0.0485 - val_accuracy: 0.6458\n",
      "Epoch 176/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0486 - accuracy: 0.6443 - val_loss: 0.0483 - val_accuracy: 0.6477\n",
      "Epoch 177/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0484 - accuracy: 0.6458 - val_loss: 0.0482 - val_accuracy: 0.6475\n",
      "Epoch 178/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0484 - accuracy: 0.6483 - val_loss: 0.0480 - val_accuracy: 0.6496\n",
      "Epoch 179/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0483 - accuracy: 0.6467 - val_loss: 0.0480 - val_accuracy: 0.6502\n",
      "Epoch 180/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0484 - accuracy: 0.6441 - val_loss: 0.0479 - val_accuracy: 0.6506\n",
      "Epoch 181/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0482 - accuracy: 0.6485 - val_loss: 0.0477 - val_accuracy: 0.6508\n",
      "Epoch 182/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0479 - accuracy: 0.6513 - val_loss: 0.0477 - val_accuracy: 0.6508\n",
      "Epoch 183/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0481 - accuracy: 0.6497 - val_loss: 0.0477 - val_accuracy: 0.6505\n",
      "Epoch 184/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0479 - accuracy: 0.6482 - val_loss: 0.0475 - val_accuracy: 0.6521\n",
      "Epoch 185/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0478 - accuracy: 0.6510 - val_loss: 0.0476 - val_accuracy: 0.6499\n",
      "Epoch 186/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0477 - accuracy: 0.6505 - val_loss: 0.0473 - val_accuracy: 0.6539\n",
      "Epoch 187/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0476 - accuracy: 0.6534 - val_loss: 0.0473 - val_accuracy: 0.6545\n",
      "Epoch 188/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0475 - accuracy: 0.6524 - val_loss: 0.0473 - val_accuracy: 0.6513\n",
      "Epoch 189/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0476 - accuracy: 0.6536 - val_loss: 0.0472 - val_accuracy: 0.6554\n",
      "Epoch 190/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0474 - accuracy: 0.6559 - val_loss: 0.0470 - val_accuracy: 0.6584\n",
      "Epoch 191/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0472 - accuracy: 0.6561 - val_loss: 0.0470 - val_accuracy: 0.6564\n",
      "Epoch 192/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0472 - accuracy: 0.6564 - val_loss: 0.0471 - val_accuracy: 0.6574\n",
      "Epoch 193/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0471 - accuracy: 0.6553 - val_loss: 0.0467 - val_accuracy: 0.6605\n",
      "Epoch 194/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0470 - accuracy: 0.6599 - val_loss: 0.0469 - val_accuracy: 0.6565\n",
      "Epoch 195/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0470 - accuracy: 0.6586 - val_loss: 0.0467 - val_accuracy: 0.6588\n",
      "Epoch 196/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0469 - accuracy: 0.6578 - val_loss: 0.0465 - val_accuracy: 0.6624\n",
      "Epoch 197/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0469 - accuracy: 0.6584 - val_loss: 0.0465 - val_accuracy: 0.6617\n",
      "Epoch 198/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0468 - accuracy: 0.6599 - val_loss: 0.0465 - val_accuracy: 0.6620\n",
      "Epoch 199/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0466 - accuracy: 0.6624 - val_loss: 0.0463 - val_accuracy: 0.6651\n",
      "Epoch 200/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0467 - accuracy: 0.6600 - val_loss: 0.0462 - val_accuracy: 0.6648\n",
      "Epoch 201/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0465 - accuracy: 0.6624 - val_loss: 0.0461 - val_accuracy: 0.6636\n",
      "Epoch 202/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0465 - accuracy: 0.6623 - val_loss: 0.0461 - val_accuracy: 0.6637\n",
      "Epoch 203/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0464 - accuracy: 0.6638 - val_loss: 0.0460 - val_accuracy: 0.6673\n",
      "Epoch 204/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0463 - accuracy: 0.6643 - val_loss: 0.0459 - val_accuracy: 0.6664\n",
      "Epoch 205/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0462 - accuracy: 0.6658 - val_loss: 0.0458 - val_accuracy: 0.6679\n",
      "Epoch 206/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0461 - accuracy: 0.6657 - val_loss: 0.0458 - val_accuracy: 0.6692\n",
      "Epoch 207/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0461 - accuracy: 0.6672 - val_loss: 0.0457 - val_accuracy: 0.6693\n",
      "Epoch 208/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0460 - accuracy: 0.6650 - val_loss: 0.0456 - val_accuracy: 0.6684\n",
      "Epoch 209/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0459 - accuracy: 0.6687 - val_loss: 0.0456 - val_accuracy: 0.6694\n",
      "Epoch 210/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0458 - accuracy: 0.6675 - val_loss: 0.0455 - val_accuracy: 0.6729\n",
      "Epoch 211/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0458 - accuracy: 0.6690 - val_loss: 0.0455 - val_accuracy: 0.6703\n",
      "Epoch 212/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0458 - accuracy: 0.6688 - val_loss: 0.0453 - val_accuracy: 0.6719\n",
      "Epoch 213/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0455 - accuracy: 0.6714 - val_loss: 0.0452 - val_accuracy: 0.6736\n",
      "Epoch 214/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0456 - accuracy: 0.6682 - val_loss: 0.0452 - val_accuracy: 0.6723\n",
      "Epoch 215/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0456 - accuracy: 0.6691 - val_loss: 0.0452 - val_accuracy: 0.6728\n",
      "Epoch 216/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0453 - accuracy: 0.6752 - val_loss: 0.0450 - val_accuracy: 0.6759\n",
      "Epoch 217/900\n",
      "45676/45676 [==============================] - 4s 77us/step - loss: 0.0455 - accuracy: 0.6717 - val_loss: 0.0449 - val_accuracy: 0.6745\n",
      "Epoch 218/900\n",
      "45676/45676 [==============================] - 3s 76us/step - loss: 0.0454 - accuracy: 0.6733 - val_loss: 0.0448 - val_accuracy: 0.6757\n",
      "Epoch 219/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0453 - accuracy: 0.6726 - val_loss: 0.0449 - val_accuracy: 0.6778\n",
      "Epoch 220/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0450 - accuracy: 0.6757 - val_loss: 0.0447 - val_accuracy: 0.6787\n",
      "Epoch 221/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0452 - accuracy: 0.6722 - val_loss: 0.0446 - val_accuracy: 0.6794\n",
      "Epoch 222/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0450 - accuracy: 0.6764 - val_loss: 0.0445 - val_accuracy: 0.6797\n",
      "Epoch 223/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0449 - accuracy: 0.6757 - val_loss: 0.0445 - val_accuracy: 0.6807\n",
      "Epoch 224/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0449 - accuracy: 0.6770 - val_loss: 0.0444 - val_accuracy: 0.6793\n",
      "Epoch 225/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0448 - accuracy: 0.6769 - val_loss: 0.0444 - val_accuracy: 0.6780\n",
      "Epoch 226/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0448 - accuracy: 0.6779 - val_loss: 0.0443 - val_accuracy: 0.6820\n",
      "Epoch 227/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0447 - accuracy: 0.6759 - val_loss: 0.0443 - val_accuracy: 0.6801\n",
      "Epoch 228/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0446 - accuracy: 0.6783 - val_loss: 0.0441 - val_accuracy: 0.6826\n",
      "Epoch 229/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0444 - accuracy: 0.6801 - val_loss: 0.0440 - val_accuracy: 0.6846\n",
      "Epoch 230/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0446 - accuracy: 0.6799 - val_loss: 0.0439 - val_accuracy: 0.6830\n",
      "Epoch 231/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0444 - accuracy: 0.6827 - val_loss: 0.0441 - val_accuracy: 0.6827\n",
      "Epoch 232/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0443 - accuracy: 0.6837 - val_loss: 0.0439 - val_accuracy: 0.6851\n",
      "Epoch 233/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0444 - accuracy: 0.6816 - val_loss: 0.0438 - val_accuracy: 0.6852\n",
      "Epoch 234/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0441 - accuracy: 0.6827 - val_loss: 0.0437 - val_accuracy: 0.6847\n",
      "Epoch 235/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0441 - accuracy: 0.6840 - val_loss: 0.0437 - val_accuracy: 0.6825\n",
      "Epoch 236/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0440 - accuracy: 0.6832 - val_loss: 0.0435 - val_accuracy: 0.6863\n",
      "Epoch 237/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0438 - accuracy: 0.6841 - val_loss: 0.0435 - val_accuracy: 0.6863\n",
      "Epoch 238/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0440 - accuracy: 0.6846 - val_loss: 0.0435 - val_accuracy: 0.6891\n",
      "Epoch 239/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0439 - accuracy: 0.6860 - val_loss: 0.0434 - val_accuracy: 0.6878\n",
      "Epoch 240/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0438 - accuracy: 0.6841 - val_loss: 0.0434 - val_accuracy: 0.6865\n",
      "Epoch 241/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0436 - accuracy: 0.6876 - val_loss: 0.0433 - val_accuracy: 0.6906\n",
      "Epoch 242/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0437 - accuracy: 0.6873 - val_loss: 0.0432 - val_accuracy: 0.6867\n",
      "Epoch 243/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0437 - accuracy: 0.6896 - val_loss: 0.0432 - val_accuracy: 0.6893\n",
      "Epoch 244/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0433 - accuracy: 0.6902 - val_loss: 0.0430 - val_accuracy: 0.6920\n",
      "Epoch 245/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0435 - accuracy: 0.6885 - val_loss: 0.0431 - val_accuracy: 0.6885\n",
      "Epoch 246/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0434 - accuracy: 0.6874 - val_loss: 0.0430 - val_accuracy: 0.6923\n",
      "Epoch 247/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0432 - accuracy: 0.6917 - val_loss: 0.0428 - val_accuracy: 0.6943\n",
      "Epoch 248/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0434 - accuracy: 0.6875 - val_loss: 0.0428 - val_accuracy: 0.6923\n",
      "Epoch 249/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0433 - accuracy: 0.6886 - val_loss: 0.0428 - val_accuracy: 0.6915\n",
      "Epoch 250/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0431 - accuracy: 0.6920 - val_loss: 0.0425 - val_accuracy: 0.6967\n",
      "Epoch 251/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0432 - accuracy: 0.6920 - val_loss: 0.0426 - val_accuracy: 0.6949\n",
      "Epoch 252/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0430 - accuracy: 0.6920 - val_loss: 0.0425 - val_accuracy: 0.6960\n",
      "Epoch 253/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0429 - accuracy: 0.6943 - val_loss: 0.0425 - val_accuracy: 0.6964\n",
      "Epoch 254/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0430 - accuracy: 0.6923 - val_loss: 0.0423 - val_accuracy: 0.6991\n",
      "Epoch 255/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0427 - accuracy: 0.6954 - val_loss: 0.0423 - val_accuracy: 0.6965\n",
      "Epoch 256/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0428 - accuracy: 0.6964 - val_loss: 0.0423 - val_accuracy: 0.6980\n",
      "Epoch 257/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0427 - accuracy: 0.6940 - val_loss: 0.0422 - val_accuracy: 0.6965\n",
      "Epoch 258/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0427 - accuracy: 0.6960 - val_loss: 0.0421 - val_accuracy: 0.6964\n",
      "Epoch 259/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0426 - accuracy: 0.6971 - val_loss: 0.0420 - val_accuracy: 0.7001\n",
      "Epoch 260/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0424 - accuracy: 0.6978 - val_loss: 0.0420 - val_accuracy: 0.7000\n",
      "Epoch 261/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0425 - accuracy: 0.6972 - val_loss: 0.0419 - val_accuracy: 0.6996\n",
      "Epoch 262/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0423 - accuracy: 0.6985 - val_loss: 0.0419 - val_accuracy: 0.6996\n",
      "Epoch 263/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0424 - accuracy: 0.6985 - val_loss: 0.0418 - val_accuracy: 0.7000\n",
      "Epoch 264/900\n",
      "45676/45676 [==============================] - 3s 60us/step - loss: 0.0424 - accuracy: 0.6964 - val_loss: 0.0418 - val_accuracy: 0.7004\n",
      "Epoch 265/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0422 - accuracy: 0.6992 - val_loss: 0.0418 - val_accuracy: 0.7020\n",
      "Epoch 266/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0421 - accuracy: 0.6997 - val_loss: 0.0416 - val_accuracy: 0.7039\n",
      "Epoch 267/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0422 - accuracy: 0.6994 - val_loss: 0.0416 - val_accuracy: 0.7041\n",
      "Epoch 268/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0421 - accuracy: 0.7000 - val_loss: 0.0415 - val_accuracy: 0.7039\n",
      "Epoch 269/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0418 - accuracy: 0.7047 - val_loss: 0.0415 - val_accuracy: 0.7041\n",
      "Epoch 270/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0418 - accuracy: 0.7037 - val_loss: 0.0413 - val_accuracy: 0.7052\n",
      "Epoch 271/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0419 - accuracy: 0.7009 - val_loss: 0.0413 - val_accuracy: 0.7053\n",
      "Epoch 272/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0419 - accuracy: 0.7021 - val_loss: 0.0411 - val_accuracy: 0.7070\n",
      "Epoch 273/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0420 - accuracy: 0.6992 - val_loss: 0.0412 - val_accuracy: 0.7059\n",
      "Epoch 274/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0418 - accuracy: 0.7035 - val_loss: 0.0411 - val_accuracy: 0.7075\n",
      "Epoch 275/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0416 - accuracy: 0.7054 - val_loss: 0.0409 - val_accuracy: 0.7080\n",
      "Epoch 276/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0415 - accuracy: 0.7057 - val_loss: 0.0410 - val_accuracy: 0.7095\n",
      "Epoch 277/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0414 - accuracy: 0.7075 - val_loss: 0.0409 - val_accuracy: 0.7116\n",
      "Epoch 278/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0413 - accuracy: 0.7073 - val_loss: 0.0409 - val_accuracy: 0.7073\n",
      "Epoch 279/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0415 - accuracy: 0.7038 - val_loss: 0.0408 - val_accuracy: 0.7099\n",
      "Epoch 280/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0414 - accuracy: 0.7038 - val_loss: 0.0408 - val_accuracy: 0.7094\n",
      "Epoch 281/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0413 - accuracy: 0.7078 - val_loss: 0.0407 - val_accuracy: 0.7109\n",
      "Epoch 282/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0412 - accuracy: 0.7084 - val_loss: 0.0406 - val_accuracy: 0.7112\n",
      "Epoch 283/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0413 - accuracy: 0.7053 - val_loss: 0.0406 - val_accuracy: 0.7110\n",
      "Epoch 284/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0410 - accuracy: 0.7082 - val_loss: 0.0406 - val_accuracy: 0.7128\n",
      "Epoch 285/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0411 - accuracy: 0.7068 - val_loss: 0.0405 - val_accuracy: 0.7123\n",
      "Epoch 286/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0410 - accuracy: 0.7106 - val_loss: 0.0405 - val_accuracy: 0.7155\n",
      "Epoch 287/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0412 - accuracy: 0.7071 - val_loss: 0.0405 - val_accuracy: 0.7141\n",
      "Epoch 288/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0407 - accuracy: 0.7119 - val_loss: 0.0404 - val_accuracy: 0.7127\n",
      "Epoch 289/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0408 - accuracy: 0.7109 - val_loss: 0.0403 - val_accuracy: 0.7158\n",
      "Epoch 290/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0408 - accuracy: 0.7112 - val_loss: 0.0402 - val_accuracy: 0.7178\n",
      "Epoch 291/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0408 - accuracy: 0.7104 - val_loss: 0.0401 - val_accuracy: 0.7146\n",
      "Epoch 292/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0407 - accuracy: 0.7125 - val_loss: 0.0400 - val_accuracy: 0.7183\n",
      "Epoch 293/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0406 - accuracy: 0.7123 - val_loss: 0.0401 - val_accuracy: 0.7163\n",
      "Epoch 294/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0406 - accuracy: 0.7131 - val_loss: 0.0400 - val_accuracy: 0.7173\n",
      "Epoch 295/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0405 - accuracy: 0.7135 - val_loss: 0.0399 - val_accuracy: 0.7171\n",
      "Epoch 296/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0404 - accuracy: 0.7134 - val_loss: 0.0400 - val_accuracy: 0.7182\n",
      "Epoch 297/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0406 - accuracy: 0.7128 - val_loss: 0.0399 - val_accuracy: 0.7194\n",
      "Epoch 298/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0403 - accuracy: 0.7154 - val_loss: 0.0397 - val_accuracy: 0.7215\n",
      "Epoch 299/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0405 - accuracy: 0.7132 - val_loss: 0.0397 - val_accuracy: 0.7214\n",
      "Epoch 300/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0403 - accuracy: 0.7160 - val_loss: 0.0397 - val_accuracy: 0.7205\n",
      "Epoch 301/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0402 - accuracy: 0.7149 - val_loss: 0.0395 - val_accuracy: 0.7231\n",
      "Epoch 302/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0402 - accuracy: 0.7148 - val_loss: 0.0395 - val_accuracy: 0.7216\n",
      "Epoch 303/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0402 - accuracy: 0.7164 - val_loss: 0.0395 - val_accuracy: 0.7222\n",
      "Epoch 304/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0399 - accuracy: 0.7176 - val_loss: 0.0394 - val_accuracy: 0.7238\n",
      "Epoch 305/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0400 - accuracy: 0.7167 - val_loss: 0.0394 - val_accuracy: 0.7232\n",
      "Epoch 306/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0400 - accuracy: 0.7187 - val_loss: 0.0394 - val_accuracy: 0.7226\n",
      "Epoch 307/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0399 - accuracy: 0.7190 - val_loss: 0.0394 - val_accuracy: 0.7257\n",
      "Epoch 308/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0399 - accuracy: 0.7183 - val_loss: 0.0392 - val_accuracy: 0.7264\n",
      "Epoch 309/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0400 - accuracy: 0.7171 - val_loss: 0.0393 - val_accuracy: 0.7250\n",
      "Epoch 310/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0397 - accuracy: 0.7193 - val_loss: 0.0391 - val_accuracy: 0.7262\n",
      "Epoch 311/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0399 - accuracy: 0.7183 - val_loss: 0.0391 - val_accuracy: 0.7264\n",
      "Epoch 312/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0398 - accuracy: 0.7189 - val_loss: 0.0390 - val_accuracy: 0.7289\n",
      "Epoch 313/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0396 - accuracy: 0.7215 - val_loss: 0.0390 - val_accuracy: 0.7285\n",
      "Epoch 314/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0396 - accuracy: 0.7189 - val_loss: 0.0389 - val_accuracy: 0.7277\n",
      "Epoch 315/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0394 - accuracy: 0.7239 - val_loss: 0.0389 - val_accuracy: 0.7294\n",
      "Epoch 316/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0394 - accuracy: 0.7224 - val_loss: 0.0389 - val_accuracy: 0.7281\n",
      "Epoch 317/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0394 - accuracy: 0.7226 - val_loss: 0.0388 - val_accuracy: 0.7272\n",
      "Epoch 318/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0395 - accuracy: 0.7227 - val_loss: 0.0388 - val_accuracy: 0.7296\n",
      "Epoch 319/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0394 - accuracy: 0.7228 - val_loss: 0.0386 - val_accuracy: 0.7306\n",
      "Epoch 320/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0393 - accuracy: 0.7222 - val_loss: 0.0386 - val_accuracy: 0.7298\n",
      "Epoch 321/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0392 - accuracy: 0.7238 - val_loss: 0.0386 - val_accuracy: 0.7306\n",
      "Epoch 322/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0391 - accuracy: 0.7236 - val_loss: 0.0386 - val_accuracy: 0.7316\n",
      "Epoch 323/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0391 - accuracy: 0.7252 - val_loss: 0.0384 - val_accuracy: 0.7334\n",
      "Epoch 324/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0390 - accuracy: 0.7253 - val_loss: 0.0385 - val_accuracy: 0.7323\n",
      "Epoch 325/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0390 - accuracy: 0.7253 - val_loss: 0.0384 - val_accuracy: 0.7335\n",
      "Epoch 326/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0389 - accuracy: 0.7261 - val_loss: 0.0384 - val_accuracy: 0.7319\n",
      "Epoch 327/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0389 - accuracy: 0.7264 - val_loss: 0.0383 - val_accuracy: 0.7321\n",
      "Epoch 328/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0388 - accuracy: 0.7277 - val_loss: 0.0382 - val_accuracy: 0.7342\n",
      "Epoch 329/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0387 - accuracy: 0.7280 - val_loss: 0.0381 - val_accuracy: 0.7327\n",
      "Epoch 330/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0388 - accuracy: 0.7269 - val_loss: 0.0381 - val_accuracy: 0.7356\n",
      "Epoch 331/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0387 - accuracy: 0.7280 - val_loss: 0.0381 - val_accuracy: 0.7365\n",
      "Epoch 332/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0386 - accuracy: 0.7286 - val_loss: 0.0381 - val_accuracy: 0.7345\n",
      "Epoch 333/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0386 - accuracy: 0.7280 - val_loss: 0.0380 - val_accuracy: 0.7368\n",
      "Epoch 334/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0386 - accuracy: 0.7299 - val_loss: 0.0379 - val_accuracy: 0.7373\n",
      "Epoch 335/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0387 - accuracy: 0.7277 - val_loss: 0.0378 - val_accuracy: 0.7366\n",
      "Epoch 336/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0386 - accuracy: 0.7297 - val_loss: 0.0378 - val_accuracy: 0.7364\n",
      "Epoch 337/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0386 - accuracy: 0.7269 - val_loss: 0.0378 - val_accuracy: 0.7369\n",
      "Epoch 338/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0383 - accuracy: 0.7315 - val_loss: 0.0378 - val_accuracy: 0.7388\n",
      "Epoch 339/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0383 - accuracy: 0.7325 - val_loss: 0.0377 - val_accuracy: 0.7377\n",
      "Epoch 340/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0384 - accuracy: 0.7299 - val_loss: 0.0377 - val_accuracy: 0.7392\n",
      "Epoch 341/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0384 - accuracy: 0.7302 - val_loss: 0.0376 - val_accuracy: 0.7380\n",
      "Epoch 342/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0383 - accuracy: 0.7309 - val_loss: 0.0375 - val_accuracy: 0.7396\n",
      "Epoch 343/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0381 - accuracy: 0.7320 - val_loss: 0.0375 - val_accuracy: 0.7388\n",
      "Epoch 344/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0381 - accuracy: 0.7337 - val_loss: 0.0374 - val_accuracy: 0.7421\n",
      "Epoch 345/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0382 - accuracy: 0.7312 - val_loss: 0.0374 - val_accuracy: 0.7398\n",
      "Epoch 346/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0378 - accuracy: 0.7359 - val_loss: 0.0375 - val_accuracy: 0.7391\n",
      "Epoch 347/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0380 - accuracy: 0.7321 - val_loss: 0.0373 - val_accuracy: 0.7402\n",
      "Epoch 348/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0381 - accuracy: 0.7323 - val_loss: 0.0372 - val_accuracy: 0.7417\n",
      "Epoch 349/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0381 - accuracy: 0.7336 - val_loss: 0.0373 - val_accuracy: 0.7412\n",
      "Epoch 350/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0379 - accuracy: 0.7352 - val_loss: 0.0373 - val_accuracy: 0.7425\n",
      "Epoch 351/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0380 - accuracy: 0.7350 - val_loss: 0.0371 - val_accuracy: 0.7442\n",
      "Epoch 352/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0378 - accuracy: 0.7362 - val_loss: 0.0370 - val_accuracy: 0.7427\n",
      "Epoch 353/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0377 - accuracy: 0.7371 - val_loss: 0.0370 - val_accuracy: 0.7431\n",
      "Epoch 354/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0377 - accuracy: 0.7347 - val_loss: 0.0370 - val_accuracy: 0.7442\n",
      "Epoch 355/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0375 - accuracy: 0.7387 - val_loss: 0.0369 - val_accuracy: 0.7468\n",
      "Epoch 356/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0374 - accuracy: 0.7387 - val_loss: 0.0369 - val_accuracy: 0.7462\n",
      "Epoch 357/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0375 - accuracy: 0.7379 - val_loss: 0.0368 - val_accuracy: 0.7457\n",
      "Epoch 358/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0373 - accuracy: 0.7388 - val_loss: 0.0369 - val_accuracy: 0.7464\n",
      "Epoch 359/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0374 - accuracy: 0.7382 - val_loss: 0.0368 - val_accuracy: 0.7465\n",
      "Epoch 360/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0375 - accuracy: 0.7377 - val_loss: 0.0367 - val_accuracy: 0.7454\n",
      "Epoch 361/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0374 - accuracy: 0.7369 - val_loss: 0.0366 - val_accuracy: 0.7468\n",
      "Epoch 362/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0372 - accuracy: 0.7393 - val_loss: 0.0367 - val_accuracy: 0.7469\n",
      "Epoch 363/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0371 - accuracy: 0.7390 - val_loss: 0.0365 - val_accuracy: 0.7473\n",
      "Epoch 364/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0373 - accuracy: 0.7391 - val_loss: 0.0366 - val_accuracy: 0.7464\n",
      "Epoch 365/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0372 - accuracy: 0.7380 - val_loss: 0.0365 - val_accuracy: 0.7478\n",
      "Epoch 366/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0371 - accuracy: 0.7403 - val_loss: 0.0365 - val_accuracy: 0.7470\n",
      "Epoch 367/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0372 - accuracy: 0.7395 - val_loss: 0.0363 - val_accuracy: 0.7507\n",
      "Epoch 368/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0371 - accuracy: 0.7405 - val_loss: 0.0364 - val_accuracy: 0.7508\n",
      "Epoch 369/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0370 - accuracy: 0.7433 - val_loss: 0.0363 - val_accuracy: 0.7488\n",
      "Epoch 370/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0371 - accuracy: 0.7408 - val_loss: 0.0362 - val_accuracy: 0.7506\n",
      "Epoch 371/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0369 - accuracy: 0.7423 - val_loss: 0.0362 - val_accuracy: 0.7496\n",
      "Epoch 372/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0368 - accuracy: 0.7430 - val_loss: 0.0362 - val_accuracy: 0.7500\n",
      "Epoch 373/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0368 - accuracy: 0.7439 - val_loss: 0.0362 - val_accuracy: 0.7490\n",
      "Epoch 374/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0368 - accuracy: 0.7421 - val_loss: 0.0362 - val_accuracy: 0.7498\n",
      "Epoch 375/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0367 - accuracy: 0.7439 - val_loss: 0.0360 - val_accuracy: 0.7505\n",
      "Epoch 376/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0366 - accuracy: 0.7450 - val_loss: 0.0360 - val_accuracy: 0.7508\n",
      "Epoch 377/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0367 - accuracy: 0.7436 - val_loss: 0.0361 - val_accuracy: 0.7496\n",
      "Epoch 378/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0364 - accuracy: 0.7451 - val_loss: 0.0359 - val_accuracy: 0.7522\n",
      "Epoch 379/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0366 - accuracy: 0.7438 - val_loss: 0.0359 - val_accuracy: 0.7528\n",
      "Epoch 380/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0365 - accuracy: 0.7438 - val_loss: 0.0359 - val_accuracy: 0.7525\n",
      "Epoch 381/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0366 - accuracy: 0.7439 - val_loss: 0.0358 - val_accuracy: 0.7525\n",
      "Epoch 382/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0364 - accuracy: 0.7473 - val_loss: 0.0357 - val_accuracy: 0.7520\n",
      "Epoch 383/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0365 - accuracy: 0.7431 - val_loss: 0.0357 - val_accuracy: 0.7536\n",
      "Epoch 384/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0365 - accuracy: 0.7444 - val_loss: 0.0357 - val_accuracy: 0.7530\n",
      "Epoch 385/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0363 - accuracy: 0.7466 - val_loss: 0.0356 - val_accuracy: 0.7545\n",
      "Epoch 386/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0364 - accuracy: 0.7472 - val_loss: 0.0356 - val_accuracy: 0.7549\n",
      "Epoch 387/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0363 - accuracy: 0.7468 - val_loss: 0.0357 - val_accuracy: 0.7547\n",
      "Epoch 388/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0362 - accuracy: 0.7489 - val_loss: 0.0356 - val_accuracy: 0.7533\n",
      "Epoch 389/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0362 - accuracy: 0.7470 - val_loss: 0.0354 - val_accuracy: 0.7572\n",
      "Epoch 390/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0359 - accuracy: 0.7499 - val_loss: 0.0354 - val_accuracy: 0.7559\n",
      "Epoch 391/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0360 - accuracy: 0.7487 - val_loss: 0.0355 - val_accuracy: 0.7546\n",
      "Epoch 392/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0361 - accuracy: 0.7478 - val_loss: 0.0353 - val_accuracy: 0.7574\n",
      "Epoch 393/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0360 - accuracy: 0.7498 - val_loss: 0.0353 - val_accuracy: 0.7573\n",
      "Epoch 394/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0359 - accuracy: 0.7488 - val_loss: 0.0352 - val_accuracy: 0.7569\n",
      "Epoch 395/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0359 - accuracy: 0.7488 - val_loss: 0.0352 - val_accuracy: 0.7569\n",
      "Epoch 396/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0359 - accuracy: 0.7515 - val_loss: 0.0352 - val_accuracy: 0.7550\n",
      "Epoch 397/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0360 - accuracy: 0.7478 - val_loss: 0.0351 - val_accuracy: 0.7577\n",
      "Epoch 398/900\n",
      "45676/45676 [==============================] - 2s 49us/step - loss: 0.0358 - accuracy: 0.7492 - val_loss: 0.0350 - val_accuracy: 0.7577\n",
      "Epoch 399/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0359 - accuracy: 0.7490 - val_loss: 0.0351 - val_accuracy: 0.7579\n",
      "Epoch 400/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0356 - accuracy: 0.7528 - val_loss: 0.0350 - val_accuracy: 0.7584\n",
      "Epoch 401/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0356 - accuracy: 0.7531 - val_loss: 0.0349 - val_accuracy: 0.7577\n",
      "Epoch 402/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0357 - accuracy: 0.7530 - val_loss: 0.0349 - val_accuracy: 0.7593\n",
      "Epoch 403/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0357 - accuracy: 0.7504 - val_loss: 0.0349 - val_accuracy: 0.7576\n",
      "Epoch 404/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0357 - accuracy: 0.7511 - val_loss: 0.0349 - val_accuracy: 0.7605\n",
      "Epoch 405/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0355 - accuracy: 0.7509 - val_loss: 0.0348 - val_accuracy: 0.7592\n",
      "Epoch 406/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0354 - accuracy: 0.7532 - val_loss: 0.0347 - val_accuracy: 0.7591\n",
      "Epoch 407/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0354 - accuracy: 0.7525 - val_loss: 0.0348 - val_accuracy: 0.7593\n",
      "Epoch 408/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0354 - accuracy: 0.7553 - val_loss: 0.0347 - val_accuracy: 0.7595\n",
      "Epoch 409/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0354 - accuracy: 0.7540 - val_loss: 0.0346 - val_accuracy: 0.7599\n",
      "Epoch 410/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0353 - accuracy: 0.7539 - val_loss: 0.0345 - val_accuracy: 0.7613\n",
      "Epoch 411/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0353 - accuracy: 0.7555 - val_loss: 0.0346 - val_accuracy: 0.7604\n",
      "Epoch 412/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0352 - accuracy: 0.7555 - val_loss: 0.0345 - val_accuracy: 0.7626\n",
      "Epoch 413/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0353 - accuracy: 0.7524 - val_loss: 0.0345 - val_accuracy: 0.7630\n",
      "Epoch 414/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0354 - accuracy: 0.7520 - val_loss: 0.0345 - val_accuracy: 0.7630\n",
      "Epoch 415/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0353 - accuracy: 0.7538 - val_loss: 0.0345 - val_accuracy: 0.7627\n",
      "Epoch 416/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0352 - accuracy: 0.7553 - val_loss: 0.0345 - val_accuracy: 0.7631\n",
      "Epoch 417/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0352 - accuracy: 0.7547 - val_loss: 0.0343 - val_accuracy: 0.7635\n",
      "Epoch 418/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0351 - accuracy: 0.7567 - val_loss: 0.0343 - val_accuracy: 0.7625\n",
      "Epoch 419/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0350 - accuracy: 0.7558 - val_loss: 0.0344 - val_accuracy: 0.7646\n",
      "Epoch 420/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0350 - accuracy: 0.7556 - val_loss: 0.0342 - val_accuracy: 0.7641\n",
      "Epoch 421/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0351 - accuracy: 0.7557 - val_loss: 0.0343 - val_accuracy: 0.7653\n",
      "Epoch 422/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0348 - accuracy: 0.7571 - val_loss: 0.0343 - val_accuracy: 0.7620\n",
      "Epoch 423/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0349 - accuracy: 0.7579 - val_loss: 0.0342 - val_accuracy: 0.7654\n",
      "Epoch 424/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0349 - accuracy: 0.7576 - val_loss: 0.0340 - val_accuracy: 0.7661\n",
      "Epoch 425/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0347 - accuracy: 0.7595 - val_loss: 0.0341 - val_accuracy: 0.7659\n",
      "Epoch 426/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0348 - accuracy: 0.7583 - val_loss: 0.0341 - val_accuracy: 0.7658\n",
      "Epoch 427/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0348 - accuracy: 0.7575 - val_loss: 0.0340 - val_accuracy: 0.7663\n",
      "Epoch 428/900\n",
      "45676/45676 [==============================] - 2s 49us/step - loss: 0.0347 - accuracy: 0.7593 - val_loss: 0.0339 - val_accuracy: 0.7673\n",
      "Epoch 429/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0346 - accuracy: 0.7614 - val_loss: 0.0339 - val_accuracy: 0.7665\n",
      "Epoch 430/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0347 - accuracy: 0.7599 - val_loss: 0.0340 - val_accuracy: 0.7658\n",
      "Epoch 431/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0347 - accuracy: 0.7579 - val_loss: 0.0339 - val_accuracy: 0.7685\n",
      "Epoch 432/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0346 - accuracy: 0.7595 - val_loss: 0.0338 - val_accuracy: 0.7686\n",
      "Epoch 433/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0344 - accuracy: 0.7614 - val_loss: 0.0338 - val_accuracy: 0.7695\n",
      "Epoch 434/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0346 - accuracy: 0.7603 - val_loss: 0.0338 - val_accuracy: 0.7694\n",
      "Epoch 435/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0345 - accuracy: 0.7612 - val_loss: 0.0337 - val_accuracy: 0.7696\n",
      "Epoch 436/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0346 - accuracy: 0.7596 - val_loss: 0.0338 - val_accuracy: 0.7682\n",
      "Epoch 437/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0345 - accuracy: 0.7618 - val_loss: 0.0336 - val_accuracy: 0.7702\n",
      "Epoch 438/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0345 - accuracy: 0.7608 - val_loss: 0.0336 - val_accuracy: 0.7709\n",
      "Epoch 439/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0342 - accuracy: 0.7653 - val_loss: 0.0336 - val_accuracy: 0.7701\n",
      "Epoch 440/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0345 - accuracy: 0.7614 - val_loss: 0.0336 - val_accuracy: 0.7692\n",
      "Epoch 441/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0345 - accuracy: 0.7597 - val_loss: 0.0335 - val_accuracy: 0.7687\n",
      "Epoch 442/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0342 - accuracy: 0.7629 - val_loss: 0.0334 - val_accuracy: 0.7719\n",
      "Epoch 443/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0342 - accuracy: 0.7639 - val_loss: 0.0334 - val_accuracy: 0.7714\n",
      "Epoch 444/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0340 - accuracy: 0.7635 - val_loss: 0.0333 - val_accuracy: 0.7713\n",
      "Epoch 445/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0341 - accuracy: 0.7641 - val_loss: 0.0333 - val_accuracy: 0.7720\n",
      "Epoch 446/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0340 - accuracy: 0.7642 - val_loss: 0.0333 - val_accuracy: 0.7714\n",
      "Epoch 447/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0339 - accuracy: 0.7649 - val_loss: 0.0332 - val_accuracy: 0.7727\n",
      "Epoch 448/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0340 - accuracy: 0.7633 - val_loss: 0.0333 - val_accuracy: 0.7722\n",
      "Epoch 449/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0341 - accuracy: 0.7642 - val_loss: 0.0332 - val_accuracy: 0.7719\n",
      "Epoch 450/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0340 - accuracy: 0.7664 - val_loss: 0.0331 - val_accuracy: 0.7743\n",
      "Epoch 451/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0338 - accuracy: 0.7672 - val_loss: 0.0332 - val_accuracy: 0.7725\n",
      "Epoch 452/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0337 - accuracy: 0.7676 - val_loss: 0.0330 - val_accuracy: 0.7719\n",
      "Epoch 453/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0341 - accuracy: 0.7647 - val_loss: 0.0330 - val_accuracy: 0.7739\n",
      "Epoch 454/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0337 - accuracy: 0.7679 - val_loss: 0.0330 - val_accuracy: 0.7750\n",
      "Epoch 455/900\n",
      "45676/45676 [==============================] - 2s 49us/step - loss: 0.0338 - accuracy: 0.7681 - val_loss: 0.0332 - val_accuracy: 0.7730\n",
      "Epoch 456/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0337 - accuracy: 0.7661 - val_loss: 0.0329 - val_accuracy: 0.7729\n",
      "Epoch 457/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0335 - accuracy: 0.7659 - val_loss: 0.0330 - val_accuracy: 0.7743\n",
      "Epoch 458/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0335 - accuracy: 0.7694 - val_loss: 0.0328 - val_accuracy: 0.7757\n",
      "Epoch 459/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0337 - accuracy: 0.7670 - val_loss: 0.0328 - val_accuracy: 0.7763\n",
      "Epoch 460/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0335 - accuracy: 0.7662 - val_loss: 0.0328 - val_accuracy: 0.7749\n",
      "Epoch 461/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0336 - accuracy: 0.7662 - val_loss: 0.0327 - val_accuracy: 0.7757\n",
      "Epoch 462/900\n",
      "45676/45676 [==============================] - 2s 50us/step - loss: 0.0335 - accuracy: 0.7675 - val_loss: 0.0327 - val_accuracy: 0.7757\n",
      "Epoch 463/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0333 - accuracy: 0.7704 - val_loss: 0.0327 - val_accuracy: 0.7736\n",
      "Epoch 464/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0334 - accuracy: 0.7691 - val_loss: 0.0327 - val_accuracy: 0.7753\n",
      "Epoch 465/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0335 - accuracy: 0.7683 - val_loss: 0.0327 - val_accuracy: 0.7749\n",
      "Epoch 466/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0333 - accuracy: 0.7691 - val_loss: 0.0326 - val_accuracy: 0.7780\n",
      "Epoch 467/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0334 - accuracy: 0.7695 - val_loss: 0.0325 - val_accuracy: 0.7767\n",
      "Epoch 468/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0332 - accuracy: 0.7702 - val_loss: 0.0326 - val_accuracy: 0.7750\n",
      "Epoch 469/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0333 - accuracy: 0.7710 - val_loss: 0.0325 - val_accuracy: 0.7795\n",
      "Epoch 470/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0332 - accuracy: 0.7687 - val_loss: 0.0324 - val_accuracy: 0.7792\n",
      "Epoch 471/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0331 - accuracy: 0.7715 - val_loss: 0.0324 - val_accuracy: 0.7776\n",
      "Epoch 472/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0331 - accuracy: 0.7703 - val_loss: 0.0324 - val_accuracy: 0.7785\n",
      "Epoch 473/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0330 - accuracy: 0.7713 - val_loss: 0.0324 - val_accuracy: 0.7799\n",
      "Epoch 474/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0330 - accuracy: 0.7725 - val_loss: 0.0324 - val_accuracy: 0.7784\n",
      "Epoch 475/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0331 - accuracy: 0.7707 - val_loss: 0.0324 - val_accuracy: 0.7784\n",
      "Epoch 476/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0330 - accuracy: 0.7724 - val_loss: 0.0322 - val_accuracy: 0.7800\n",
      "Epoch 477/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0328 - accuracy: 0.7740 - val_loss: 0.0322 - val_accuracy: 0.7811\n",
      "Epoch 478/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0329 - accuracy: 0.7717 - val_loss: 0.0322 - val_accuracy: 0.7781\n",
      "Epoch 479/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0330 - accuracy: 0.7724 - val_loss: 0.0322 - val_accuracy: 0.7797\n",
      "Epoch 480/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0328 - accuracy: 0.7711 - val_loss: 0.0322 - val_accuracy: 0.7777\n",
      "Epoch 481/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0328 - accuracy: 0.7710 - val_loss: 0.0322 - val_accuracy: 0.7787\n",
      "Epoch 482/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0330 - accuracy: 0.7727 - val_loss: 0.0320 - val_accuracy: 0.7811\n",
      "Epoch 483/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0328 - accuracy: 0.7729 - val_loss: 0.0320 - val_accuracy: 0.7806\n",
      "Epoch 484/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0326 - accuracy: 0.7752 - val_loss: 0.0320 - val_accuracy: 0.7807\n",
      "Epoch 485/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0327 - accuracy: 0.7751 - val_loss: 0.0319 - val_accuracy: 0.7817\n",
      "Epoch 486/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0328 - accuracy: 0.7734 - val_loss: 0.0319 - val_accuracy: 0.7814\n",
      "Epoch 487/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0327 - accuracy: 0.7735 - val_loss: 0.0319 - val_accuracy: 0.7813\n",
      "Epoch 488/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0326 - accuracy: 0.7734 - val_loss: 0.0319 - val_accuracy: 0.7814\n",
      "Epoch 489/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0326 - accuracy: 0.7742 - val_loss: 0.0319 - val_accuracy: 0.7826\n",
      "Epoch 490/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0325 - accuracy: 0.7769 - val_loss: 0.0318 - val_accuracy: 0.7831\n",
      "Epoch 491/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0326 - accuracy: 0.7747 - val_loss: 0.0317 - val_accuracy: 0.7830\n",
      "Epoch 492/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0325 - accuracy: 0.7753 - val_loss: 0.0317 - val_accuracy: 0.7847\n",
      "Epoch 493/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0325 - accuracy: 0.7763 - val_loss: 0.0317 - val_accuracy: 0.7829\n",
      "Epoch 494/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0323 - accuracy: 0.7784 - val_loss: 0.0316 - val_accuracy: 0.7849\n",
      "Epoch 495/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0322 - accuracy: 0.7784 - val_loss: 0.0315 - val_accuracy: 0.7849\n",
      "Epoch 496/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0324 - accuracy: 0.7772 - val_loss: 0.0317 - val_accuracy: 0.7842\n",
      "Epoch 497/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0324 - accuracy: 0.7760 - val_loss: 0.0316 - val_accuracy: 0.7851\n",
      "Epoch 498/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0324 - accuracy: 0.7758 - val_loss: 0.0315 - val_accuracy: 0.7864\n",
      "Epoch 499/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0322 - accuracy: 0.7773 - val_loss: 0.0315 - val_accuracy: 0.7863\n",
      "Epoch 500/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0322 - accuracy: 0.7784 - val_loss: 0.0315 - val_accuracy: 0.7847\n",
      "Epoch 501/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0321 - accuracy: 0.7788 - val_loss: 0.0314 - val_accuracy: 0.7845\n",
      "Epoch 502/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0321 - accuracy: 0.7782 - val_loss: 0.0314 - val_accuracy: 0.7849\n",
      "Epoch 503/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0320 - accuracy: 0.7792 - val_loss: 0.0313 - val_accuracy: 0.7863\n",
      "Epoch 504/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0321 - accuracy: 0.7788 - val_loss: 0.0313 - val_accuracy: 0.7871\n",
      "Epoch 505/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0320 - accuracy: 0.7803 - val_loss: 0.0313 - val_accuracy: 0.7847\n",
      "Epoch 506/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0319 - accuracy: 0.7804 - val_loss: 0.0313 - val_accuracy: 0.7870\n",
      "Epoch 507/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0322 - accuracy: 0.7794 - val_loss: 0.0312 - val_accuracy: 0.7877\n",
      "Epoch 508/900\n",
      "45676/45676 [==============================] - 3s 76us/step - loss: 0.0319 - accuracy: 0.7794 - val_loss: 0.0313 - val_accuracy: 0.7874\n",
      "Epoch 509/900\n",
      "45676/45676 [==============================] - 4s 84us/step - loss: 0.0320 - accuracy: 0.7796 - val_loss: 0.0312 - val_accuracy: 0.7851\n",
      "Epoch 510/900\n",
      "45676/45676 [==============================] - 3s 63us/step - loss: 0.0320 - accuracy: 0.7792 - val_loss: 0.0311 - val_accuracy: 0.7869\n",
      "Epoch 511/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0318 - accuracy: 0.7807 - val_loss: 0.0311 - val_accuracy: 0.7874\n",
      "Epoch 512/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0321 - accuracy: 0.7793 - val_loss: 0.0311 - val_accuracy: 0.7884\n",
      "Epoch 513/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0320 - accuracy: 0.7791 - val_loss: 0.0311 - val_accuracy: 0.7870\n",
      "Epoch 514/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0318 - accuracy: 0.7801 - val_loss: 0.0310 - val_accuracy: 0.7889\n",
      "Epoch 515/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0320 - accuracy: 0.7797 - val_loss: 0.0310 - val_accuracy: 0.7884\n",
      "Epoch 516/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0317 - accuracy: 0.7820 - val_loss: 0.0310 - val_accuracy: 0.7888\n",
      "Epoch 517/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0317 - accuracy: 0.7813 - val_loss: 0.0310 - val_accuracy: 0.7897\n",
      "Epoch 518/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0316 - accuracy: 0.7821 - val_loss: 0.0309 - val_accuracy: 0.7888\n",
      "Epoch 519/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0316 - accuracy: 0.7837 - val_loss: 0.0308 - val_accuracy: 0.7901\n",
      "Epoch 520/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0318 - accuracy: 0.7808 - val_loss: 0.0308 - val_accuracy: 0.7905\n",
      "Epoch 521/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0316 - accuracy: 0.7832 - val_loss: 0.0309 - val_accuracy: 0.7891\n",
      "Epoch 522/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0316 - accuracy: 0.7823 - val_loss: 0.0308 - val_accuracy: 0.7910\n",
      "Epoch 523/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0315 - accuracy: 0.7834 - val_loss: 0.0307 - val_accuracy: 0.7905\n",
      "Epoch 524/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0316 - accuracy: 0.7830 - val_loss: 0.0308 - val_accuracy: 0.7914\n",
      "Epoch 525/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0313 - accuracy: 0.7849 - val_loss: 0.0308 - val_accuracy: 0.7897\n",
      "Epoch 526/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0313 - accuracy: 0.7865 - val_loss: 0.0307 - val_accuracy: 0.7892\n",
      "Epoch 527/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0312 - accuracy: 0.7854 - val_loss: 0.0307 - val_accuracy: 0.7913\n",
      "Epoch 528/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0315 - accuracy: 0.7832 - val_loss: 0.0306 - val_accuracy: 0.7903\n",
      "Epoch 529/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0313 - accuracy: 0.7840 - val_loss: 0.0306 - val_accuracy: 0.7918\n",
      "Epoch 530/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0311 - accuracy: 0.7866 - val_loss: 0.0306 - val_accuracy: 0.7930\n",
      "Epoch 531/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0312 - accuracy: 0.7849 - val_loss: 0.0306 - val_accuracy: 0.7917\n",
      "Epoch 532/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0311 - accuracy: 0.7860 - val_loss: 0.0305 - val_accuracy: 0.7916\n",
      "Epoch 533/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0309 - accuracy: 0.7885 - val_loss: 0.0305 - val_accuracy: 0.7932\n",
      "Epoch 534/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0313 - accuracy: 0.7865 - val_loss: 0.0305 - val_accuracy: 0.7912\n",
      "Epoch 535/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0312 - accuracy: 0.7868 - val_loss: 0.0304 - val_accuracy: 0.7916\n",
      "Epoch 536/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0310 - accuracy: 0.7869 - val_loss: 0.0303 - val_accuracy: 0.7934\n",
      "Epoch 537/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0311 - accuracy: 0.7857 - val_loss: 0.0305 - val_accuracy: 0.7905\n",
      "Epoch 538/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0310 - accuracy: 0.7870 - val_loss: 0.0303 - val_accuracy: 0.7933\n",
      "Epoch 539/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0309 - accuracy: 0.7876 - val_loss: 0.0303 - val_accuracy: 0.7934\n",
      "Epoch 540/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0309 - accuracy: 0.7883 - val_loss: 0.0302 - val_accuracy: 0.7938\n",
      "Epoch 541/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0309 - accuracy: 0.7870 - val_loss: 0.0301 - val_accuracy: 0.7942\n",
      "Epoch 542/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0310 - accuracy: 0.7866 - val_loss: 0.0301 - val_accuracy: 0.7979\n",
      "Epoch 543/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0309 - accuracy: 0.7877 - val_loss: 0.0301 - val_accuracy: 0.7945\n",
      "Epoch 544/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0311 - accuracy: 0.7860 - val_loss: 0.0301 - val_accuracy: 0.7941\n",
      "Epoch 545/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0309 - accuracy: 0.7879 - val_loss: 0.0300 - val_accuracy: 0.7956\n",
      "Epoch 546/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0308 - accuracy: 0.7891 - val_loss: 0.0301 - val_accuracy: 0.7956\n",
      "Epoch 547/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0309 - accuracy: 0.7885 - val_loss: 0.0300 - val_accuracy: 0.7944\n",
      "Epoch 548/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0308 - accuracy: 0.7878 - val_loss: 0.0299 - val_accuracy: 0.7963\n",
      "Epoch 549/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0307 - accuracy: 0.7892 - val_loss: 0.0300 - val_accuracy: 0.7935\n",
      "Epoch 550/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0309 - accuracy: 0.7880 - val_loss: 0.0301 - val_accuracy: 0.7958\n",
      "Epoch 551/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0306 - accuracy: 0.7889 - val_loss: 0.0299 - val_accuracy: 0.7950\n",
      "Epoch 552/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0306 - accuracy: 0.7895 - val_loss: 0.0299 - val_accuracy: 0.7973\n",
      "Epoch 553/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0307 - accuracy: 0.7888 - val_loss: 0.0299 - val_accuracy: 0.7960\n",
      "Epoch 554/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0306 - accuracy: 0.7909 - val_loss: 0.0301 - val_accuracy: 0.7951\n",
      "Epoch 555/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0305 - accuracy: 0.7920 - val_loss: 0.0298 - val_accuracy: 0.7967\n",
      "Epoch 556/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0304 - accuracy: 0.7921 - val_loss: 0.0300 - val_accuracy: 0.7958\n",
      "Epoch 557/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0306 - accuracy: 0.7897 - val_loss: 0.0298 - val_accuracy: 0.7975\n",
      "Epoch 558/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0306 - accuracy: 0.7893 - val_loss: 0.0297 - val_accuracy: 0.7976\n",
      "Epoch 559/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0304 - accuracy: 0.7916 - val_loss: 0.0297 - val_accuracy: 0.7989\n",
      "Epoch 560/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0302 - accuracy: 0.7926 - val_loss: 0.0297 - val_accuracy: 0.7987\n",
      "Epoch 561/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0306 - accuracy: 0.7900 - val_loss: 0.0296 - val_accuracy: 0.7990\n",
      "Epoch 562/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0303 - accuracy: 0.7920 - val_loss: 0.0296 - val_accuracy: 0.7989\n",
      "Epoch 563/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0303 - accuracy: 0.7932 - val_loss: 0.0296 - val_accuracy: 0.7993\n",
      "Epoch 564/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0306 - accuracy: 0.7910 - val_loss: 0.0297 - val_accuracy: 0.7984\n",
      "Epoch 565/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0304 - accuracy: 0.7906 - val_loss: 0.0296 - val_accuracy: 0.7989\n",
      "Epoch 566/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0303 - accuracy: 0.7915 - val_loss: 0.0296 - val_accuracy: 0.7961\n",
      "Epoch 567/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0302 - accuracy: 0.7933 - val_loss: 0.0296 - val_accuracy: 0.7999\n",
      "Epoch 568/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0302 - accuracy: 0.7936 - val_loss: 0.0294 - val_accuracy: 0.8007\n",
      "Epoch 569/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0301 - accuracy: 0.7939 - val_loss: 0.0294 - val_accuracy: 0.7989\n",
      "Epoch 570/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0301 - accuracy: 0.7936 - val_loss: 0.0294 - val_accuracy: 0.8012\n",
      "Epoch 571/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0303 - accuracy: 0.7932 - val_loss: 0.0294 - val_accuracy: 0.8018\n",
      "Epoch 572/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0302 - accuracy: 0.7926 - val_loss: 0.0294 - val_accuracy: 0.8007\n",
      "Epoch 573/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0300 - accuracy: 0.7938 - val_loss: 0.0293 - val_accuracy: 0.8011\n",
      "Epoch 574/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0300 - accuracy: 0.7957 - val_loss: 0.0293 - val_accuracy: 0.7992\n",
      "Epoch 575/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0300 - accuracy: 0.7940 - val_loss: 0.0292 - val_accuracy: 0.8007\n",
      "Epoch 576/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0302 - accuracy: 0.7914 - val_loss: 0.0292 - val_accuracy: 0.8016\n",
      "Epoch 577/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0301 - accuracy: 0.7946 - val_loss: 0.0293 - val_accuracy: 0.7997\n",
      "Epoch 578/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0299 - accuracy: 0.7946 - val_loss: 0.0293 - val_accuracy: 0.8012\n",
      "Epoch 579/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0297 - accuracy: 0.7958 - val_loss: 0.0292 - val_accuracy: 0.8021\n",
      "Epoch 580/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0300 - accuracy: 0.7943 - val_loss: 0.0292 - val_accuracy: 0.8031\n",
      "Epoch 581/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0299 - accuracy: 0.7949 - val_loss: 0.0292 - val_accuracy: 0.8034\n",
      "Epoch 582/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0298 - accuracy: 0.7961 - val_loss: 0.0291 - val_accuracy: 0.8027\n",
      "Epoch 583/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0298 - accuracy: 0.7973 - val_loss: 0.0292 - val_accuracy: 0.8010\n",
      "Epoch 584/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0297 - accuracy: 0.7964 - val_loss: 0.0290 - val_accuracy: 0.8044\n",
      "Epoch 585/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0298 - accuracy: 0.7963 - val_loss: 0.0291 - val_accuracy: 0.8026\n",
      "Epoch 586/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0298 - accuracy: 0.7957 - val_loss: 0.0291 - val_accuracy: 0.8038\n",
      "Epoch 587/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0296 - accuracy: 0.7974 - val_loss: 0.0290 - val_accuracy: 0.8027\n",
      "Epoch 588/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0296 - accuracy: 0.7977 - val_loss: 0.0290 - val_accuracy: 0.8045\n",
      "Epoch 589/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0299 - accuracy: 0.7959 - val_loss: 0.0290 - val_accuracy: 0.8027\n",
      "Epoch 590/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0297 - accuracy: 0.7979 - val_loss: 0.0289 - val_accuracy: 0.8032\n",
      "Epoch 591/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0296 - accuracy: 0.7959 - val_loss: 0.0290 - val_accuracy: 0.8027\n",
      "Epoch 592/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0296 - accuracy: 0.7983 - val_loss: 0.0289 - val_accuracy: 0.8032\n",
      "Epoch 593/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0298 - accuracy: 0.7956 - val_loss: 0.0288 - val_accuracy: 0.8060\n",
      "Epoch 594/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0293 - accuracy: 0.7994 - val_loss: 0.0289 - val_accuracy: 0.8053\n",
      "Epoch 595/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0295 - accuracy: 0.7980 - val_loss: 0.0287 - val_accuracy: 0.8055\n",
      "Epoch 596/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0293 - accuracy: 0.8003 - val_loss: 0.0288 - val_accuracy: 0.8053\n",
      "Epoch 597/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0295 - accuracy: 0.8001 - val_loss: 0.0286 - val_accuracy: 0.8068\n",
      "Epoch 598/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0293 - accuracy: 0.8013 - val_loss: 0.0288 - val_accuracy: 0.8047\n",
      "Epoch 599/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0294 - accuracy: 0.7988 - val_loss: 0.0287 - val_accuracy: 0.8068\n",
      "Epoch 600/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0294 - accuracy: 0.7981 - val_loss: 0.0287 - val_accuracy: 0.8045\n",
      "Epoch 601/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0294 - accuracy: 0.7986 - val_loss: 0.0286 - val_accuracy: 0.8067\n",
      "Epoch 602/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0295 - accuracy: 0.7986 - val_loss: 0.0286 - val_accuracy: 0.8064\n",
      "Epoch 603/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0293 - accuracy: 0.7994 - val_loss: 0.0286 - val_accuracy: 0.8066\n",
      "Epoch 604/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0293 - accuracy: 0.7990 - val_loss: 0.0286 - val_accuracy: 0.8067\n",
      "Epoch 605/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0294 - accuracy: 0.7977 - val_loss: 0.0286 - val_accuracy: 0.8065\n",
      "Epoch 606/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0291 - accuracy: 0.8019 - val_loss: 0.0286 - val_accuracy: 0.8074\n",
      "Epoch 607/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0293 - accuracy: 0.8001 - val_loss: 0.0285 - val_accuracy: 0.8078\n",
      "Epoch 608/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0292 - accuracy: 0.8005 - val_loss: 0.0285 - val_accuracy: 0.8069\n",
      "Epoch 609/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0293 - accuracy: 0.7990 - val_loss: 0.0284 - val_accuracy: 0.8081\n",
      "Epoch 610/900\n",
      "45676/45676 [==============================] - 2s 42us/step - loss: 0.0289 - accuracy: 0.8035 - val_loss: 0.0284 - val_accuracy: 0.8079\n",
      "Epoch 611/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0290 - accuracy: 0.8020 - val_loss: 0.0285 - val_accuracy: 0.8086\n",
      "Epoch 612/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0292 - accuracy: 0.8007 - val_loss: 0.0284 - val_accuracy: 0.8087\n",
      "Epoch 613/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0290 - accuracy: 0.8021 - val_loss: 0.0284 - val_accuracy: 0.8074\n",
      "Epoch 614/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0291 - accuracy: 0.8012 - val_loss: 0.0284 - val_accuracy: 0.8066\n",
      "Epoch 615/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0291 - accuracy: 0.8018 - val_loss: 0.0284 - val_accuracy: 0.8088\n",
      "Epoch 616/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0289 - accuracy: 0.8034 - val_loss: 0.0284 - val_accuracy: 0.8067\n",
      "Epoch 617/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0289 - accuracy: 0.8028 - val_loss: 0.0283 - val_accuracy: 0.8094\n",
      "Epoch 618/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0289 - accuracy: 0.8015 - val_loss: 0.0282 - val_accuracy: 0.8090\n",
      "Epoch 619/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0289 - accuracy: 0.8029 - val_loss: 0.0281 - val_accuracy: 0.8085\n",
      "Epoch 620/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0288 - accuracy: 0.8037 - val_loss: 0.0283 - val_accuracy: 0.8072\n",
      "Epoch 621/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0289 - accuracy: 0.8024 - val_loss: 0.0282 - val_accuracy: 0.8100\n",
      "Epoch 622/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0288 - accuracy: 0.8029 - val_loss: 0.0282 - val_accuracy: 0.8102\n",
      "Epoch 623/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0289 - accuracy: 0.8033 - val_loss: 0.0281 - val_accuracy: 0.8098\n",
      "Epoch 624/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0288 - accuracy: 0.8035 - val_loss: 0.0282 - val_accuracy: 0.8094\n",
      "Epoch 625/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0288 - accuracy: 0.8032 - val_loss: 0.0281 - val_accuracy: 0.8096\n",
      "Epoch 626/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0290 - accuracy: 0.8008 - val_loss: 0.0280 - val_accuracy: 0.8118\n",
      "Epoch 627/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0288 - accuracy: 0.8046 - val_loss: 0.0280 - val_accuracy: 0.8123\n",
      "Epoch 628/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0289 - accuracy: 0.8031 - val_loss: 0.0280 - val_accuracy: 0.8115\n",
      "Epoch 629/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0288 - accuracy: 0.8021 - val_loss: 0.0279 - val_accuracy: 0.8099\n",
      "Epoch 630/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0288 - accuracy: 0.8035 - val_loss: 0.0280 - val_accuracy: 0.8113\n",
      "Epoch 631/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0286 - accuracy: 0.8043 - val_loss: 0.0279 - val_accuracy: 0.8117\n",
      "Epoch 632/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0286 - accuracy: 0.8051 - val_loss: 0.0279 - val_accuracy: 0.8113\n",
      "Epoch 633/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0286 - accuracy: 0.8042 - val_loss: 0.0278 - val_accuracy: 0.8116\n",
      "Epoch 634/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0287 - accuracy: 0.8033 - val_loss: 0.0279 - val_accuracy: 0.8121\n",
      "Epoch 635/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0285 - accuracy: 0.8067 - val_loss: 0.0278 - val_accuracy: 0.8121\n",
      "Epoch 636/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0283 - accuracy: 0.8057 - val_loss: 0.0277 - val_accuracy: 0.8133\n",
      "Epoch 637/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0284 - accuracy: 0.8058 - val_loss: 0.0278 - val_accuracy: 0.8118\n",
      "Epoch 638/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0283 - accuracy: 0.8072 - val_loss: 0.0277 - val_accuracy: 0.8121\n",
      "Epoch 639/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0282 - accuracy: 0.8083 - val_loss: 0.0278 - val_accuracy: 0.8112\n",
      "Epoch 640/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0285 - accuracy: 0.8053 - val_loss: 0.0276 - val_accuracy: 0.8138\n",
      "Epoch 641/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0282 - accuracy: 0.8090 - val_loss: 0.0277 - val_accuracy: 0.8112\n",
      "Epoch 642/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0282 - accuracy: 0.8092 - val_loss: 0.0277 - val_accuracy: 0.8107\n",
      "Epoch 643/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0285 - accuracy: 0.8061 - val_loss: 0.0277 - val_accuracy: 0.8141\n",
      "Epoch 644/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0285 - accuracy: 0.8054 - val_loss: 0.0277 - val_accuracy: 0.8114\n",
      "Epoch 645/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0284 - accuracy: 0.8074 - val_loss: 0.0276 - val_accuracy: 0.8128\n",
      "Epoch 646/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0283 - accuracy: 0.8071 - val_loss: 0.0276 - val_accuracy: 0.8116\n",
      "Epoch 647/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0282 - accuracy: 0.8081 - val_loss: 0.0275 - val_accuracy: 0.8128\n",
      "Epoch 648/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0283 - accuracy: 0.8058 - val_loss: 0.0275 - val_accuracy: 0.8133\n",
      "Epoch 649/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0280 - accuracy: 0.8100 - val_loss: 0.0275 - val_accuracy: 0.8142\n",
      "Epoch 650/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0282 - accuracy: 0.8067 - val_loss: 0.0275 - val_accuracy: 0.8123\n",
      "Epoch 651/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0281 - accuracy: 0.8086 - val_loss: 0.0274 - val_accuracy: 0.8158\n",
      "Epoch 652/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0281 - accuracy: 0.8094 - val_loss: 0.0274 - val_accuracy: 0.8152\n",
      "Epoch 653/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0282 - accuracy: 0.8086 - val_loss: 0.0275 - val_accuracy: 0.8146\n",
      "Epoch 654/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0279 - accuracy: 0.8110 - val_loss: 0.0274 - val_accuracy: 0.8146\n",
      "Epoch 655/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0279 - accuracy: 0.8103 - val_loss: 0.0273 - val_accuracy: 0.8147\n",
      "Epoch 656/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0281 - accuracy: 0.8081 - val_loss: 0.0274 - val_accuracy: 0.8129\n",
      "Epoch 657/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0278 - accuracy: 0.8100 - val_loss: 0.0272 - val_accuracy: 0.8178\n",
      "Epoch 658/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0280 - accuracy: 0.8087 - val_loss: 0.0274 - val_accuracy: 0.8129\n",
      "Epoch 659/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0278 - accuracy: 0.8099 - val_loss: 0.0272 - val_accuracy: 0.8168\n",
      "Epoch 660/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0279 - accuracy: 0.8093 - val_loss: 0.0272 - val_accuracy: 0.8156\n",
      "Epoch 661/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0279 - accuracy: 0.8099 - val_loss: 0.0272 - val_accuracy: 0.8173\n",
      "Epoch 662/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0278 - accuracy: 0.8121 - val_loss: 0.0273 - val_accuracy: 0.8162\n",
      "Epoch 663/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0278 - accuracy: 0.8109 - val_loss: 0.0272 - val_accuracy: 0.8162\n",
      "Epoch 664/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0277 - accuracy: 0.8131 - val_loss: 0.0271 - val_accuracy: 0.8175\n",
      "Epoch 665/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0277 - accuracy: 0.8121 - val_loss: 0.0272 - val_accuracy: 0.8144\n",
      "Epoch 666/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0277 - accuracy: 0.8108 - val_loss: 0.0271 - val_accuracy: 0.8166\n",
      "Epoch 667/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0277 - accuracy: 0.8127 - val_loss: 0.0272 - val_accuracy: 0.8145\n",
      "Epoch 668/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0279 - accuracy: 0.8096 - val_loss: 0.0270 - val_accuracy: 0.8155\n",
      "Epoch 669/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0279 - accuracy: 0.8090 - val_loss: 0.0271 - val_accuracy: 0.8157\n",
      "Epoch 670/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0277 - accuracy: 0.8114 - val_loss: 0.0271 - val_accuracy: 0.8169\n",
      "Epoch 671/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0275 - accuracy: 0.8139 - val_loss: 0.0270 - val_accuracy: 0.8169\n",
      "Epoch 672/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0275 - accuracy: 0.8127 - val_loss: 0.0270 - val_accuracy: 0.8160\n",
      "Epoch 673/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0277 - accuracy: 0.8115 - val_loss: 0.0269 - val_accuracy: 0.8177\n",
      "Epoch 674/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0276 - accuracy: 0.8127 - val_loss: 0.0270 - val_accuracy: 0.8171\n",
      "Epoch 675/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0274 - accuracy: 0.8137 - val_loss: 0.0268 - val_accuracy: 0.8190\n",
      "Epoch 676/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0274 - accuracy: 0.8136 - val_loss: 0.0271 - val_accuracy: 0.8129\n",
      "Epoch 677/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0274 - accuracy: 0.8133 - val_loss: 0.0269 - val_accuracy: 0.8187\n",
      "Epoch 678/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0275 - accuracy: 0.8133 - val_loss: 0.0269 - val_accuracy: 0.8175\n",
      "Epoch 679/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0274 - accuracy: 0.8135 - val_loss: 0.0269 - val_accuracy: 0.8207\n",
      "Epoch 680/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0274 - accuracy: 0.8139 - val_loss: 0.0268 - val_accuracy: 0.8182\n",
      "Epoch 681/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0276 - accuracy: 0.8114 - val_loss: 0.0268 - val_accuracy: 0.8192\n",
      "Epoch 682/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0275 - accuracy: 0.8131 - val_loss: 0.0268 - val_accuracy: 0.8197\n",
      "Epoch 683/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0276 - accuracy: 0.8116 - val_loss: 0.0267 - val_accuracy: 0.8198\n",
      "Epoch 684/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0275 - accuracy: 0.8121 - val_loss: 0.0267 - val_accuracy: 0.8187\n",
      "Epoch 685/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0273 - accuracy: 0.8148 - val_loss: 0.0266 - val_accuracy: 0.8190\n",
      "Epoch 686/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0273 - accuracy: 0.8146 - val_loss: 0.0266 - val_accuracy: 0.8188\n",
      "Epoch 687/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0273 - accuracy: 0.8139 - val_loss: 0.0266 - val_accuracy: 0.8203\n",
      "Epoch 688/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0274 - accuracy: 0.8142 - val_loss: 0.0266 - val_accuracy: 0.8192\n",
      "Epoch 689/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0271 - accuracy: 0.8164 - val_loss: 0.0265 - val_accuracy: 0.8196\n",
      "Epoch 690/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0274 - accuracy: 0.8134 - val_loss: 0.0265 - val_accuracy: 0.8209\n",
      "Epoch 691/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0270 - accuracy: 0.8172 - val_loss: 0.0267 - val_accuracy: 0.8193\n",
      "Epoch 692/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0272 - accuracy: 0.8158 - val_loss: 0.0266 - val_accuracy: 0.8202\n",
      "Epoch 693/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0273 - accuracy: 0.8136 - val_loss: 0.0266 - val_accuracy: 0.8194\n",
      "Epoch 694/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0272 - accuracy: 0.8151 - val_loss: 0.0265 - val_accuracy: 0.8215\n",
      "Epoch 695/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0271 - accuracy: 0.8162 - val_loss: 0.0265 - val_accuracy: 0.8206\n",
      "Epoch 696/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0273 - accuracy: 0.8146 - val_loss: 0.0266 - val_accuracy: 0.8200\n",
      "Epoch 697/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0272 - accuracy: 0.8144 - val_loss: 0.0266 - val_accuracy: 0.8214\n",
      "Epoch 698/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0271 - accuracy: 0.8155 - val_loss: 0.0263 - val_accuracy: 0.8224\n",
      "Epoch 699/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0270 - accuracy: 0.8156 - val_loss: 0.0264 - val_accuracy: 0.8201\n",
      "Epoch 700/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0270 - accuracy: 0.8156 - val_loss: 0.0263 - val_accuracy: 0.8234\n",
      "Epoch 701/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0270 - accuracy: 0.8160 - val_loss: 0.0265 - val_accuracy: 0.8201\n",
      "Epoch 702/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0270 - accuracy: 0.8162 - val_loss: 0.0263 - val_accuracy: 0.8225\n",
      "Epoch 703/900\n",
      "45676/45676 [==============================] - 2s 54us/step - loss: 0.0269 - accuracy: 0.8161 - val_loss: 0.0263 - val_accuracy: 0.8217\n",
      "Epoch 704/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0270 - accuracy: 0.8158 - val_loss: 0.0263 - val_accuracy: 0.8225\n",
      "Epoch 705/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0269 - accuracy: 0.8179 - val_loss: 0.0263 - val_accuracy: 0.8224\n",
      "Epoch 706/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0271 - accuracy: 0.8149 - val_loss: 0.0263 - val_accuracy: 0.8213\n",
      "Epoch 707/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0268 - accuracy: 0.8174 - val_loss: 0.0262 - val_accuracy: 0.8223\n",
      "Epoch 708/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0267 - accuracy: 0.8194 - val_loss: 0.0262 - val_accuracy: 0.8237\n",
      "Epoch 709/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0269 - accuracy: 0.8177 - val_loss: 0.0261 - val_accuracy: 0.8246\n",
      "Epoch 710/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0267 - accuracy: 0.8185 - val_loss: 0.0262 - val_accuracy: 0.8229\n",
      "Epoch 711/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0266 - accuracy: 0.8187 - val_loss: 0.0263 - val_accuracy: 0.8219\n",
      "Epoch 712/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0269 - accuracy: 0.8176 - val_loss: 0.0261 - val_accuracy: 0.8229\n",
      "Epoch 713/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0269 - accuracy: 0.8168 - val_loss: 0.0261 - val_accuracy: 0.8236\n",
      "Epoch 714/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0267 - accuracy: 0.8185 - val_loss: 0.0260 - val_accuracy: 0.8252\n",
      "Epoch 715/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0267 - accuracy: 0.8178 - val_loss: 0.0262 - val_accuracy: 0.8222\n",
      "Epoch 716/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0266 - accuracy: 0.8192 - val_loss: 0.0261 - val_accuracy: 0.8236\n",
      "Epoch 717/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0265 - accuracy: 0.8205 - val_loss: 0.0262 - val_accuracy: 0.8219\n",
      "Epoch 718/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0266 - accuracy: 0.8197 - val_loss: 0.0261 - val_accuracy: 0.8226\n",
      "Epoch 719/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0267 - accuracy: 0.8174 - val_loss: 0.0259 - val_accuracy: 0.8247\n",
      "Epoch 720/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0267 - accuracy: 0.8184 - val_loss: 0.0260 - val_accuracy: 0.8228\n",
      "Epoch 721/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0265 - accuracy: 0.8212 - val_loss: 0.0259 - val_accuracy: 0.8242\n",
      "Epoch 722/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0265 - accuracy: 0.8202 - val_loss: 0.0259 - val_accuracy: 0.8240\n",
      "Epoch 723/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0265 - accuracy: 0.8191 - val_loss: 0.0259 - val_accuracy: 0.8236\n",
      "Epoch 724/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0266 - accuracy: 0.8188 - val_loss: 0.0258 - val_accuracy: 0.8261\n",
      "Epoch 725/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0266 - accuracy: 0.8188 - val_loss: 0.0259 - val_accuracy: 0.8244\n",
      "Epoch 726/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0265 - accuracy: 0.8201 - val_loss: 0.0259 - val_accuracy: 0.8260\n",
      "Epoch 727/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0263 - accuracy: 0.8218 - val_loss: 0.0259 - val_accuracy: 0.8245\n",
      "Epoch 728/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0265 - accuracy: 0.8198 - val_loss: 0.0258 - val_accuracy: 0.8270\n",
      "Epoch 729/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0264 - accuracy: 0.8215 - val_loss: 0.0259 - val_accuracy: 0.8243\n",
      "Epoch 730/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0265 - accuracy: 0.8202 - val_loss: 0.0258 - val_accuracy: 0.8251\n",
      "Epoch 731/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0264 - accuracy: 0.8199 - val_loss: 0.0258 - val_accuracy: 0.8251\n",
      "Epoch 732/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0264 - accuracy: 0.8208 - val_loss: 0.0257 - val_accuracy: 0.8262\n",
      "Epoch 733/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0262 - accuracy: 0.8225 - val_loss: 0.0257 - val_accuracy: 0.8258\n",
      "Epoch 734/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0263 - accuracy: 0.8198 - val_loss: 0.0258 - val_accuracy: 0.8257\n",
      "Epoch 735/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0263 - accuracy: 0.8215 - val_loss: 0.0257 - val_accuracy: 0.8243\n",
      "Epoch 736/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0262 - accuracy: 0.8229 - val_loss: 0.0257 - val_accuracy: 0.8260\n",
      "Epoch 737/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0261 - accuracy: 0.8218 - val_loss: 0.0256 - val_accuracy: 0.8264\n",
      "Epoch 738/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0261 - accuracy: 0.8219 - val_loss: 0.0256 - val_accuracy: 0.8274\n",
      "Epoch 739/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0259 - accuracy: 0.8252 - val_loss: 0.0256 - val_accuracy: 0.8274\n",
      "Epoch 740/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0262 - accuracy: 0.8215 - val_loss: 0.0255 - val_accuracy: 0.8278\n",
      "Epoch 741/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0261 - accuracy: 0.8231 - val_loss: 0.0255 - val_accuracy: 0.8263\n",
      "Epoch 742/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0262 - accuracy: 0.8231 - val_loss: 0.0255 - val_accuracy: 0.8280\n",
      "Epoch 743/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0260 - accuracy: 0.8235 - val_loss: 0.0255 - val_accuracy: 0.8271\n",
      "Epoch 744/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0259 - accuracy: 0.8252 - val_loss: 0.0254 - val_accuracy: 0.8278\n",
      "Epoch 745/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0261 - accuracy: 0.8231 - val_loss: 0.0254 - val_accuracy: 0.8289\n",
      "Epoch 746/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0259 - accuracy: 0.8245 - val_loss: 0.0256 - val_accuracy: 0.8262\n",
      "Epoch 747/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0259 - accuracy: 0.8227 - val_loss: 0.0255 - val_accuracy: 0.8253\n",
      "Epoch 748/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0260 - accuracy: 0.8227 - val_loss: 0.0254 - val_accuracy: 0.8259\n",
      "Epoch 749/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0261 - accuracy: 0.8231 - val_loss: 0.0254 - val_accuracy: 0.8292\n",
      "Epoch 750/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0259 - accuracy: 0.8232 - val_loss: 0.0254 - val_accuracy: 0.8277\n",
      "Epoch 751/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0262 - accuracy: 0.8220 - val_loss: 0.0253 - val_accuracy: 0.8290\n",
      "Epoch 752/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0260 - accuracy: 0.8257 - val_loss: 0.0253 - val_accuracy: 0.8280\n",
      "Epoch 753/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0259 - accuracy: 0.8234 - val_loss: 0.0253 - val_accuracy: 0.8278\n",
      "Epoch 754/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0260 - accuracy: 0.8245 - val_loss: 0.0252 - val_accuracy: 0.8279\n",
      "Epoch 755/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0258 - accuracy: 0.8257 - val_loss: 0.0254 - val_accuracy: 0.8283\n",
      "Epoch 756/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0260 - accuracy: 0.8229 - val_loss: 0.0253 - val_accuracy: 0.8283\n",
      "Epoch 757/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0257 - accuracy: 0.8254 - val_loss: 0.0252 - val_accuracy: 0.8291\n",
      "Epoch 758/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0258 - accuracy: 0.8256 - val_loss: 0.0253 - val_accuracy: 0.8290\n",
      "Epoch 759/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0257 - accuracy: 0.8259 - val_loss: 0.0253 - val_accuracy: 0.8284\n",
      "Epoch 760/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0260 - accuracy: 0.8232 - val_loss: 0.0253 - val_accuracy: 0.8295\n",
      "Epoch 761/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0259 - accuracy: 0.8245 - val_loss: 0.0252 - val_accuracy: 0.8289\n",
      "Epoch 762/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0256 - accuracy: 0.8273 - val_loss: 0.0253 - val_accuracy: 0.8275\n",
      "Epoch 763/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0257 - accuracy: 0.8257 - val_loss: 0.0252 - val_accuracy: 0.8312\n",
      "Epoch 764/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0259 - accuracy: 0.8245 - val_loss: 0.0251 - val_accuracy: 0.8294\n",
      "Epoch 765/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0258 - accuracy: 0.8254 - val_loss: 0.0251 - val_accuracy: 0.8292\n",
      "Epoch 766/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0258 - accuracy: 0.8246 - val_loss: 0.0250 - val_accuracy: 0.8308\n",
      "Epoch 767/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0256 - accuracy: 0.8256 - val_loss: 0.0251 - val_accuracy: 0.8280\n",
      "Epoch 768/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0255 - accuracy: 0.8263 - val_loss: 0.0250 - val_accuracy: 0.8283\n",
      "Epoch 769/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0256 - accuracy: 0.8266 - val_loss: 0.0251 - val_accuracy: 0.8299\n",
      "Epoch 770/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0255 - accuracy: 0.8254 - val_loss: 0.0249 - val_accuracy: 0.8292\n",
      "Epoch 771/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0256 - accuracy: 0.8259 - val_loss: 0.0250 - val_accuracy: 0.8301\n",
      "Epoch 772/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0255 - accuracy: 0.8267 - val_loss: 0.0249 - val_accuracy: 0.8311\n",
      "Epoch 773/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0255 - accuracy: 0.8277 - val_loss: 0.0249 - val_accuracy: 0.8317\n",
      "Epoch 774/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0256 - accuracy: 0.8256 - val_loss: 0.0249 - val_accuracy: 0.8311\n",
      "Epoch 775/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0256 - accuracy: 0.8247 - val_loss: 0.0249 - val_accuracy: 0.8288\n",
      "Epoch 776/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0253 - accuracy: 0.8290 - val_loss: 0.0249 - val_accuracy: 0.8320\n",
      "Epoch 777/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0252 - accuracy: 0.8290 - val_loss: 0.0249 - val_accuracy: 0.8303\n",
      "Epoch 778/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0255 - accuracy: 0.8264 - val_loss: 0.0249 - val_accuracy: 0.8308\n",
      "Epoch 779/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0255 - accuracy: 0.8267 - val_loss: 0.0248 - val_accuracy: 0.8329\n",
      "Epoch 780/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0254 - accuracy: 0.8298 - val_loss: 0.0248 - val_accuracy: 0.8325\n",
      "Epoch 781/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0255 - accuracy: 0.8272 - val_loss: 0.0248 - val_accuracy: 0.8296\n",
      "Epoch 782/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0253 - accuracy: 0.8302 - val_loss: 0.0248 - val_accuracy: 0.8308\n",
      "Epoch 783/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0254 - accuracy: 0.8267 - val_loss: 0.0248 - val_accuracy: 0.8310\n",
      "Epoch 784/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0253 - accuracy: 0.8280 - val_loss: 0.0247 - val_accuracy: 0.8334\n",
      "Epoch 785/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0253 - accuracy: 0.8307 - val_loss: 0.0247 - val_accuracy: 0.8313\n",
      "Epoch 786/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0253 - accuracy: 0.8294 - val_loss: 0.0247 - val_accuracy: 0.8306\n",
      "Epoch 787/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0252 - accuracy: 0.8289 - val_loss: 0.0248 - val_accuracy: 0.8318\n",
      "Epoch 788/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0254 - accuracy: 0.8279 - val_loss: 0.0247 - val_accuracy: 0.8317\n",
      "Epoch 789/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0253 - accuracy: 0.8277 - val_loss: 0.0246 - val_accuracy: 0.8324\n",
      "Epoch 790/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0251 - accuracy: 0.8291 - val_loss: 0.0247 - val_accuracy: 0.8316\n",
      "Epoch 791/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0249 - accuracy: 0.8317 - val_loss: 0.0245 - val_accuracy: 0.8321\n",
      "Epoch 792/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0253 - accuracy: 0.8274 - val_loss: 0.0246 - val_accuracy: 0.8315\n",
      "Epoch 793/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0252 - accuracy: 0.8295 - val_loss: 0.0247 - val_accuracy: 0.8313\n",
      "Epoch 794/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0250 - accuracy: 0.8314 - val_loss: 0.0247 - val_accuracy: 0.8318\n",
      "Epoch 795/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0252 - accuracy: 0.8276 - val_loss: 0.0244 - val_accuracy: 0.8326\n",
      "Epoch 796/900\n",
      "45676/45676 [==============================] - 2s 43us/step - loss: 0.0250 - accuracy: 0.8316 - val_loss: 0.0245 - val_accuracy: 0.8328\n",
      "Epoch 797/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0249 - accuracy: 0.8321 - val_loss: 0.0245 - val_accuracy: 0.8347\n",
      "Epoch 798/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0251 - accuracy: 0.8300 - val_loss: 0.0246 - val_accuracy: 0.8338\n",
      "Epoch 799/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0249 - accuracy: 0.8319 - val_loss: 0.0244 - val_accuracy: 0.8340\n",
      "Epoch 800/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0251 - accuracy: 0.8291 - val_loss: 0.0243 - val_accuracy: 0.8356\n",
      "Epoch 801/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0252 - accuracy: 0.8285 - val_loss: 0.0245 - val_accuracy: 0.8331\n",
      "Epoch 802/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0250 - accuracy: 0.8308 - val_loss: 0.0244 - val_accuracy: 0.8338\n",
      "Epoch 803/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0247 - accuracy: 0.8345 - val_loss: 0.0244 - val_accuracy: 0.8342\n",
      "Epoch 804/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0246 - accuracy: 0.8344 - val_loss: 0.0244 - val_accuracy: 0.8338\n",
      "Epoch 805/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0248 - accuracy: 0.8326 - val_loss: 0.0243 - val_accuracy: 0.8352\n",
      "Epoch 806/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0251 - accuracy: 0.8303 - val_loss: 0.0244 - val_accuracy: 0.8341\n",
      "Epoch 807/900\n",
      "45676/45676 [==============================] - 4s 79us/step - loss: 0.0250 - accuracy: 0.8296 - val_loss: 0.0243 - val_accuracy: 0.8342\n",
      "Epoch 808/900\n",
      "45676/45676 [==============================] - 3s 73us/step - loss: 0.0247 - accuracy: 0.8337 - val_loss: 0.0242 - val_accuracy: 0.8362\n",
      "Epoch 809/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0251 - accuracy: 0.8291 - val_loss: 0.0242 - val_accuracy: 0.8343\n",
      "Epoch 810/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0249 - accuracy: 0.8320 - val_loss: 0.0242 - val_accuracy: 0.8360\n",
      "Epoch 811/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0247 - accuracy: 0.8324 - val_loss: 0.0243 - val_accuracy: 0.8349\n",
      "Epoch 812/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0248 - accuracy: 0.8323 - val_loss: 0.0242 - val_accuracy: 0.8352\n",
      "Epoch 813/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0247 - accuracy: 0.8332 - val_loss: 0.0242 - val_accuracy: 0.8353\n",
      "Epoch 814/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0247 - accuracy: 0.8327 - val_loss: 0.0242 - val_accuracy: 0.8356\n",
      "Epoch 815/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0247 - accuracy: 0.8331 - val_loss: 0.0241 - val_accuracy: 0.8360\n",
      "Epoch 816/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0248 - accuracy: 0.8330 - val_loss: 0.0241 - val_accuracy: 0.8360\n",
      "Epoch 817/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0247 - accuracy: 0.8337 - val_loss: 0.0242 - val_accuracy: 0.8355\n",
      "Epoch 818/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0249 - accuracy: 0.8302 - val_loss: 0.0241 - val_accuracy: 0.8371\n",
      "Epoch 819/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0247 - accuracy: 0.8317 - val_loss: 0.0241 - val_accuracy: 0.8349\n",
      "Epoch 820/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0246 - accuracy: 0.8322 - val_loss: 0.0242 - val_accuracy: 0.8362\n",
      "Epoch 821/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0246 - accuracy: 0.8338 - val_loss: 0.0240 - val_accuracy: 0.8370\n",
      "Epoch 822/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0244 - accuracy: 0.8359 - val_loss: 0.0239 - val_accuracy: 0.8386\n",
      "Epoch 823/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0247 - accuracy: 0.8319 - val_loss: 0.0240 - val_accuracy: 0.8372\n",
      "Epoch 824/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0244 - accuracy: 0.8342 - val_loss: 0.0240 - val_accuracy: 0.8341\n",
      "Epoch 825/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0246 - accuracy: 0.8329 - val_loss: 0.0240 - val_accuracy: 0.8375\n",
      "Epoch 826/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0245 - accuracy: 0.8348 - val_loss: 0.0240 - val_accuracy: 0.8376\n",
      "Epoch 827/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0246 - accuracy: 0.8332 - val_loss: 0.0240 - val_accuracy: 0.8372\n",
      "Epoch 828/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0247 - accuracy: 0.8330 - val_loss: 0.0240 - val_accuracy: 0.8376\n",
      "Epoch 829/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0244 - accuracy: 0.8353 - val_loss: 0.0240 - val_accuracy: 0.8381\n",
      "Epoch 830/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0245 - accuracy: 0.8354 - val_loss: 0.0239 - val_accuracy: 0.8380\n",
      "Epoch 831/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0245 - accuracy: 0.8345 - val_loss: 0.0240 - val_accuracy: 0.8371\n",
      "Epoch 832/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0242 - accuracy: 0.8361 - val_loss: 0.0238 - val_accuracy: 0.8381\n",
      "Epoch 833/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0245 - accuracy: 0.8337 - val_loss: 0.0239 - val_accuracy: 0.8367\n",
      "Epoch 834/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0245 - accuracy: 0.8330 - val_loss: 0.0239 - val_accuracy: 0.8370\n",
      "Epoch 835/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0244 - accuracy: 0.8345 - val_loss: 0.0239 - val_accuracy: 0.8382\n",
      "Epoch 836/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0244 - accuracy: 0.8352 - val_loss: 0.0238 - val_accuracy: 0.8396\n",
      "Epoch 837/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0243 - accuracy: 0.8354 - val_loss: 0.0239 - val_accuracy: 0.8386\n",
      "Epoch 838/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0244 - accuracy: 0.8355 - val_loss: 0.0238 - val_accuracy: 0.8377\n",
      "Epoch 839/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0243 - accuracy: 0.8356 - val_loss: 0.0237 - val_accuracy: 0.8384\n",
      "Epoch 840/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0242 - accuracy: 0.8369 - val_loss: 0.0237 - val_accuracy: 0.8387\n",
      "Epoch 841/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0240 - accuracy: 0.8386 - val_loss: 0.0237 - val_accuracy: 0.8389\n",
      "Epoch 842/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0243 - accuracy: 0.8369 - val_loss: 0.0237 - val_accuracy: 0.8384\n",
      "Epoch 843/900\n",
      "45676/45676 [==============================] - 2s 48us/step - loss: 0.0241 - accuracy: 0.8385 - val_loss: 0.0238 - val_accuracy: 0.8380\n",
      "Epoch 844/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0243 - accuracy: 0.8357 - val_loss: 0.0236 - val_accuracy: 0.8397\n",
      "Epoch 845/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0243 - accuracy: 0.8356 - val_loss: 0.0237 - val_accuracy: 0.8378\n",
      "Epoch 846/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0241 - accuracy: 0.8369 - val_loss: 0.0236 - val_accuracy: 0.8397\n",
      "Epoch 847/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0243 - accuracy: 0.8358 - val_loss: 0.0235 - val_accuracy: 0.8398\n",
      "Epoch 848/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0242 - accuracy: 0.8361 - val_loss: 0.0236 - val_accuracy: 0.8386\n",
      "Epoch 849/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0243 - accuracy: 0.8364 - val_loss: 0.0236 - val_accuracy: 0.8395\n",
      "Epoch 850/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0242 - accuracy: 0.8366 - val_loss: 0.0236 - val_accuracy: 0.8397\n",
      "Epoch 851/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0244 - accuracy: 0.8355 - val_loss: 0.0236 - val_accuracy: 0.8396\n",
      "Epoch 852/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0242 - accuracy: 0.8363 - val_loss: 0.0235 - val_accuracy: 0.8403\n",
      "Epoch 853/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0242 - accuracy: 0.8363 - val_loss: 0.0235 - val_accuracy: 0.8391\n",
      "Epoch 854/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0242 - accuracy: 0.8372 - val_loss: 0.0235 - val_accuracy: 0.8392\n",
      "Epoch 855/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0242 - accuracy: 0.8371 - val_loss: 0.0235 - val_accuracy: 0.8402\n",
      "Epoch 856/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0239 - accuracy: 0.8388 - val_loss: 0.0234 - val_accuracy: 0.8422\n",
      "Epoch 857/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0241 - accuracy: 0.8371 - val_loss: 0.0235 - val_accuracy: 0.8396\n",
      "Epoch 858/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0241 - accuracy: 0.8373 - val_loss: 0.0234 - val_accuracy: 0.8412\n",
      "Epoch 859/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0240 - accuracy: 0.8374 - val_loss: 0.0234 - val_accuracy: 0.8427\n",
      "Epoch 860/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0240 - accuracy: 0.8379 - val_loss: 0.0234 - val_accuracy: 0.8418\n",
      "Epoch 861/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0240 - accuracy: 0.8385 - val_loss: 0.0234 - val_accuracy: 0.8410\n",
      "Epoch 862/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0240 - accuracy: 0.8384 - val_loss: 0.0233 - val_accuracy: 0.8421\n",
      "Epoch 863/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0241 - accuracy: 0.8387 - val_loss: 0.0235 - val_accuracy: 0.8411\n",
      "Epoch 864/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0240 - accuracy: 0.8378 - val_loss: 0.0234 - val_accuracy: 0.8406\n",
      "Epoch 865/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0239 - accuracy: 0.8396 - val_loss: 0.0234 - val_accuracy: 0.8405\n",
      "Epoch 866/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0238 - accuracy: 0.8406 - val_loss: 0.0233 - val_accuracy: 0.8410\n",
      "Epoch 867/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0239 - accuracy: 0.8373 - val_loss: 0.0233 - val_accuracy: 0.8424\n",
      "Epoch 868/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0240 - accuracy: 0.8381 - val_loss: 0.0232 - val_accuracy: 0.8436\n",
      "Epoch 869/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0241 - accuracy: 0.8358 - val_loss: 0.0233 - val_accuracy: 0.8412\n",
      "Epoch 870/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0237 - accuracy: 0.8397 - val_loss: 0.0233 - val_accuracy: 0.8399\n",
      "Epoch 871/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0239 - accuracy: 0.8390 - val_loss: 0.0233 - val_accuracy: 0.8420\n",
      "Epoch 872/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0240 - accuracy: 0.8384 - val_loss: 0.0232 - val_accuracy: 0.8419\n",
      "Epoch 873/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0238 - accuracy: 0.8393 - val_loss: 0.0232 - val_accuracy: 0.8435\n",
      "Epoch 874/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0239 - accuracy: 0.8384 - val_loss: 0.0232 - val_accuracy: 0.8436\n",
      "Epoch 875/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0238 - accuracy: 0.8392 - val_loss: 0.0231 - val_accuracy: 0.8425\n",
      "Epoch 876/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0239 - accuracy: 0.8390 - val_loss: 0.0231 - val_accuracy: 0.8432\n",
      "Epoch 877/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0235 - accuracy: 0.8411 - val_loss: 0.0231 - val_accuracy: 0.8423\n",
      "Epoch 878/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0235 - accuracy: 0.8417 - val_loss: 0.0230 - val_accuracy: 0.8437\n",
      "Epoch 879/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0235 - accuracy: 0.8421 - val_loss: 0.0232 - val_accuracy: 0.8422\n",
      "Epoch 880/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0239 - accuracy: 0.8381 - val_loss: 0.0232 - val_accuracy: 0.8420\n",
      "Epoch 881/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0237 - accuracy: 0.8385 - val_loss: 0.0230 - val_accuracy: 0.8430\n",
      "Epoch 882/900\n",
      "45676/45676 [==============================] - 2s 44us/step - loss: 0.0236 - accuracy: 0.8409 - val_loss: 0.0231 - val_accuracy: 0.8433\n",
      "Epoch 883/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0234 - accuracy: 0.8430 - val_loss: 0.0231 - val_accuracy: 0.8440\n",
      "Epoch 884/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0237 - accuracy: 0.8401 - val_loss: 0.0230 - val_accuracy: 0.8444\n",
      "Epoch 885/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0237 - accuracy: 0.8397 - val_loss: 0.0231 - val_accuracy: 0.8445\n",
      "Epoch 886/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0235 - accuracy: 0.8412 - val_loss: 0.0231 - val_accuracy: 0.8447\n",
      "Epoch 887/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0236 - accuracy: 0.8410 - val_loss: 0.0231 - val_accuracy: 0.8445\n",
      "Epoch 888/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0237 - accuracy: 0.8398 - val_loss: 0.0231 - val_accuracy: 0.8431\n",
      "Epoch 889/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0236 - accuracy: 0.8406 - val_loss: 0.0230 - val_accuracy: 0.8442\n",
      "Epoch 890/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0236 - accuracy: 0.8401 - val_loss: 0.0229 - val_accuracy: 0.8454\n",
      "Epoch 891/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0234 - accuracy: 0.8420 - val_loss: 0.0228 - val_accuracy: 0.8457\n",
      "Epoch 892/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0234 - accuracy: 0.8414 - val_loss: 0.0229 - val_accuracy: 0.8464\n",
      "Epoch 893/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0233 - accuracy: 0.8439 - val_loss: 0.0229 - val_accuracy: 0.8452\n",
      "Epoch 894/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0235 - accuracy: 0.8406 - val_loss: 0.0229 - val_accuracy: 0.8452\n",
      "Epoch 895/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0234 - accuracy: 0.8418 - val_loss: 0.0228 - val_accuracy: 0.8461\n",
      "Epoch 896/900\n",
      "45676/45676 [==============================] - 2s 47us/step - loss: 0.0235 - accuracy: 0.8421 - val_loss: 0.0228 - val_accuracy: 0.8445\n",
      "Epoch 897/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0232 - accuracy: 0.8437 - val_loss: 0.0229 - val_accuracy: 0.8454\n",
      "Epoch 898/900\n",
      "45676/45676 [==============================] - 2s 45us/step - loss: 0.0233 - accuracy: 0.8419 - val_loss: 0.0229 - val_accuracy: 0.8445\n",
      "Epoch 899/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0234 - accuracy: 0.8428 - val_loss: 0.0228 - val_accuracy: 0.8454\n",
      "Epoch 900/900\n",
      "45676/45676 [==============================] - 2s 46us/step - loss: 0.0233 - accuracy: 0.8435 - val_loss: 0.0228 - val_accuracy: 0.8473\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 900, batch_size=150,validation_data= (x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "eNF8mZelgcis",
    "outputId": "0244a7e5-ed7f-4b79-cb32-e92657f0c720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, None, 100)         68400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, None, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 99,110\n",
      "Trainable params: 99,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, batch_input_shape = (None, None, x.shape[2]),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "rmsprop =keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08)\n",
    "model.compile(loss='mean_squared_error',\n",
    "                  optimizer=rmsprop,\n",
    "                  metrics=['accuracy'])\n",
    "#adam = keras.optimizers.Adam(lr=0.5, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13UOXLiKUfBK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRmWs2V1S4oL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOeAesWjvyTH30TLuaD8D7b",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1oKh3_PBZwmCW5KC0LaLWskFdjnTrXGC4",
   "name": "Copy of erakure.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
